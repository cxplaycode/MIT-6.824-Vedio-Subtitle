WEBVTT

1
00:00:00.800 --> 00:00:05.209
all right let's get started
   ok 开始上课了
2
00:00:05.209 --> 00:00:11.820
this is 6.824 distributed systems so
  这是分布式系统课程6.824
3
00:00:11.820 --> 00:00:13.230
I'd like to start with just a brief
我简短的介绍一下
4
00:00:13.230 --> 00:00:14.640
explanation of what I think a
    我认为的分布式系统
5
00:00:14.640 --> 00:00:18.960
distributed system is you know the core
一连串的互相协昨的计算机，他们是通过网络来进行彼此通信
6
00:00:18.960 --> 00:00:21.629
of it is a set of cooperating computers

7
00:00:21.629 --> 00:00:23.190
that are communicating with each other

8
00:00:23.190 --> 00:00:26.190
over networked to get some coherent TAsk
然后完成一些具有一致性的任务
9
00:00:26.190 --> 00:00:29.730
done and so the kinds of examples that
所以我们会聚焦于一些例子如
10
00:00:29.730 --> 00:00:31.530
we'll be focusing on in this class are

11
00:00:31.530 --> 00:00:34.969
things like storage for big websites or
大型网站的数据存储
12
00:00:34.969 --> 00:00:38.659
big data computations such as MapReduce
大数据计算如mapreduce
13
00:00:38.659 --> 00:00:41.759
and also somewhat more exotic things
以及一些其他的
14
00:00:41.759 --> 00:00:43.950
like peer-to-peer file sharing so
点对点的文件共享
15
00:00:43.950 --> 00:00:45.869
they're all just examples the kinds of
它们是我们要看的一些案例
16
00:00:45.869 --> 00:00:48.329
case studies we'll look at and the

17
00:00:48.329 --> 00:00:49.799
reason why all this is important is that
这些很重要的原因就是
18
00:00:49.799 --> 00:00:51.600
a lot of critical infrastructure out
许多关键的基础设施都是
19
00:00:51.600 --> 00:00:53.490
there is built out of distributed
基于分布式系统构建的
20
00:00:53.490 --> 00:00:56.250
systems infrastructure that requires
这些基础的设施需要
21
00:00:56.250 --> 00:00:57.570
more than one computer to get its job
很多计算机去完成工作
22
00:00:57.570 --> 00:00:59.579
done or it's sort of inherently needs to
或者说 这是一种内在的需求需要被散布
23
00:01:06.299 --> 00:01:08.189
all before I even talk about distributed
讨论分布式系统
24
00:01:08.189 --> 00:01:10.500
systems sort of remind you that you know
就是想告诉你
25
00:01:10.500 --> 00:01:12.420
if you're designing a system redesigning
如果你要设计一个系统
26
00:01:12.420 --> 00:01:14.280
you need to solve some problem if you
或者设计去解决一些问题   如果你
27
00:01:14.280 --> 00:01:16.409
can possibly solve it on a single
可以在一台计算机上解决这些问题
28
00:01:16.409 --> 00:01:18.569
computer you know without building a
而不需要构建一个分布式系统  友们多的任务都可以再单节点上做
而且总是比分布式的简单
29
00:01:29.219 --> 00:01:30.780
know you should try everything else
所以你应该尽力去做
30
00:01:30.780 --> 00:01:32.400
before you try building distributed
  再建立分布式的高性能的
31
00:01:41.640 --> 00:01:43.349
high-performance and the way to think
他们需要高性能
32
00:01:43.349 --> 00:01:45.180
about that is they want to get achieve
他们想实现
33
00:01:45.180 --> 00:01:50.519
some sort of parallelism lots of CPUs
一些cpu、内存、磁盘的的并行使用
34
00:01:50.519 --> 00:01:52.409
lots of memories lots of disk arms

35
00:01:52.409 --> 00:01:56.579
moving in parallel another reason why
 另一个原因 人们为什么
36
00:01:56.579 --> 00:01:58.290
people build this stuff is to be able to
建立分布式主要是能够容错  以及
37
00:02:05.310 --> 00:02:07.900
have two computers do the exact same
有两台电脑做完全一样的事情
38
00:02:07.900 --> 00:02:09.580
thing if one of them fails you can cut
如果 一个失败了 你可以切换到
39
00:02:09.580 --> 00:02:12.610
over to the other one another is that
 另外一个上
40
00:02:17.560 --> 00:02:19.419
do interbank transfers of money or
你想做银行间的金钱交易或者其他
41
00:02:19.419 --> 00:02:21.819
something well you know bank a has this
well 银行a 有一个
42
00:02:21.819 --> 00:02:23.979
computer in New York City and Bank B as
电脑在纽约   银行B
43
00:02:23.979 --> 00:02:26.110
this computer in London you know you
在伦敦     你知道
44 
00:02:26.110 --> 00:02:27.789
just have to have some way for them to
这需要 有一些方法 
45
00:02:27.789 --> 00:02:29.710
talk to each other and cooperate in
去让他们彼此通信
46
00:02:29.710 --> 00:02:31.330
order to carry that out so there's some

47
00:02:31.330 --> 00:02:36.310
natural sort of physical reasons systems

48
00:02:36.310 --> 00:02:37.360
that are inherently physically

49
00:02:44.319 --> 00:02:46.659
often by if there's some code you don't

50
00:02:50.919 --> 00:02:53.169
be immediate malicious or maybe their

51
00:02:53.169 --> 00:02:55.419
code has bugs in it so you don't want to

52
00:02:55.419 --> 00:02:57.159
have to trust it you may want to split

53
00:02:57.159 --> 00:02:59.349
up the computation so you know your

54
00:03:02.500 --> 00:03:04.150
they only talk to each other to some

55
00:03:04.150 --> 00:03:06.580
sort of narrow narrowly defined network

56
00:03:06.580 --> 00:03:10.330
protocol assuming we may be worried

57
00:03:10.330 --> 00:03:13.569
about you know security and that's

58
00:03:13.569 --> 00:03:14.979
achieved by splitting things up into

59
00:03:14.979 --> 00:03:16.419
multiple computers so that they can be

60
00:03:16.419 --> 00:03:21.460
isolated the most of this course is

61
00:03:21.460 --> 00:03:23.919
going to be about performance and fault

62
00:03:23.919 --> 00:03:26.409
tolerance although the other two often

63
00:03:26.409 --> 00:03:28.629
work themselves in by way of the sort of

64
00:03:32.800 --> 00:03:34.389
these distributed systems so these

65
00:03:34.389 --> 00:03:36.430
problems are because they have many

66
00:03:36.430 --> 00:03:39.810
parts and the parts execute concurrently

67
00:03:39.810 --> 00:03:42.219
because there are multiple computers you

68
00:03:42.219 --> 00:03:43.509
get all the problems that come up with

69
00:03:43.509 --> 00:03:45.189
concurrent programming all the sort of

70
00:03:45.189 --> 00:03:46.719
complex interactions and we're

71
00:03:46.719 --> 00:03:49.659
timing-dependent stuff and that's part

72
00:03:49.659 --> 00:03:51.780
of what makes distributed systems hard

73
00:03:51.780 --> 00:03:54.340
another thing that makes distributed

74
00:03:54.340 --> 00:03:56.919
systems hard is that because again you

75
00:03:56.919 --> 00:03:59.409
have multiple pieces plus a network you

76
00:03:59.409 --> 00:04:02.439
can have very unexpected failure

77
00:04:02.439 --> 00:04:04.539
patterns that is if you have a single

78
00:04:04.539 --> 00:04:06.280
computer it's usually the case either

79
00:04:06.280 --> 00:04:08.349
computer works or maybe it crashes or

80
00:04:12.280 --> 00:04:14.469
work distributed systems made up of lots

81
00:04:14.469 --> 00:04:15.939
of computers you can have partial

82
00:04:15.939 --> 00:04:18.399
failures that is some pieces stopped

83
00:04:18.399 --> 00:04:20.139
working other people other pieces

84
00:04:20.139 --> 00:04:22.449
continue working or maybe the computers

85
00:04:22.449 --> 00:04:24.279
are working but some part of the network

86
00:04:30.870 --> 00:04:50.230
distributed systems are hard and a final

87
00:04:50.230 --> 00:04:51.879
reason why it's hard is that you know

88
00:04:51.879 --> 00:04:53.290
them the original reason to build the

89
00:04:53.290 --> 00:04:54.779
distributed system is often to get

90
00:04:54.779 --> 00:04:57.610
higher performance to get you know a

91
00:04:57.610 --> 00:04:59.379
thousand computers worth of performance

92
00:04:59.379 --> 00:05:01.660
or a thousand disk arms were the

93
00:05:01.660 --> 00:05:03.579
performance but it's actually very

94
00:05:03.579 --> 00:05:06.819
tricky to obtain that thousand X speed

95
00:05:06.819 --> 00:05:09.310
up with a thousand computers there's

96
00:05:22.959 --> 00:05:24.269
you the performance you feel you deserve

97
00:05:24.269 --> 00:05:26.439
so solving these problems of course

98
00:05:26.439 --> 00:05:27.689
going to be all about you know

99
00:05:27.689 --> 00:05:31.959
addressing these issues the reason to

100
00:05:31.959 --> 00:05:33.730
take the course is because often the

101
00:05:33.730 --> 00:05:35.410
problems and the solutions are quite

102
00:05:35.410 --> 00:05:38.319
just technically interesting they're

103
00:05:38.319 --> 00:05:40.329
hard problems for some of these problems

104
00:05:40.329 --> 00:05:42.639
there's pretty good solutions known for

105
00:05:42.639 --> 00:05:44.500
other problems they're not such great

106
00:05:44.500 --> 00:05:47.740
solutions now distributed systems are

107
00:05:47.740 --> 00:05:50.920
used by a lot of real-world systems out

108
00:05:50.920 --> 00:05:53.350
there like big websites often involved

109
00:05:53.350 --> 00:05:55.240
you know vast numbers computers that are

110
00:05:55.240 --> 00:05:57.970
you know put together as distributed

111
00:05:57.970 --> 00:06:00.430
systems when I first started teaching

112
00:06:00.430 --> 00:06:03.490
this course it was distributed systems

113
00:06:03.490 --> 00:06:05.230
were something of an academic curiosity

114
00:06:05.230 --> 00:06:07.660
you know people thought oh you know at a

115
00:06:07.660 --> 00:06:09.759
small scale they were used sometimes and

116
00:06:14.740 --> 00:06:16.689
driven by the rise of giant websites

117
00:06:16.689 --> 00:06:18.970
that have you know vast amounts of data

118
00:06:18.970 --> 00:06:21.959
and entire warehouses full of computers

119
00:06:21.959 --> 00:06:23.649
distributed systems in the last

120
00:06:23.649 --> 00:06:25.860
twenty years have gotten to be very

121
00:06:25.860 --> 00:06:29.259
seriously important part of computing

122
00:06:29.259 --> 00:06:32.889
infrastructure this means that there's

123
00:06:32.889 --> 00:06:34.480
been a lot of attention paid to them a

124
00:06:34.480 --> 00:06:36.250
lot of problems have been solved but

125
00:06:36.250 --> 00:06:37.689
there's still quite a few unsolved

126
00:06:37.689 --> 00:06:39.939
problems so if you're a graduate student

127
00:06:39.939 --> 00:06:42.490
or you're interested in research there's

128
00:06:42.490 --> 00:06:45.310
a lot to you let a lot of problems yet

129
00:06:45.310 --> 00:06:47.290
to be solved in distributed systems that

130
00:06:47.290 --> 00:06:49.720
you could look into his research and

131
00:06:49.720 --> 00:06:51.639
finally if you like building stuff this

132
00:06:51.639 --> 00:06:54.220
is a good class because it has a lab

133
00:06:58.689 --> 00:07:00.610
focused on performance and fault

134
00:07:00.610 --> 00:07:01.180
tolerance

135
00:07:01.180 --> 00:07:04.600
so you've got a lot of practice building

136
00:07:04.600 --> 00:07:06.790
districts just building distributed

137
00:07:06.790 --> 00:07:09.550
systems and making them work all right

138
00:07:09.550 --> 00:07:12.420
let me talk about course structure a bit

139
00:07:12.420 --> 00:07:16.540
before I get started on real technical

140
00:07:22.779 --> 00:07:24.819
course website is the lab assignments

141
00:07:24.819 --> 00:07:28.149
and course schedule and also link to a

142
00:07:28.149 --> 00:07:31.209
Piazza page where you can post questions

143
00:07:31.209 --> 00:07:35.319
get answers the course staff I'm Robert

144
00:07:35.319 --> 00:07:36.970
Morris I'll be giving the lectures I

145
00:07:36.970 --> 00:07:39.250
also have four TAs you guys want to stand

146
00:07:39.250 --> 00:07:44.350
up and show your faces the TAs are

147
00:07:44.350 --> 00:07:47.949
experts at in particular at doing the

148
00:07:47.949 --> 00:07:49.689
solving the labs they'll be holding

149
00:07:49.689 --> 00:07:51.399
office hours so if you have questions

150
00:07:55.360 --> 00:07:59.649
questions to Piazza the course has a

151
00:08:09.100 --> 00:08:16.379
every lecture there's two exams

152
00:08:17.790 --> 00:08:22.649
there's the labs programming labs and

153
00:08:22.649 --> 00:08:25.480
there's an optional final project that

154
00:08:25.480 --> 00:08:28.740
you can do instead of one of the labs

155
00:08:43.809 --> 00:08:47.139
sort of lab programming stuff a lot of

156
00:08:47.139 --> 00:08:48.639
our lectures will be taken up by case

157
00:08:48.639 --> 00:08:50.590
studies a lot of the way that I sort of

158
00:08:50.590 --> 00:08:53.649
try to bring out the content of

159
00:08:53.649 --> 00:08:55.210
distributed systems is by looking at

160
00:08:55.210 --> 00:08:58.110
papers some academics some written by

161
00:08:58.110 --> 00:09:01.960
people in industry describing real

162
00:09:01.960 --> 00:09:05.100
solutions to real problems

163
00:09:05.590 --> 00:09:07.590
these lectures actually be videotaped

164
00:09:07.590 --> 00:09:10.269
and I'm hoping to post them online so

165
00:09:10.269 --> 00:09:12.940
that you can so if you're not here or

166
00:09:12.940 --> 00:09:15.279
you want to review the lectures you'll

167
00:09:15.279 --> 00:09:16.269
be able to look at the videotape

168
00:09:16.269 --> 00:09:20.110
lectures the papers again there's one to

169
00:09:24.610 --> 00:09:26.440
today's paper which I hope some of you

170
00:09:26.440 --> 00:09:28.389
have read on MapReduce it's an old paper

171
00:09:28.389 --> 00:09:31.389
but it was the beginning of its spurred

172
00:09:31.389 --> 00:09:33.309
an enormous amount of interesting work

173
00:09:33.309 --> 00:09:35.860
both academic and in the real world so

174
00:09:41.500 --> 00:09:44.529
currently worried about and from the

175
00:09:49.120 --> 00:09:50.529
have had that might or might not be

176
00:09:50.529 --> 00:09:52.179
useful in solving distributed system

177
00:09:52.179 --> 00:09:54.970
problems we'll be looking at sometimes

178
00:09:54.970 --> 00:09:56.649
in implementation details in some of

179
00:09:56.649 --> 00:09:58.720
these papers because a lot of this has

180
00:09:58.720 --> 00:10:01.120
to do with actual construction of of

181
00:10:01.120 --> 00:10:03.429
software based systems and we're also

182
00:10:03.429 --> 00:10:04.899
going to spend a certain time looking at

183
00:10:04.899 --> 00:10:07.539
evaluations people evaluating how fault

184
00:10:07.539 --> 00:10:09.309
tolerant their systems by measuring them

185
00:10:09.309 --> 00:10:11.200
or people measuring how much performance

186
00:10:11.200 --> 00:10:12.789
or whether they got performance

187
00:10:12.789 --> 00:10:17.950
improvement at all so I'm hoping that

188
00:10:17.950 --> 00:10:19.809
you'll read the papers before coming to

189
00:10:19.809 --> 00:10:22.659
class the lectures are maybe not going

190
00:10:22.659 --> 00:10:24.220
to make as much sense if you haven't

191
00:10:24.220 --> 00:10:26.110
already read the lecture because there's

192
00:10:26.110 --> 00:10:28.210
not enough time to both explaining all

193
00:10:28.210 --> 00:10:30.820
the content of the paper and have a sort

194
00:10:30.820 --> 00:10:32.830
of interesting reflection on what the

195
00:10:37.210 --> 00:10:38.620
into class and hopefully one of the

196
00:10:42.470 --> 00:10:44.940
and skip over the parts that maybe

197
00:10:44.940 --> 00:10:47.429
aren't that important and sort of focus

198
00:10:47.429 --> 00:10:51.360
on teasing out the important ideas on

199
00:10:51.360 --> 00:10:53.730
the website there's for every link to

200
00:10:53.730 --> 00:10:56.730
by the schedule there's a question that

201
00:10:56.730 --> 00:10:59.159
you should submit an answer for

202
00:10:59.159 --> 00:11:00.750
every paper I think the answers are due

203
00:11:00.750 --> 00:11:02.789
at midnight and we also ask that you

204
00:11:09.389 --> 00:11:11.279
I'm preparing the lecture and if I have

205
00:11:11.279 --> 00:11:13.889
time I'll try to answer at least a few

206
00:11:13.889 --> 00:11:17.279
of the questions by email and the

207
00:11:17.279 --> 00:11:18.600
question and the answer for each paper

208
00:11:18.600 --> 00:11:22.139
due midnight the night before there's

209
00:11:22.139 --> 00:11:24.659
two exams there's a midterm exam in

210
00:11:24.659 --> 00:11:26.850
class I think on the last class meeting

211
00:11:26.850 --> 00:11:32.639
before spring break and there's a final

212
00:11:37.769 --> 00:11:42.120
focus mostly on papers and the labs and

213
00:11:42.120 --> 00:11:44.129
probably the best way to prepare for

214
00:11:44.129 --> 00:11:46.320
them as well as attending lecture and

215
00:11:46.320 --> 00:11:49.230
reading the papers a good way to prepare

216
00:11:49.230 --> 00:11:51.360
for the exams is to look at all exams we

217
00:11:51.360 --> 00:11:55.679
have links to 20 years of old exams and

218
00:11:55.679 --> 00:11:57.240
solutions and so you look at those and

219
00:11:57.240 --> 00:11:58.710
sort of get a feel for what kind of

220
00:11:58.710 --> 00:12:01.350
questions that I like to ask and indeed

221
00:12:01.350 --> 00:12:03.169
because we read many of the same papers

222
00:12:03.169 --> 00:12:05.549
inevitably I ask questions each year

223
00:12:05.549 --> 00:12:08.909
that can't help but resemble questions

224
00:12:08.909 --> 00:12:15.419
asked in previous years the labs there's

225
00:12:15.419 --> 00:12:17.490
four programming labs the first one of

226
00:12:17.490 --> 00:12:25.649
them is due Friday next week lab one is

227
00:12:25.649 --> 00:12:31.980
a simple MapReduce lab to implement your

228
00:12:31.980 --> 00:12:33.809
own version of the paper they write

229
00:12:33.809 --> 00:12:35.100
today in which I'll be discussing in a

230
00:12:40.320 --> 00:12:43.620
raft in order to get fault taught in

231
00:12:43.620 --> 00:12:47.190
order to sort of allow in theory allow

232
00:12:47.190 --> 00:12:49.679
any system to be made fault tolerant by

233
00:12:49.679 --> 00:12:51.210
replicating it and having this raft

234
00:12:51.210 --> 00:12:53.850
technique manage the replication and

235
00:12:53.850 --> 00:12:55.250
manage sort of automatic cut over

236
00:12:55.250 --> 00:12:57.659
if there's a fault if one of the

237
00:12:57.659 --> 00:13:00.149
replicated servers fails so this is raft for

238
00:13:00.149 --> 00:13:08.700
fault tolerance in lad 3 you'll use your

239
00:13:08.700 --> 00:13:11.370
raft implementation in order to build a

240
00:13:11.370 --> 00:13:18.990
fault tolerant key-value server it'll be

241
00:13:18.990 --> 00:13:22.639
replicated and fault tolerant in a lab 4

242
00:13:22.639 --> 00:13:25.950
you'll take your replicated key-value

243
00:13:25.950 --> 00:13:28.200
server and clone it into a number of

244
00:13:28.200 --> 00:13:30.389
independent groups and you'll split the

245
00:13:30.389 --> 00:13:33.870
data in your key value storage system

246
00:13:33.870 --> 00:13:35.159
across all of these individual

247
00:13:35.159 --> 00:13:36.779
replicated groups to get parallel

248
00:13:36.779 --> 00:13:39.659
speed-up by running multiple replicated

249
00:13:39.659 --> 00:13:42.240
groups in parallel and you'll also be

250
00:13:42.240 --> 00:13:47.789
responsible for moving the various

251
00:13:52.500 --> 00:13:54.809
balls so this is a what's often called a

252
00:13:54.809 --> 00:14:03.330
sharded key value service sharding

253
00:14:03.330 --> 00:14:04.830
refers to splitting up the data

254
00:14:04.830 --> 00:14:07.409
partitioning the data among multiple

255
00:14:07.409 --> 00:14:10.289
servers in order to get parallel speed

256
00:14:10.289 --> 00:14:16.610
up if you want instead of doing lab 4

257
00:14:16.610 --> 00:14:19.740
you can do a project of your own choice

258
00:14:19.740 --> 00:14:21.480
and the idea here is if you have some

259
00:14:21.480 --> 00:14:23.700
idea for a distributed system you know

260
00:14:27.330 --> 00:14:28.529
you have your own idea that you want to

261
00:14:28.529 --> 00:14:30.299
pursue and you like to build something

262
00:14:30.299 --> 00:14:32.100
and measure whether it worked in order

263
00:14:32.100 --> 00:14:34.500
to explore your idea you can do a

264
00:14:34.500 --> 00:14:38.370
project and so for a project you'll pick

265
00:14:38.370 --> 00:14:40.169
some teammates because we require that

266
00:14:50.940 --> 00:14:53.580
maybe give you some advice and then if

267
00:14:53.580 --> 00:14:55.230
you go ahead and do if we say yes and

268
00:14:55.230 --> 00:14:56.610
you want to do a project you do that and

269
00:14:56.610 --> 00:14:59.159
instead of lab 4 and it's due at the end

270
00:14:59.159 --> 00:15:00.870
of the semester and you know you'll you

271
00:15:00.870 --> 00:15:05.250
should do some design work and build a

272
00:15:05.250 --> 00:15:06.960
real system and then in the last day of

273
00:15:06.960 --> 00:15:08.940
class you'll demonstrate your system

274
00:15:08.940 --> 00:15:11.370
as well as handing in a short sort of

275
00:15:11.370 --> 00:15:12.899
written report to us about what you

276
00:15:12.899 --> 00:15:17.730
built and I posted on the website some

277
00:15:22.350 --> 00:15:25.139
about what projects you might build but

278
00:15:25.139 --> 00:15:27.690
really the best projects are one where

279
00:15:32.700 --> 00:15:34.919
do a project you should choose an idea

280
00:15:34.919 --> 00:15:36.809
that's sort of in the same vein as the

281
00:15:36.809 --> 00:15:39.149
systems that were talked about in this

282
00:15:39.149 --> 00:15:40.639
class

283
00:15:47.940 --> 00:15:49.710
you're great early based on how many

284
00:15:49.710 --> 00:15:51.870
tests you pass we give you all the tests

285
00:15:51.870 --> 00:15:55.169
that we use those no hidden tests so if

286
00:15:55.169 --> 00:15:56.850
you implement the lab and it reliably

287
00:15:56.850 --> 00:15:58.950
passes all the tests and chances are

288
00:15:58.950 --> 00:16:00.750
good unless there's something funny

289
00:16:00.750 --> 00:16:02.309
going on which there sometimes is

290
00:16:02.309 --> 00:16:04.649
chances are good that if you your coop

291
00:16:07.559 --> 00:16:10.320
you'll get a four score full score so

292
00:16:10.320 --> 00:16:11.519
hopefully there'll be no mystery about

293
00:16:11.519 --> 00:16:13.830
what score you're likely to get on the

294
00:16:13.830 --> 00:16:18.779
labs let me warn you that debugging

295
00:16:18.779 --> 00:16:21.929
these labs can be time-consuming because

296
00:16:21.929 --> 00:16:23.549
they're distributed systems and a lot of

297
00:16:23.549 --> 00:16:26.820
concurrency and communication sort of

298
00:16:37.379 --> 00:16:39.210
of trouble if you be elapsed to the last

299
00:16:39.210 --> 00:16:41.370
moment you got to start early if your

300
00:16:41.370 --> 00:16:43.679
problems please come to the TAs office

301
00:16:43.679 --> 00:16:45.779
hours and please feel free to ask

302
00:16:51.269 --> 00:16:52.769
that you'll answer people's questions on

303
00:16:52.769 --> 00:16:56.340
Piazza as well all right any questions

304
00:16:56.340 --> 00:17:04.759
about the mechanics of the course yes

305
00:17:10.339 --> 00:17:13.140
so the question is what is how does how

306
00:17:13.140 --> 00:17:15.329
do the different factor these things

307
00:17:15.329 --> 00:17:17.549
factoring the grade I forget but it's

308
00:17:17.549 --> 00:17:20.180
all on the it's on the website under

309
00:17:20.180 --> 00:17:24.900
something I think though it's the labs

310
00:17:24.900 --> 00:17:29.569
are the single most important component

311
00:17:29.569 --> 00:17:36.349
okay alright so this is a course about

312
00:17:36.349 --> 00:17:39.779
about infrastructure for applications

313
00:17:39.779 --> 00:17:41.460
and so all through this course there's

314
00:17:41.460 --> 00:17:42.809
going to be a sort of split in the way I

315
00:17:42.809 --> 00:17:45.180
talk about things between applications

316
00:17:45.180 --> 00:17:47.549
which are sort of other people the

317
00:17:47.549 --> 00:17:49.980
customer somebody else writes but the

318
00:17:49.980 --> 00:17:51.390
applications are going to use the

319
00:17:51.390 --> 00:17:53.160
infrastructure that we're thinking about

320
00:17:53.160 --> 00:17:55.740
in this course and so the kinds of

321
00:17:55.740 --> 00:17:58.950
infrastructure that tend to come up a

322
00:17:58.950 --> 00:18:13.619
lot our storage communication and

323
00:18:13.619 --> 00:18:16.920
computation and we'll talk about systems

324
00:18:23.369 --> 00:18:24.900
out that storage is going to be the one

325
00:18:24.900 --> 00:18:27.990
we focus most on because it's a very

326
00:18:27.990 --> 00:18:30.980
well-defined and useful abstraction and

327
00:18:30.980 --> 00:18:32.819
usually fairly straightforward

328
00:18:32.819 --> 00:18:34.319
abstraction so people know a lot about

329
00:18:34.319 --> 00:18:36.230
how to build how to use and build

330
00:18:36.230 --> 00:18:40.349
storage systems and how to build sort of

331
00:18:40.349 --> 00:18:41.670
replicated fault tolerant

332
00:18:41.670 --> 00:18:43.680
high-performance distributed

333
00:18:43.680 --> 00:18:46.410
implementations of storage we'll also

334
00:18:46.410 --> 00:18:48.720
talk about some some of our computation

335
00:18:48.720 --> 00:18:50.970
systems like MapReduce for today is a

336
00:18:50.970 --> 00:18:54.750
computation system and we will talk

337
00:18:54.750 --> 00:18:57.119
about communications some but mostly

338
00:18:57.119 --> 00:18:58.710
from the point is a tool that we need to

339
00:18:58.710 --> 00:19:00.509
use to build distributed systems like

340
00:19:00.509 --> 00:19:01.980
computers have to talk to each other

341
00:19:01.980 --> 00:19:03.750
over a network you know maybe you need

342
00:19:03.750 --> 00:19:06.329
reliability or something and so we'll

343
00:19:06.329 --> 00:19:08.670
talk a bit about what we're actually

344
00:19:08.670 --> 00:19:11.970
mostly consumers of communication if you

345
00:19:11.970 --> 00:19:12.980
want to learn about communication

346
00:19:20.779 --> 00:19:24.750
so for storage and computation a lot of

347
00:19:24.750 --> 00:19:27.200
our goal is to be able to discover

348
00:19:27.200 --> 00:19:31.619
abstractions where use of simplifying

349
00:19:31.619 --> 00:19:34.440
the interface to these storage and

350
00:19:34.440 --> 00:19:36.660
computation distributed storage and

351
00:19:36.660 --> 00:19:38.759
computation infrastructure so that it's

352
00:19:38.759 --> 00:19:40.859
easy to build applications on top of it

353
00:19:40.859 --> 00:19:43.500
and what that really means is that we

354
00:19:43.500 --> 00:19:45.269
need to we'd like to be able to build

355
00:19:45.269 --> 00:19:47.369
abstraction that hide the distributed

356
00:19:47.369 --> 00:19:51.240
nature of these of these systems so the

357
00:19:51.240 --> 00:19:54.299
dream which is rarely fully achieved but

358
00:19:54.299 --> 00:19:56.579
the dream would be to be able to build

359
00:19:56.579 --> 00:19:58.710
an interface that looks to an

360
00:19:58.710 --> 00:20:00.630
application is if it's a non-distributed

361
00:20:00.630 --> 00:20:02.309
storage system just like a file system

362
00:20:02.309 --> 00:20:03.750
or something that everybody already

363
00:20:03.750 --> 00:20:05.279
knows how to program and has a pretty

364
00:20:09.990 --> 00:20:13.769
act just like non-distributed storage

365
00:20:13.769 --> 00:20:17.599
and computation systems but are actually

366
00:20:17.599 --> 00:20:20.369
you know vast extremely high performance

367
00:20:20.369 --> 00:20:22.589
fault tolerant distributed systems

368
00:20:22.589 --> 00:20:27.890
underneath so we both have abstractions

369
00:20:37.950 --> 00:20:39.809
the way there it's rare that you find an

370
00:20:39.809 --> 00:20:41.819
abstraction for a distributed version of

371
00:20:41.819 --> 00:20:44.730
storage or computation that has simple

372
00:20:44.730 --> 00:20:49.109
behavior behave just like the non just

373
00:20:49.109 --> 00:20:51.119
non-distributed version of storage that

374
00:20:51.119 --> 00:20:52.920
everybody understands but people getting

375
00:20:52.920 --> 00:20:59.640
better at this and we're gonna try to

376
00:20:59.640 --> 00:21:01.680
study the ways and what people have

377
00:21:01.680 --> 00:21:03.859
learned about building such abstractions

378
00:21:03.859 --> 00:21:08.759
ok so what kind of what kind of topics

379
00:21:08.759 --> 00:21:10.170
show up is we're considering these

380
00:21:10.170 --> 00:21:13.589
abstractions the first one this first

381
00:21:13.589 --> 00:21:15.839
topic general topic that we'll see a lot

382
00:21:15.839 --> 00:21:18.690
a lot of the systems we looked at have

383
00:21:18.690 --> 00:21:24.920
to do with implementation so for example

384
00:21:24.920 --> 00:21:27.569
the kind of tools that you see a lot for

385
00:21:27.569 --> 00:21:30.150
for ways people learn how to build these

386
00:21:30.150 --> 00:21:31.650
systems are things like remote procedure

387
00:21:31.650 --> 00:21:32.589
call

388
00:21:32.589 --> 00:21:35.529
whose goal is to mask the fact that

389
00:21:35.529 --> 00:21:36.970
we're communicating over an unreliable

390
00:21:36.970 --> 00:21:44.849
Network another kind of implementation

391
00:21:44.849 --> 00:21:49.230
topic that we'll see a lot is threads

392
00:21:49.230 --> 00:21:51.940
which are a programming technique that

393
00:21:56.859 --> 00:21:58.750
more important for this class threads

394
00:21:58.750 --> 00:22:00.309
are a way of structuring concurrent

395
00:22:10.329 --> 00:22:12.279
we're going to need to also you know

396
00:22:12.279 --> 00:22:13.779
just as from an implementation level

397
00:22:13.779 --> 00:22:15.160
spend a certain amount of time thinking

398
00:22:15.160 --> 00:22:16.839
about concurrency control things like

399
00:22:16.839 --> 00:22:25.180
locks and the main place that these

400
00:22:25.180 --> 00:22:26.589
implementation ideas will come up in the

401
00:22:26.589 --> 00:22:28.599
class they'll be touched on in many of

402
00:22:28.599 --> 00:22:30.190
the papers but you're gonna come face

403
00:22:30.190 --> 00:22:31.869
the face of all this in a big way in the

404
00:22:31.869 --> 00:22:34.210
labs you need to build distributed you

405
00:22:34.210 --> 00:22:35.710
know do the programming for distributed

406
00:22:50.279 --> 00:22:54.130
another big topic that comes up in all

407
00:22:54.130 --> 00:22:55.390
the papers we're going to talk about is

408
00:22:55.390 --> 00:23:05.440
performance usually the high-level goal

409
00:23:05.440 --> 00:23:07.569
of building a distributed system is to

410
00:23:07.569 --> 00:23:11.849
get what people call scalable speed-up

411
00:23:11.849 --> 00:23:17.460
so we're looking for scalability and

412
00:23:23.529 --> 00:23:26.109
that I'm solving with one computer and I

413
00:23:26.109 --> 00:23:29.559
buy a second computer to help me execute

414
00:23:29.559 --> 00:23:31.779
my problem if I can now solve the

415
00:23:37.450 --> 00:23:39.849
per minute on two computers as I had on

416
00:23:39.849 --> 00:23:42.339
one then that's an example of

417
00:23:42.339 --> 00:23:44.319
scalability so

418
00:23:44.319 --> 00:23:47.559
sort of two times you know computers or

419
00:24:02.920 --> 00:24:05.170
that actually has this behavior Namie

420
00:24:05.170 --> 00:24:07.329
that if you increase the number of

421
00:24:07.329 --> 00:24:08.829
computers you throw at the problem by

422
00:24:08.829 --> 00:24:12.190
some factor you get that factor more

423
00:24:12.190 --> 00:24:14.650
throughput more performance out of the

424
00:24:14.650 --> 00:24:17.890
system that's a huge win because you can

425
00:24:28.630 --> 00:24:31.380
programmers to restructure your software

426
00:24:31.380 --> 00:24:33.549
to get better performance to make it

427
00:24:33.549 --> 00:24:35.920
more efficient or to apply some sort of

428
00:24:35.920 --> 00:24:37.900
specialized techniques better algorithms

429
00:24:37.900 --> 00:24:39.970
or something if you have to pay

430
00:24:39.970 --> 00:24:42.940
programmers to fix your code to be

431
00:24:42.940 --> 00:24:45.460
faster that's an expensive way to go

432
00:24:45.460 --> 00:24:47.589
we'd love to be able just oh by thousand

433
00:24:47.589 --> 00:24:49.690
computers instead of ten computers and

434
00:24:49.690 --> 00:24:51.609
get a hundred times more throughput

435
00:24:51.609 --> 00:24:53.740
that's fanTAstic and so this sort of

436
00:24:53.740 --> 00:24:56.829
scalability idea is a huge idea in the

437
00:24:59.559 --> 00:25:01.599
that run on are you know building full

438
00:25:01.599 --> 00:25:04.509
of computers if the building full of

439
00:25:04.509 --> 00:25:06.670
computers is there to get a sort of

440
00:25:06.670 --> 00:25:09.609
corresponding amount of performance but

441
00:25:13.450 --> 00:25:19.779
performance so often the way this looks

442
00:25:19.779 --> 00:25:21.910
when we're looking at diagrams or I'm

443
00:25:21.910 --> 00:25:23.529
writing diagrams in this course is that

444
00:25:23.529 --> 00:25:25.480
I'm not supposing we're building a

445
00:25:25.480 --> 00:25:27.700
website ordinarily you might have a

446
00:25:36.900 --> 00:25:42.190
many web browsers and they talk to a web

447
00:25:42.190 --> 00:25:44.890
server running Python or PHP or whatever

448
00:25:44.890 --> 00:25:49.299
sort of web server and the web server

449
00:25:49.299 --> 00:25:52.740
talks to some kind of database

450
00:25:54.230 --> 00:25:56.490
you know when you have one or two users

451
00:25:56.490 --> 00:25:58.619
you can just have one computer running

452
00:25:58.619 --> 00:26:00.720
both and maybe a computer for the web

453
00:26:00.720 --> 00:26:02.400
server and a computer from the database

454
00:26:02.400 --> 00:26:03.839
but maybe all of a sudden you get really

455
00:26:03.839 --> 00:26:05.339
proper popular and you'll be up and

456
00:26:05.339 --> 00:26:08.579
you've you know 100 million people sign

457
00:26:08.579 --> 00:26:13.500
up your service ID how do you how do you

458
00:26:13.500 --> 00:26:15.180
fix your c-certainly can it support

459
00:26:15.180 --> 00:26:17.900
millions of people on a single computer

460
00:26:17.900 --> 00:26:20.640
except by extremely careful

461
00:26:20.640 --> 00:26:24.630
labor-intensive optimization but you

462
00:26:24.630 --> 00:26:27.779
don't have time for so typically the way

463
00:26:27.779 --> 00:26:29.519
you're going to speed things up the

464
00:26:29.519 --> 00:26:31.109
first thing you do is buy more web

465
00:26:31.109 --> 00:26:33.539
servers and just split the user so that

466
00:26:33.539 --> 00:26:35.579
you know how few users or some fraction

467
00:26:35.579 --> 00:26:37.230
the user go to a web server 1 and the

468
00:26:37.230 --> 00:26:39.750
other half you send them to a web server

469
00:26:39.750 --> 00:26:45.960
2 and because maybe you're building I

470
00:26:45.960 --> 00:26:47.759
don't know what reddit or something

471
00:26:47.759 --> 00:26:49.440
where all the users need to see the same

472
00:26:49.440 --> 00:26:51.210
data ultimately you have all the web

473
00:26:55.559 --> 00:27:01.680
time here and so this is a way of

474
00:27:04.259 --> 00:27:05.910
PHP or Python maybe it's not too

475
00:27:05.910 --> 00:27:09.569
efficient as long as each individual web

476
00:27:09.569 --> 00:27:11.490
server doesn't put too much load on the

477
00:27:11.490 --> 00:27:12.839
database you can add a lot of web

478
00:27:12.839 --> 00:27:17.700
servers before you run into problems but

479
00:27:17.700 --> 00:27:20.670
this kind of scalability is rarely

480
00:27:20.670 --> 00:27:23.609
infinite unfortunately certainly not

481
00:27:23.609 --> 00:27:25.289
without serious thought and so what

482
00:27:25.289 --> 00:27:26.700
tends to happen with these systems is

483
00:27:26.700 --> 00:27:29.309
that at some point after you have 10 or

484
00:27:29.309 --> 00:27:31.380
20 or 100 web servers all talking to the

485
00:27:31.380 --> 00:27:33.450
same database now all of a sudden the

486
00:27:38.910 --> 00:27:42.710
ability to sort of infinite numbers of

487
00:27:42.710 --> 00:27:44.849
adding infinite numbers of computers

488
00:27:44.849 --> 00:27:46.710
some point you run out of gas because

489
00:27:46.710 --> 00:27:48.599
the place at which you are adding more

490
00:27:48.599 --> 00:27:51.420
computers is no longer the bottleneck by

491
00:27:51.420 --> 00:27:52.740
having lots and lots of web servers we

492
00:27:52.740 --> 00:27:54.269
basically moved the bottleneck

493
00:27:54.269 --> 00:27:56.549
I think it's limiting performance from

494
00:27:56.549 --> 00:28:01.650
the web servers to the database and at

495
00:28:01.650 --> 00:28:03.450
this point actually you almost certainly

496
00:28:03.450 --> 00:28:05.730
have to do a bit of design work because

497
00:28:05.730 --> 00:28:07.589
it's rare that you can

498
00:28:07.589 --> 00:28:09.930
there's any straightforward way to take

499
00:28:09.930 --> 00:28:13.410
a single database and sort of refactor

500
00:28:13.410 --> 00:28:17.460
things with it or you can take data

501
00:28:17.460 --> 00:28:19.349
sorta in a single database and refactor

502
00:28:23.839 --> 00:28:26.839
but it's often a fair amount of work and

503
00:28:26.839 --> 00:28:29.309
because it's awkward but people many

504
00:28:33.390 --> 00:28:34.890
course in which the distributed system

505
00:28:34.890 --> 00:28:37.529
people are talking about is a storage

506
00:28:37.529 --> 00:28:40.859
system because the authors were running

507
00:28:40.859 --> 00:28:42.660
you know something like a big website

508
00:28:42.660 --> 00:28:45.809
that ran out of gas on a single database

509
00:28:45.809 --> 00:28:49.430
or storage servers anyway so the

510
00:28:49.430 --> 00:28:51.599
scalability story is we love to build

511
00:28:51.599 --> 00:28:56.329
systems that scale this way but you know

512
00:29:01.950 --> 00:29:11.880
infinitely far ok so another big topic

513
00:29:11.880 --> 00:29:16.250
that comes up a lot is fault tolerance

514
00:29:22.250 --> 00:29:24.690
if you're building a system with a

515
00:29:24.690 --> 00:29:27.450
single computer in it well a single

516
00:29:27.450 --> 00:29:29.549
computer often can stay up for years

517
00:29:29.549 --> 00:29:31.140
like I have servers in my office that

518
00:29:31.140 --> 00:29:33.529
have been up for years without crashing

519
00:29:33.529 --> 00:29:35.910
you know the computer is pretty reliable

520
00:29:35.910 --> 00:29:37.740
the operating systems reliable

521
00:29:37.740 --> 00:29:39.690
apparently the power in my building is

522
00:29:39.690 --> 00:29:41.640
pretty reliable so it's not uncommon to

523
00:29:41.640 --> 00:29:43.140
have single computers it's just stay up for

524
00:29:43.140 --> 00:29:46.589
amazing amount of time however if you're

525
00:29:46.589 --> 00:29:48.150
building systems out of thousands of

526
00:29:48.150 --> 00:29:50.819
computers then even if each computer can

527
00:29:50.819 --> 00:29:53.700
be expected to stay up for a year with a

528
00:29:53.700 --> 00:29:55.319
thousand computers that means you're

529
00:29:55.319 --> 00:29:57.119
going to have like about three computer

530
00:30:02.549 --> 00:30:04.380
problems with big distributed systems

531
00:30:04.380 --> 00:30:07.829
turns sort of very rare fault tolerance

532
00:30:07.829 --> 00:30:10.529
very rare failure very rare failure

533
00:30:10.529 --> 00:30:12.180
problems into failure problems that

534
00:30:12.180 --> 00:30:14.549
happen just all the time in a system

535
00:30:14.549 --> 00:30:15.809
with a thousand computers there's almost

536
00:30:24.839 --> 00:30:26.940
thing or maybe there's some piece of the

537
00:30:26.940 --> 00:30:28.890
network with a thousand computers we got

538
00:30:28.890 --> 00:30:31.230
a lot of network cables and a lot of

539
00:30:31.230 --> 00:30:33.599
network switches and so you know there's

540
00:30:37.200 --> 00:30:38.819
network cable that fell out or some

541
00:30:38.819 --> 00:30:40.740
networks which whose fan is broken and

542
00:30:40.740 --> 00:30:43.259
the switch overheated and failed there's

543
00:30:43.259 --> 00:30:44.849
always some little problem somewhere in

544
00:30:44.849 --> 00:30:48.769
your building sized distributed system

545
00:30:48.769 --> 00:30:52.559
so big scale turns problems from very

546
00:30:56.549 --> 00:30:59.279
problems that means the failure has to

547
00:31:03.720 --> 00:31:05.640
failures just has to be built into the

548
00:31:05.640 --> 00:31:08.900
design because there's always failures

549
00:31:08.900 --> 00:31:12.900
and you know it's part of building you

550
00:31:12.900 --> 00:31:14.460
know convenient abstractions for

551
00:31:14.460 --> 00:31:16.619
application programmers we really need

552
00:31:16.619 --> 00:31:17.640
that but to be able to build

553
00:31:17.640 --> 00:31:19.349
infrastructure that as much as possible

554
00:31:19.349 --> 00:31:21.900
hides the failures from application

555
00:31:21.900 --> 00:31:23.930
programmers or masks them or something

556
00:31:23.930 --> 00:31:26.460
so that every application programmer

557
00:31:37.829 --> 00:31:41.160
can have about what it means to be fault

558
00:31:41.160 --> 00:31:43.559
tolerant about a little more but you

559
00:31:43.559 --> 00:31:46.680
know exactly what we mean by that we'll

560
00:31:50.880 --> 00:31:58.349
one is availability so you know some

561
00:31:58.349 --> 00:32:01.470
systems are designed so that under some

562
00:32:01.470 --> 00:32:03.930
kind certain kinds of failures not all

563
00:32:03.930 --> 00:32:05.460
failures but certain kinds of failures

564
00:32:05.460 --> 00:32:09.119
the system will keep operating despite

565
00:32:09.119 --> 00:32:13.140
the failure while providing you know

566
00:32:13.140 --> 00:32:16.410
undamaged service the same kind of

567
00:32:16.410 --> 00:32:17.819
service it would have provided even if

568
00:32:17.819 --> 00:32:19.500
there had been no failure so some

569
00:32:19.500 --> 00:32:21.289
systems are available in that sense that

570
00:32:21.289 --> 00:32:24.150
up and up you know so if you build a

571
00:32:24.150 --> 00:32:25.950
replicated service that maybe has two

572
00:32:25.950 --> 00:32:28.910
copies you know one of the replicas

573
00:32:28.910 --> 00:32:31.769
replica servers fail fails maybe the

574
00:32:31.769 --> 00:32:34.529
other server can continue operating

575
00:32:40.529 --> 00:32:42.150
that case so available systems usually

576
00:32:42.150 --> 00:32:44.670
say well under certain set of failures

577
00:32:44.670 --> 00:32:46.140
we're going to continue providing

578
00:32:46.140 --> 00:32:48.450
service we're going to be available more

579
00:32:48.450 --> 00:32:50.789
failures than that occur it won't be

580
00:32:50.789 --> 00:32:52.730
available anymore

581
00:32:57.930 --> 00:32:59.130
availability or by itself as

582
00:32:59.130 --> 00:33:06.480
recoverability and what this means is

583
00:33:06.480 --> 00:33:08.160
that if something goes wrong maybe the

584
00:33:08.160 --> 00:33:10.200
service will stop working that it is

585
00:33:15.779 --> 00:33:17.430
along and repair or whatever went wrong

586
00:33:17.430 --> 00:33:19.650
but after the repair occurs the system

587
00:33:19.650 --> 00:33:21.900
will be able to continue as if nothing

588
00:33:21.900 --> 00:33:24.420
bad had gone wrong right so this is sort

589
00:33:24.420 --> 00:33:25.589
of a weaker requirement than

590
00:33:25.589 --> 00:33:27.809
availability because here we're not

591
00:33:27.809 --> 00:33:29.640
going to do anything while while the

592
00:33:29.640 --> 00:33:31.140
failed come until the failed component

593
00:33:31.140 --> 00:33:33.720
has been repaired but the fact that we

594
00:33:33.720 --> 00:33:37.230
can get up get going again without you

595
00:33:37.230 --> 00:33:39.509
know but without any loss of correctness

596
00:33:39.509 --> 00:33:41.880
is still a significant requirement it

597
00:33:41.880 --> 00:33:43.500
means you know recoverable systems

598
00:33:43.500 --> 00:33:45.690
typically need to do things like save

599
00:33:57.869 --> 00:34:01.890
life usually what the way available

600
00:34:01.890 --> 00:34:04.289
systems are SPECT is that they're

601
00:34:04.289 --> 00:34:07.349
available until some number of failures

602
00:34:07.349 --> 00:34:09.150
have happened if too many failures have

603
00:34:09.150 --> 00:34:11.670
happened an available system will stop

604
00:34:11.670 --> 00:34:14.820
working or you know will stop responding

605
00:34:14.820 --> 00:34:18.869
at all but when enough things have been

606
00:34:18.869 --> 00:34:21.329
repaired it'll continue operating so a

607
00:34:21.329 --> 00:34:23.429
good available system will sort of be

608
00:34:23.429 --> 00:34:25.139
recoverable as well in a sensitive to

609
00:34:25.139 --> 00:34:26.820
many failures occur

610
00:34:26.820 --> 00:34:28.469
it'll stop answering but then will

611
00:34:28.469 --> 00:34:35.519
continue correctly after that so this is

612
00:34:35.519 --> 00:34:38.429
what we love - this is what we'd love to

613
00:34:38.429 --> 00:34:43.289
obtain the biggest hammer what we'll see

614
00:34:43.289 --> 00:34:45.119
a number of approaches to solving these

615
00:34:45.119 --> 00:34:47.780
problems there's really sort of

616
00:34:47.780 --> 00:34:50.179
things that are the most important tools

617
00:34:50.179 --> 00:34:52.639
we have in this department one is

618
00:34:52.639 --> 00:34:55.849
non-volatile storage so that you know

619
00:34:55.849 --> 00:34:58.420
something crash power fails or whatever

620
00:34:58.420 --> 00:35:01.309
there's a building wide power failure we

621
00:35:01.309 --> 00:35:02.750
can use non-volatile store it's like

622
00:35:02.750 --> 00:35:05.329
hard drives or flash or solid-state

623
00:35:05.329 --> 00:35:07.429
drives or something to sort of store a

624
00:35:07.429 --> 00:35:12.679
check point or a log of the state of a

625
00:35:12.679 --> 00:35:14.480
system and then when the power comes

626
00:35:14.480 --> 00:35:16.760
back up or somebody repairs our power

627
00:35:20.510 --> 00:35:24.619
and continue from there so so one tool

628
00:35:24.619 --> 00:35:29.389
is sort of non-volatile storage and the

629
00:35:37.460 --> 00:35:39.139
nitty-gritty of building sort of

630
00:35:39.139 --> 00:35:42.469
high-performance fault-tolerant systems

631
00:35:42.469 --> 00:35:45.289
is in you know clever ways to avoid

632
00:35:45.289 --> 00:35:47.599
having to write the non-volatile storage

633
00:35:47.599 --> 00:35:49.969
too much in the old days and even today

634
00:36:00.920 --> 00:36:04.219
the scale of you know three gigahertz

635
00:36:04.219 --> 00:36:06.969
microprocessors good things like flash

636
00:36:06.969 --> 00:36:08.989
life is quite a bit better but still

637
00:36:08.989 --> 00:36:10.760
requires a lot of thought to get good

638
00:36:10.760 --> 00:36:12.949
performance out of and the other big

639
00:36:12.949 --> 00:36:14.360
tool we have for fault tolerance is

640
00:36:22.760 --> 00:36:26.510
know that sort of he problem lurking in

641
00:36:26.510 --> 00:36:28.489
any replicated system where we have two

642
00:36:28.489 --> 00:36:30.800
servers each with a supposedly identical

643
00:36:30.800 --> 00:36:34.159
copy of the system state the key problem

644
00:36:38.599 --> 00:36:41.329
sync and will stop being replicas right

645
00:36:41.329 --> 00:36:43.309
and this is just you know with the back

646
00:36:43.309 --> 00:36:45.409
of the every design that we're gonna see

647
00:36:45.409 --> 00:36:47.780
for using replication to get fault

648
00:36:47.780 --> 00:36:51.260
tolerance and lab - a lot - you're all

649
00:36:51.260 --> 00:36:53.800
about management management of

650
00:36:53.800 --> 00:36:57.449
replicated copies for fault tolerance

651
00:36:57.449 --> 00:37:02.360
as you'll see it's pretty complex a

652
00:37:03.739 --> 00:37:10.230
final topic final cross-cutting topic is

653
00:37:10.230 --> 00:37:17.550
consistency so it's an example of what I

654
00:37:17.550 --> 00:37:19.429
mean by consistency supposing we're

655
00:37:24.420 --> 00:37:26.610
supports two operations maybe there's a

656
00:37:26.610 --> 00:37:29.820
put operation and you give it a key and

657
00:37:43.920 --> 00:37:47.250
sends it a key and the storage service

658
00:37:47.250 --> 00:37:49.530
is supposed to you know respond with the

659
00:37:49.530 --> 00:37:50.820
value of the value it has stored for

660
00:37:50.820 --> 00:37:52.800
that key right and this is kind of good

661
00:37:52.800 --> 00:37:54.780
when I can't think of anything else as

662
00:37:54.780 --> 00:37:56.489
an example of a distributed system all

663
00:37:56.489 --> 00:38:00.480
Oh without key value services and

664
00:38:00.480 --> 00:38:01.949
they're very useful right they're just

665
00:38:01.949 --> 00:38:05.309
sort of a kind of fundamental simple

666
00:38:05.309 --> 00:38:09.300
version of a storage system so of course

667
00:38:09.300 --> 00:38:11.699
if you're an application programmer it's

668
00:38:11.699 --> 00:38:15.150
helpful if these two operations kind of

669
00:38:15.150 --> 00:38:16.800
have meanings attached to them that you

670
00:38:16.800 --> 00:38:18.630
can go look in the manual and the manual

671
00:38:18.630 --> 00:38:21.300
says you know what it what it means what

672
00:38:21.300 --> 00:38:23.519
you'll get back if you call get right

673
00:38:23.519 --> 00:38:25.530
and sort of what it means for you to

674
00:38:25.530 --> 00:38:28.110
call put all right so it'll be great there's

675
00:38:28.110 --> 00:38:29.429
some sort of spec for what they meant

676
00:38:29.429 --> 00:38:31.440
otherwise like who knows how can you

677
00:38:31.440 --> 00:38:32.880
possibly write an application without a

678
00:38:32.880 --> 00:38:35.250
description of what putting get are

679
00:38:35.250 --> 00:38:38.369
supposed to do and this is the topic of

680
00:38:38.369 --> 00:38:40.199
consistency and the reason why it's

681
00:38:40.199 --> 00:38:42.329
interesting in distributed systems is

682
00:38:42.329 --> 00:38:46.199
that both for performance and for fault

683
00:38:46.199 --> 00:38:48.300
tolerant reasons fault tolerance reason

684
00:38:48.300 --> 00:38:50.400
we often have more than one copy of the

685
00:38:50.400 --> 00:38:53.880
data floating around so you know in a

686
00:38:53.880 --> 00:38:55.500
non-distributed system where you just

687
00:38:55.500 --> 00:38:59.130
have a single server with a single table

688
00:38:59.130 --> 00:39:02.579
there's often although not always but

689
00:39:02.579 --> 00:39:04.199
there's often like relatively no

690
00:39:04.199 --> 00:39:05.940
ambiguity about what pudding get could

691
00:39:05.940 --> 00:39:07.360
possibly mean right in

692
00:39:07.360 --> 00:39:08.980
to ative Lee you know what put means is

693
00:39:08.980 --> 00:39:10.869
update the table and what get means is

694
00:39:10.869 --> 00:39:12.550
just get me the version that's stored in

695
00:39:18.639 --> 00:39:20.590
in the data due to replication or

696
00:39:20.590 --> 00:39:23.889
caching or who knows what there may be

697
00:39:23.889 --> 00:39:30.130
lots of different versions of this key

698
00:39:30.130 --> 00:39:32.380
value pair floating around like if one

699
00:39:51.550 --> 00:39:55.360
both of them and then some client issues

700
00:39:55.360 --> 00:39:58.989
a put nice we have client over here and

701
00:39:58.989 --> 00:40:00.309
it's gonna send a put it wants to update

702
00:40:00.309 --> 00:40:03.489
the value of one to be twenty-one all

703
00:40:03.489 --> 00:40:04.960
right maybe it's counting stuff in this

704
00:40:04.960 --> 00:40:09.550
key value server so sends a put with key

705
00:40:09.550 --> 00:40:13.750
one and value twenty one it sends it to

706
00:40:13.750 --> 00:40:15.840
the first server and it's about to send

707
00:40:20.380 --> 00:40:22.300
it's about to send this put but just

708
00:40:22.300 --> 00:40:23.530
before it sends to put to the second

709
00:40:23.530 --> 00:40:26.949
server crashes I power failure or bug an

710
00:40:26.949 --> 00:40:28.690
operating system or something so now the

711
00:40:28.690 --> 00:40:30.639
state were left in sadly is that we sent

712
00:40:30.639 --> 00:40:35.469
this put and so we've updated one of the

713
00:40:35.469 --> 00:40:37.300
two replicas didn't have value twenty

714
00:40:37.300 --> 00:40:38.590
one but the other ones still with twenty

715
00:40:38.590 --> 00:40:40.869
now somebody comes along and reads with

716
00:40:40.869 --> 00:40:42.909
a get and they might get they want to

717
00:40:50.349 --> 00:40:52.599
to the top server first if you're

718
00:40:52.599 --> 00:40:53.829
building a fault-tolerant system the

719
00:41:00.809 --> 00:41:03.639
so either way someday you risk exposing

720
00:41:03.639 --> 00:41:06.610
this stale copy of the data to some

721
00:41:06.610 --> 00:41:08.650
future again it could be that many gets

722
00:41:08.650 --> 00:41:10.630
get the updated twenty one and then like

723
00:41:10.630 --> 00:41:12.730
next week all of a sudden some get

724
00:41:12.730 --> 00:41:14.920
yields you know a week old copy of the

725
00:41:14.920 --> 00:41:19.369
data so that's not very consistent

726
00:41:19.369 --> 00:41:23.719
right so in order but you know it's the

727
00:41:23.719 --> 00:41:25.869
kind of thing that could happen right

728
00:41:25.869 --> 00:41:29.210
we're not careful so you know we need to

729
00:41:29.210 --> 00:41:32.420
have we need to actually write down what

730
00:41:32.420 --> 00:41:33.920
the rules are going to be about puts and

731
00:41:33.920 --> 00:41:36.650
gets given this danger of due to

732
00:41:36.650 --> 00:41:39.230
replication and it turns out there's

733
00:41:39.230 --> 00:41:42.650
many different definitions you can have

734
00:41:48.469 --> 00:41:52.940
sound like well I get yields the you

735
00:41:52.940 --> 00:41:55.250
know value put by the most recently

736
00:41:55.250 --> 00:42:00.170
completed put all right so that's

737
00:42:00.170 --> 00:42:02.960
usually called strong consistency it

738
00:42:02.960 --> 00:42:05.389
turns out also it's very useful to build

739
00:42:05.389 --> 00:42:06.739
systems that have much weaker

740
00:42:06.739 --> 00:42:08.659
consistency there for example do not

741
00:42:08.659 --> 00:42:11.989
guarantee anything like a get sees the

742
00:42:11.989 --> 00:42:15.170
value written by the most recent put and

743
00:42:15.170 --> 00:42:18.440
the reason so there's there strongly

744
00:42:25.130 --> 00:42:27.289
puts although you have to there's a lot

745
00:42:27.289 --> 00:42:28.820
of details to work out there's also

746
00:42:28.820 --> 00:42:32.179
weekly consistent many sort of flavors

747
00:42:32.179 --> 00:42:33.829
of weekly consistent systems that do not

748
00:42:33.829 --> 00:42:36.650
make any such guarantee that you know

749
00:42:36.650 --> 00:42:38.869
may guarantee well you're you know if

750
00:42:38.869 --> 00:42:41.690
someone does a put then you may not see

751
00:42:41.690 --> 00:42:43.610
the put you may see old values that

752
00:42:43.610 --> 00:42:45.739
weren't updated by the put for an

753
00:42:51.289 --> 00:42:53.860
in weak consistency schemes is that

754
00:43:00.860 --> 00:43:02.690
most recent right that's a very

755
00:43:02.690 --> 00:43:07.190
expensive spec to implement because what

756
00:43:07.190 --> 00:43:08.809
it means is almost certainly that you

757
00:43:08.809 --> 00:43:10.489
have to somebody has to do a lot of

758
00:43:10.489 --> 00:43:12.469
communication in order to actually

759
00:43:12.469 --> 00:43:14.179
implement some notion of strong

760
00:43:14.179 --> 00:43:16.449
consistency if you have multiple copies

761
00:43:16.449 --> 00:43:20.750
it means that either the writer or the

762
00:43:20.750 --> 00:43:22.489
reader or maybe both has to consult

763
00:43:22.489 --> 00:43:26.329
every copy like in this case where you

764
00:43:26.329 --> 00:43:28.340
know maybe a client crash left one

765
00:43:28.340 --> 00:43:30.469
updated but not the other if we wanted

766
00:43:30.469 --> 00:43:31.820
to implement strong

767
00:43:31.820 --> 00:43:33.710
consistency see in them maybe a simple way

768
00:43:33.710 --> 00:43:35.210
in this system we'd have readers read

769
00:43:39.559 --> 00:43:41.269
most recently written value that they

770
00:43:41.269 --> 00:43:44.869
find but that's expensive that's a lot

771
00:43:44.869 --> 00:43:49.639
of chitchat to read one value so in

772
00:43:49.639 --> 00:43:51.619
order to avoid communication as much as

773
00:43:51.619 --> 00:43:54.679
possible particularly if replicas are

774
00:43:54.679 --> 00:43:56.809
far away people build weak systems that

775
00:43:56.809 --> 00:43:59.480
might actually allow the stale read of

776
00:43:59.480 --> 00:44:02.840
an old value in this case although

777
00:44:02.840 --> 00:44:05.539
there's often more semantics attached to

778
00:44:05.539 --> 00:44:06.980
that to try to make these weak schemes

779
00:44:06.980 --> 00:44:10.369
more useful and we're this communication

780
00:44:10.369 --> 00:44:13.420
problem you know strong consistency

781
00:44:13.420 --> 00:44:16.849
requiring expensive communication where

782
00:44:16.849 --> 00:44:19.369
this really runs you into trouble is

783
00:44:19.369 --> 00:44:21.500
that if we're using replication for

784
00:44:21.500 --> 00:44:24.530
fault tolerance then we really want the

785
00:44:24.530 --> 00:44:26.599
replicas to have independent failure

786
00:44:26.599 --> 00:44:29.210
probability to have uncorrelated failure

787
00:44:29.210 --> 00:44:31.849
so for example putting both of the

788
00:44:31.849 --> 00:44:34.849
replicas of our data in the same iraq in

789
00:44:38.360 --> 00:44:39.829
because if someone trips over the power

790
00:44:39.829 --> 00:44:42.409
cable to that rack both of our copies of

791
00:44:46.190 --> 00:44:49.730
cable in the same rack so in the search

792
00:44:49.730 --> 00:44:53.239
for making replicas as independent and

793
00:44:53.239 --> 00:44:54.980
failure as possible in order to get

794
00:44:54.980 --> 00:44:57.860
decent fault tolerance people would love

795
00:44:57.860 --> 00:45:00.320
to put different replicas as far apart

796
00:45:00.320 --> 00:45:02.960
as possible like in different cities or

797
00:45:02.960 --> 00:45:05.210
maybe on opposite sides of the continent

798
00:45:09.260 --> 00:45:11.840
also destroy the other data center that

799
00:45:11.840 --> 00:45:15.860
as the other copy you know so we'd love

800
00:45:15.860 --> 00:45:17.420
to be able to do that if you do that

801
00:45:17.420 --> 00:45:20.659
then the other copy is thousands of

802
00:45:20.659 --> 00:45:23.659
miles away and the rate at which light

803
00:45:23.659 --> 00:45:26.480
travels means that it may take on the

804
00:45:26.480 --> 00:45:28.550
order of milliseconds or tens of

805
00:45:28.550 --> 00:45:31.670
milliseconds to communicate to a data

806
00:45:31.670 --> 00:45:33.380
center across the continent in order to

807
00:45:33.380 --> 00:45:36.590
update the other copy of the data and so

808
00:45:36.590 --> 00:45:38.449
that makes this the communication

809
00:45:38.449 --> 00:45:40.610
required for strong consistency for good

810
00:45:40.610 --> 00:45:42.349
consistency potentially extremely

811
00:45:42.349 --> 00:45:44.449
expensive like every time you want to do

812
00:45:44.449 --> 00:45:45.320
one of these put opera

813
00:45:50.510 --> 00:45:52.940
milliseconds in order to talk to both

814
00:45:52.940 --> 00:45:54.650
copies of the data to ensure that

815
00:45:54.650 --> 00:45:56.750
they're both updated or or both checked

816
00:45:56.750 --> 00:46:01.360
to find the latest copy and that

817
00:46:07.789 --> 00:46:09.500
instructions per second so we're wasting

818
00:46:09.500 --> 00:46:11.539
a lot of potential instructions while we

819
00:46:11.539 --> 00:46:14.510
wait people often go much weaker systems

820
00:46:23.139 --> 00:46:26.840
research on how to structure weak

821
00:46:30.380 --> 00:46:31.760
to take advantage of them in order to

822
00:46:31.760 --> 00:46:36.349
actually get high performance alright so

823
00:46:36.349 --> 00:46:40.150
that's a lightning preview of the

824
00:46:40.150 --> 00:46:43.730
technical ideas in the course any

825
00:46:43.730 --> 00:46:46.340
questions about this before I start

826
00:46:46.340 --> 00:46:50.869
talking about MapReduce all right I want

827
00:46:55.519 --> 00:46:57.949
going to illustrate most of the ideas

828
00:46:57.949 --> 00:47:02.420
that we've been talking about here now

829
00:47:02.420 --> 00:47:07.780
produces a system that was originally

830
00:47:07.780 --> 00:47:11.989
designed and built and used by Google I

831
00:47:11.989 --> 00:47:15.139
think the paper dates back to 2004 the

832
00:47:15.139 --> 00:47:17.269
problem they were faced with was that

833
00:47:17.269 --> 00:47:20.900
they were running huge computations on

834
00:47:20.900 --> 00:47:22.760
terabytes and terabytes of data like

835
00:47:22.760 --> 00:47:27.170
creating an index of all of the content

836
00:47:27.170 --> 00:47:29.659
of the web or analyzing the link

837
00:47:29.659 --> 00:47:32.360
structure of the entire web in order to

838
00:47:37.219 --> 00:47:39.139
whole web is what's even in those days

839
00:47:55.340 --> 00:47:56.630
entire content to the way I've been a

840
00:47:56.630 --> 00:47:58.130
single computer

841
00:47:58.130 --> 00:47:59.989
how long would have taken but you know

842
00:47:59.989 --> 00:48:01.940
it's weeks or months or years or

843
00:48:01.940 --> 00:48:04.309
something so Google the time was

844
00:48:04.309 --> 00:48:06.199
desperate to be able to run giant

845
00:48:06.199 --> 00:48:08.539
computations on giant data on thousands

846
00:48:08.539 --> 00:48:10.670
of computers in order that the

847
00:48:10.670 --> 00:48:12.980
computations could finish rapidly it's

848
00:48:12.980 --> 00:48:14.210
worth it to them to buy lots of

849
00:48:14.210 --> 00:48:16.400
computers so that their engineers

850
00:48:16.400 --> 00:48:17.719
wouldn't have to spend a lot of time

851
00:48:17.719 --> 00:48:19.519
reading the newspaper or something

852
00:48:27.409 --> 00:48:29.630
clever engineer or sort of handwrite you

853
00:48:29.630 --> 00:48:30.619
know if you needed to write a web

854
00:48:30.619 --> 00:48:32.929
indexer or some sort of Lincoln outlay a

855
00:48:32.929 --> 00:48:35.809
blink analysis tool you know Google

856
00:48:35.809 --> 00:48:37.130
bought the computers and they say here

857
00:48:37.130 --> 00:48:38.599
engineers you know do write but never

858
00:48:38.599 --> 00:48:39.889
whatever software you like on these

859
00:48:39.889 --> 00:48:41.269
computers and you know they would

860
00:48:41.269 --> 00:48:44.230
laborious ly write the sort of one-off

861
00:48:44.230 --> 00:48:46.280
manually bitten software to take

862
00:48:46.280 --> 00:48:47.659
whatever problem they were working on

863
00:48:47.659 --> 00:48:49.610
and so to somehow farm it out to a lot

864
00:48:49.610 --> 00:48:51.469
of computers and organize that

865
00:48:51.469 --> 00:48:56.809
computation and get the data back if you

866
00:48:56.809 --> 00:48:58.539
only hire engineers who are skilled

867
00:48:58.539 --> 00:49:01.789
distributed systems experts maybe that's

868
00:49:01.789 --> 00:49:04.190
ok although even then it's probably very

869
00:49:04.190 --> 00:49:07.489
wasteful of engineering effort but they

870
00:49:07.489 --> 00:49:09.289
wanted to hire people who were skilled

871
00:49:15.159 --> 00:49:16.909
engineers who wanted to spend all their

872
00:49:16.909 --> 00:49:18.559
time writing distributed system software

873
00:49:18.559 --> 00:49:20.360
so they really needed some kind of

874
00:49:20.360 --> 00:49:22.309
framework that would make it easy to

875
00:49:28.130 --> 00:49:30.139
to do like the sort algorithm or a web

876
00:49:30.139 --> 00:49:32.989
index or link analyzer or whatever just

877
00:49:32.989 --> 00:49:34.550
write the guts of that application and

878
00:49:34.550 --> 00:49:36.739
not be able to run it on a thousands of

879
00:49:36.739 --> 00:49:39.710
computers without worrying about the

880
00:49:39.710 --> 00:49:41.539
details of how to spread the work over

881
00:49:41.539 --> 00:49:43.730
the thousands of computers how to

882
00:49:43.730 --> 00:49:45.949
organize whatever data movement was

883
00:49:45.949 --> 00:49:48.349
required how to cope with the inevitable

884
00:49:48.349 --> 00:49:50.630
failures so they were looking for a

885
00:49:54.739 --> 00:50:00.320
run giant distributed computations and

886
00:50:00.320 --> 00:50:03.610
so that's what MapReduce is all about

887
00:50:03.610 --> 00:50:06.469
and the idea is that the programmer just

888
00:50:06.469 --> 00:50:09.929
write the application designer

889
00:50:18.239 --> 00:50:20.639
distribution and the MapReduce framework

890
00:50:27.869 --> 00:50:30.900
up to is it starts by assuming that

891
00:50:35.429 --> 00:50:37.559
different files or chunks in some way so

892
00:50:37.559 --> 00:50:43.110
we're imagining that no yeah you know

893
00:50:43.110 --> 00:50:51.119
input file one and put file two etc you

894
00:50:51.119 --> 00:50:54.239
know these inputs are maybe you know web

895
00:50:54.239 --> 00:50:55.920
pages crawled from the web or more

896
00:51:00.179 --> 00:51:03.420
files crawl from the web all right and

897
00:51:03.420 --> 00:51:04.820
the way Map Reduce

898
00:51:04.820 --> 00:51:07.949
starts is that you're to find a map

899
00:51:07.949 --> 00:51:09.659
function and the MapReduce framework is

900
00:51:09.659 --> 00:51:15.889
gonna run your map function on each of

901
00:51:15.889 --> 00:51:22.199
the input files and of course you can

902
00:51:22.199 --> 00:51:23.190
see here there's some obvious

903
00:51:23.190 --> 00:51:26.969
parallelism available can run the maps

904
00:51:26.969 --> 00:51:28.349
in parallel so the each of these map

905
00:51:32.400 --> 00:51:33.989
function is required to produce is a

906
00:51:33.989 --> 00:51:36.750
list you know it takes a file as input

907
00:51:36.750 --> 00:51:39.750
and the file is some fraction of the

908
00:51:39.750 --> 00:51:42.179
input data and it produces a list of key

909
00:51:42.179 --> 00:51:45.619
value pairs as output the map function

910
00:51:45.619 --> 00:51:48.510
and so for example let's suppose we're

911
00:51:48.510 --> 00:51:50.579
writing the simplest possible MapReduce

912
00:51:50.579 --> 00:51:56.400
example a word count MapReduce job goal

913
00:51:56.400 --> 00:51:58.170
is to count the number of occurrences of

914
00:51:58.170 --> 00:52:00.389
each word so your map function might

915
00:52:00.389 --> 00:52:02.820
emit key value pairs where the key is

916
00:52:02.820 --> 00:52:06.929
the word and the value is just one so

917
00:52:06.929 --> 00:52:08.909
for every word at C so then this map

918
00:52:08.909 --> 00:52:10.409
function will split the input up into

919
00:52:10.409 --> 00:52:11.760
words or everywhere ditzies

920
00:52:11.760 --> 00:52:14.309
it emits that word as the key and 1 as

921
00:52:14.309 --> 00:52:16.170
the value and then later on will count

922
00:52:16.170 --> 00:52:18.360
up all those ones in order to get the

923
00:52:18.360 --> 00:52:21.420
final output so you know maybe input 1

924
00:52:21.420 --> 00:52:23.230
has the word

925
00:52:23.230 --> 00:52:26.469
a in it and the word B in it and so the

926
00:52:26.469 --> 00:52:28.570
output the map is going to produce is

927
00:52:28.570 --> 00:52:32.679
key a value one key B value one maybe

928
00:52:32.679 --> 00:52:35.650
the second not communication sees a file

929
00:52:35.650 --> 00:52:38.889
that has a B in it and nothing else so

930
00:52:38.889 --> 00:52:43.119
it's going to implement output b1 maybe

931
00:52:50.139 --> 00:52:53.380
on all the input files and we get this

932
00:52:57.130 --> 00:53:00.420
map a set of key value pairs as output

933
00:53:00.420 --> 00:53:03.130
then the second stage of the computation

934
00:53:09.460 --> 00:53:12.610
together all instances from all maps of

935
00:53:12.610 --> 00:53:15.130
each key word so the MapReduce framework

936
00:53:15.130 --> 00:53:16.869
is going to collect together all of the

937
00:53:16.869 --> 00:53:20.739
A's you know from every map every key

938
00:53:20.739 --> 00:53:22.599
value pair whose key was a it's gonna

939
00:53:22.599 --> 00:53:28.289
take collect them all and hand them to

940
00:53:35.530 --> 00:53:38.320
all the B's and collect them together of

941
00:53:38.320 --> 00:53:39.699
course you know requires a real

942
00:53:39.699 --> 00:53:42.340
collection because they were different

943
00:53:46.989 --> 00:53:48.610
different computers so we're not talking

944
00:53:48.610 --> 00:53:50.679
about data movement I'm so we're gonna

945
00:53:50.679 --> 00:53:53.340
collect all the B keys and hand them to

946
00:53:53.340 --> 00:53:58.719
a different call to reduce that has all

947
00:53:58.719 --> 00:54:01.960
of the B keys as its arguments and same

948
00:54:01.960 --> 00:54:07.630
as C so there's going to be the

949
00:54:07.630 --> 00:54:09.159
MapReduce framework will arrange for one

950
00:54:09.159 --> 00:54:11.769
call to reduce for every key that

951
00:54:11.769 --> 00:54:17.139
occurred in any of the math output and

952
00:54:17.139 --> 00:54:19.449
you know for our sort of silly word

953
00:54:19.449 --> 00:54:23.500
count example all these reduces have to

954
00:54:28.329 --> 00:54:29.650
doesn't even have to look at the items

955
00:54:34.480 --> 00:54:35.769
the value you don't have to look at

956
00:54:35.769 --> 00:54:36.880
those ones we've just count

957
00:54:36.880 --> 00:54:41.590
so this reduce is going to produce a and

958
00:54:41.590 --> 00:54:44.579
then the count of its inputs this reduce

959
00:54:44.579 --> 00:54:47.679
it's going to produce the key associated

960
00:54:47.679 --> 00:54:50.349
with it and then count of its values

961
00:55:01.989 --> 00:55:07.199
high level just for completeness the

962
00:55:12.480 --> 00:55:16.900
anyone invocation of MapReduce is called

963
00:55:27.219 --> 00:55:29.860
it's an example for this word count you

964
00:55:29.860 --> 00:55:31.150
know the what the map and reduce

965
00:55:31.150 --> 00:55:40.750
functions would look like the map

966
00:55:40.750 --> 00:55:45.130
function takes a key in the value as

967
00:55:45.130 --> 00:55:46.420
arguments and now we're talking about

968
00:55:51.519 --> 00:55:54.820
who knows what so this is just code

969
00:55:54.820 --> 00:55:57.159
people ordinary people can write what a

970
00:55:57.159 --> 00:55:58.869
map function for word count would do is

971
00:55:58.869 --> 00:56:02.920
split the the key is the file name which

972
00:56:02.920 --> 00:56:05.110
typically is ignored we really care what

973
00:56:05.110 --> 00:56:07.599
the file name was and the V is the

974
00:56:07.599 --> 00:56:12.250
content of this maps input file so V is

975
00:56:12.250 --> 00:56:14.400
you know just contains all this text

976
00:56:14.400 --> 00:56:21.760
we're gonna split V into words and then

977
00:56:21.760 --> 00:56:24.630
for each word

978
00:56:30.889 --> 00:56:34.130
we're just gonna emit and emit takes two

979
00:56:34.130 --> 00:56:36.889
arguments mitts you know calmly map can

980
00:56:36.889 --> 00:56:38.420
make emit is provided by the MapReduce

981
00:56:38.420 --> 00:56:41.300
framework we get to produce we hand emit

982
00:56:41.300 --> 00:56:44.889
a key which is the word and a value

983
00:56:44.889 --> 00:56:49.730
which is the string one so that's it for

984
00:56:54.860 --> 00:56:56.559
could be this simple

985
00:56:56.559 --> 00:57:00.309
so there's sort of promise to make the

986
00:57:00.309 --> 00:57:02.599
and you know this map function doesn't

987
00:57:02.599 --> 00:57:04.190
know anything about distribution or

988
00:57:04.190 --> 00:57:06.170
multiple computers or the fact we need

989
00:57:06.170 --> 00:57:07.969
we need to move data across the network

990
00:57:13.400 --> 00:57:19.550
the reduce function for a word count the

991
00:57:19.550 --> 00:57:21.889
reduce is called with you know remember

992
00:57:21.889 --> 00:57:23.329
each reduce is called with sort of all

993
00:57:23.329 --> 00:57:25.429
the instances of a given key on the

994
00:57:25.429 --> 00:57:27.170
MapReduce framework calls reduce with

995
00:57:33.409 --> 00:57:38.389
produced associated with that key the

996
00:57:38.389 --> 00:57:40.639
key is the word the values are all ones

997
00:57:40.639 --> 00:57:41.960
we don't like here about them we only

998
00:57:41.960 --> 00:57:44.510
care about how many they were and so

999
00:57:44.510 --> 00:57:47.420
reduce has its own omit function that

1000
00:57:47.420 --> 00:57:51.199
just takes a value to be emitted as the

1001
00:57:51.199 --> 00:57:53.780
final output as the value for the this

1002
00:57:53.780 --> 00:57:57.829
key so we're gonna admit a length of

1003
00:57:57.829 --> 00:58:01.789
this array so this is also about as

1004
00:58:01.789 --> 00:58:04.280
simplest reduce functions have are and

1005
00:58:11.480 --> 00:58:15.860
tolerance or anything else alright any

1006
00:58:15.860 --> 00:58:20.530
questions about the basic framework yes

1007
00:58:27.389 --> 00:58:30.550
[Music]

1008
00:58:39.230 --> 00:58:48.289
reducers sort of oh yes oh yes in in in

1009
00:58:53.269 --> 00:58:55.940
MapReduce users to you know define a

1010
00:58:55.940 --> 00:58:58.219
MapReduce job that took some inputs and

1011
00:58:58.219 --> 00:59:00.170
produce some outputs and then have a

1012
00:59:00.170 --> 00:59:01.969
second MapReduce job you know you're

1013
00:59:01.969 --> 00:59:03.940
doing some very complicated multistage

1014
00:59:03.940 --> 00:59:08.900
analysis or iterative algorithm like

1015
00:59:08.900 --> 00:59:10.429
PageRank for example which is the

1016
00:59:10.429 --> 00:59:13.119
algorithm Google uses to sort of

1017
00:59:13.119 --> 00:59:16.190
estimate how important or influential

1018
00:59:16.190 --> 00:59:18.230
different webpages are that's an

1019
00:59:22.940 --> 00:59:24.440
implement in MapReduce which I think

1020
00:59:24.440 --> 00:59:26.539
they originally did you have to run the

1021
00:59:26.539 --> 00:59:28.909
MapReduce job multiple times and the

1022
00:59:28.909 --> 00:59:30.619
output of each one is sort of you know

1023
00:59:30.619 --> 00:59:34.360
list of webpages with an updated sort of

1024
00:59:34.360 --> 00:59:36.650
value or weight or importance for each

1025
00:59:36.650 --> 00:59:38.360
webpage so it was routine to take this

1026
00:59:38.360 --> 00:59:40.280
output and then use it as the input to

1027
00:59:40.280 --> 00:59:53.750
another MapReduce job oh yeah well yeah

1028
00:59:58.280 --> 00:59:59.269
function sort of in the knowledge that

1029
00:59:59.269 --> 01:00:02.659
oh I need to produce data that's in the

1030
01:00:02.659 --> 01:00:05.420
format or as the information required

1031
01:00:05.420 --> 01:00:07.940
for the next MapReduce job I mean this

1032
01:00:07.940 --> 01:00:09.230
actually brings up a little bit of a

1033
01:00:09.230 --> 01:00:11.500
shortcoming in the MapReduce framework

1034
01:00:11.500 --> 01:00:16.309
which is it's great if you are if the

1035
01:00:16.309 --> 01:00:18.679
algorithm you need to run is easily

1036
01:00:18.679 --> 01:00:20.809
expressible as a math followed by this

1037
01:00:20.809 --> 01:00:23.869
sort of shuffling of the data by key

1038
01:00:23.869 --> 01:00:26.179
followed by a reduce and that's it

1039
01:00:30.590 --> 01:00:32.119
furthermore each of the maps has to be

1040
01:00:32.119 --> 01:00:33.400
completely independent and

1041
01:00:33.400 --> 01:00:39.519
are required to be functional pure

1042
01:00:39.519 --> 01:00:42.760
functional functions that just look at

1043
01:00:42.760 --> 01:00:44.469
their arguments and nothing else

1044
01:00:44.469 --> 01:00:46.480
you know that's like it's a restriction

1045
01:00:46.480 --> 01:00:48.639
and it turns out that many people want

1046
01:00:48.639 --> 01:00:49.989
to run much longer pipelines that

1047
01:00:49.989 --> 01:00:51.429
involve lots and lots of different kinds

1048
01:00:51.429 --> 01:00:53.170
of processing and with MapReduce you

1049
01:00:53.170 --> 01:00:54.369
have to sort of cobble that together

1050
01:00:54.369 --> 01:00:58.389
from multiple MapReduce distinct

1051
01:00:58.389 --> 01:01:00.730
MapReduce jobs and more advanced systems

1052
01:01:04.869 --> 01:01:06.519
to specify the complete pipeline of

1053
01:01:06.519 --> 01:01:08.409
computations and they'll do optimization

1054
01:01:08.409 --> 01:01:10.900
you know the framework realizes all the

1055
01:01:15.670 --> 01:01:19.590
much more complicated computations

1056
01:01:39.659 --> 01:01:41.650
from the programmers point of view it's

1057
01:01:45.610 --> 01:01:49.320
worker processes and the worker servers

1058
01:01:49.320 --> 01:01:53.320
that that are they're part of MapReduce

1059
01:01:53.320 --> 01:01:55.389
framework that among many other things

1060
01:02:01.929 --> 01:02:04.239
lot about how this is organized by the

1061
01:02:04.239 --> 01:02:06.190
surrounding framework this is sort of

1062
01:02:06.190 --> 01:02:08.380
the programmers view with all the

1063
01:02:08.380 --> 01:02:14.340
distributive stuff stripped out yes

1064
01:02:15.960 --> 01:02:25.150
sorry I gotta say it again oh you mean

1065
01:02:25.150 --> 01:02:32.170
where does the immediate data go okay so

1066
01:02:32.170 --> 01:02:35.440
there's two questions one is when you

1067
01:02:46.909 --> 01:02:50.239
the actual answer is that first where

1068
01:02:56.539 --> 01:02:58.190
thing to look at here is figure one in

1069
01:02:58.190 --> 01:03:02.659
the paper sitting underneath this in the

1070
01:03:02.659 --> 01:03:04.429
real world there's some big collection

1071
01:03:12.409 --> 01:03:14.659
also a single master server that's

1072
01:03:14.659 --> 01:03:16.519
organizing the whole computation and

1073
01:03:16.519 --> 01:03:18.889
what's going on here is the master

1074
01:03:18.889 --> 01:03:22.489
server for know knows that there's some

1075
01:03:22.489 --> 01:03:24.559
number of input files you know five

1076
01:03:24.559 --> 01:03:27.800
thousand input files and it farms out in

1077
01:03:27.800 --> 01:03:29.539
vacations of map to the different

1078
01:03:29.539 --> 01:03:30.889
workers so it'll send a message to

1079
01:03:30.889 --> 01:03:34.179
worker seven saying please run you know

1080
01:03:34.179 --> 01:03:37.760
this map function on such-and-such an

1081
01:03:37.760 --> 01:03:41.750
input file and then the worker function

1082
01:03:41.750 --> 01:03:43.400
which is you know part of MapReduce and

1083
01:03:43.400 --> 01:03:47.170
knows all about Map Reduce well then

1084
01:03:54.110 --> 01:03:56.599
function with the file name value as its

1085
01:03:56.599 --> 01:04:00.400
arguments then that worker process will

1086
01:04:00.400 --> 01:04:02.750
employees what implements in it and

1087
01:04:02.750 --> 01:04:05.960
every time the map calls emit the worker

1088
01:04:05.960 --> 01:04:10.280
process will write this data to files on

1089
01:04:10.280 --> 01:04:12.769
the local disk so what happens to map

1090
01:04:12.769 --> 01:04:17.420
emits and is they produce files on the

1091
01:04:17.420 --> 01:04:19.820
map workers local disk that are

1092
01:04:19.820 --> 01:04:21.949
accumulating all the keys and values

1093
01:04:21.949 --> 01:04:26.530
produced by the maps run on that worker

1094
01:04:26.530 --> 01:04:30.199
so at the end of the maps phase what

1095
01:04:37.969 --> 01:04:42.369
worker machine then the MapReduce

1096
01:04:42.369 --> 01:04:45.710
workers arrange to move the data to

1097
01:04:45.710 --> 01:04:46.820
where it's going to be needed for the

1098
01:04:46.820 --> 01:04:50.239
reduces so and since and a you know in a

1099
01:04:50.239 --> 01:04:53.239
typical big computation you know this

1100
01:04:53.239 --> 01:04:55.219
this reduce indication is going to need

1101
01:05:01.559 --> 01:05:04.289
out you know this is a simple example

1102
01:05:04.289 --> 01:05:08.599
but probably in general every single map

1103
01:05:08.599 --> 01:05:10.469
indication will have produce lots of

1104
01:05:10.469 --> 01:05:12.960
keys including some instances of key a

1105
01:05:12.960 --> 01:05:15.389
so typically in order before we can even

1106
01:05:15.389 --> 01:05:17.460
run this reduce function the MapReduce

1107
01:05:17.460 --> 01:05:20.190
framework that is the MapReduce worker

1108
01:05:20.190 --> 01:05:22.590
running on one of our thousand servers

1109
01:05:22.590 --> 01:05:24.269
is going to have to go talk to every

1110
01:05:24.269 --> 01:05:26.579
single other of the thousand servers and

1111
01:05:26.579 --> 01:05:28.530
say look you know I'm gonna run the

1112
01:05:28.530 --> 01:05:31.170
reduce for key a please look at the

1113
01:05:31.170 --> 01:05:33.210
intermediate map output stored in your

1114
01:05:33.210 --> 01:05:35.880
disk and fish out all of the instances

1115
01:05:35.880 --> 01:05:38.159
of key a and send them over the network

1116
01:05:43.530 --> 01:05:45.960
worker all of the instances of the key

1117
01:05:45.960 --> 01:05:47.400
that it's responsible for that the

1118
01:05:47.400 --> 01:05:50.340
master has told it to be responsible for

1119
01:05:50.340 --> 01:05:51.840
and once it's collected all of that data

1120
01:05:51.840 --> 01:05:55.820
then it can call reduce and the reduce

1121
01:05:55.820 --> 01:05:58.469
function itself calls reduce omit which

1122
01:05:58.469 --> 01:06:01.889
is different from the map in it and what

1123
01:06:01.889 --> 01:06:04.710
reduces emit does is writes the output

1124
01:06:04.710 --> 01:06:12.150
to a file in a cluster file service that

1125
01:06:12.150 --> 01:06:14.519
Google uses so here's something I

1126
01:06:14.519 --> 01:06:17.969
haven't mentioned I haven't mentioned

1127
01:06:17.969 --> 01:06:21.329
where the input lives and where the

1128
01:06:21.329 --> 01:06:25.530
output lives they're both files because

1129
01:06:25.530 --> 01:06:28.800
any piece of input we want the

1130
01:06:28.800 --> 01:06:31.320
flexibility to be able to read any piece

1131
01:06:31.320 --> 01:06:34.590
of input on any worker server that means

1132
01:06:34.590 --> 01:06:36.800
we need some kind of network file system

1133
01:06:36.800 --> 01:06:42.510
to store the input data and so indeed

1134
01:06:50.159 --> 01:06:51.989
cluster file system and GFS actually

1135
01:06:51.989 --> 01:06:54.210
runs on exactly the same set of workers

1136
01:06:54.210 --> 01:06:56.719
that work our servers that run MapReduce

1137
01:06:56.719 --> 01:07:00.630
and the input GFS just automatically

1138
01:07:00.630 --> 01:07:02.219
when you you know it's a file system you

1139
01:07:02.219 --> 01:07:03.840
can read in my files it just

1140
01:07:03.840 --> 01:07:06.119
automatically splits up any big file you

1141
01:07:06.119 --> 01:07:08.489
store on it across lots of servers and

1142
01:07:08.489 --> 01:07:12.320
64 megabyte chunks so if you write

1143
01:07:12.320 --> 01:07:14.360
if you view of ten terabytes of crawled

1144
01:07:14.360 --> 01:07:17.750
web page contents and you just write

1145
01:07:17.750 --> 01:07:20.119
them to GFS even as a single big file

1146
01:07:30.949 --> 01:07:32.510
servers that Google has available and

1147
01:07:32.510 --> 01:07:34.579
that's fanTAstic that's just what we

1148
01:07:34.579 --> 01:07:36.860
need if we then want to run a MapReduce

1149
01:07:36.860 --> 01:07:39.650
job that takes the entire crawled web as

1150
01:07:39.650 --> 01:07:42.650
input the data is already stored in a

1151
01:07:42.650 --> 01:07:44.539
way that split up evenly across all the

1152
01:07:44.539 --> 01:07:47.780
servers and so that means that the map

1153
01:07:47.780 --> 01:07:49.940
workers you know we're gonna launch you

1154
01:07:49.940 --> 01:07:51.440
know if we have a thousand servers we're

1155
01:08:01.460 --> 01:08:04.489
file servers thus getting now tremendous

1156
01:08:04.489 --> 01:08:07.730
total read throughput you know the read

1157
01:08:07.730 --> 01:08:10.960
through put up a thousand servers

1158
01:08:20.989 --> 01:08:23.489
so so are you thinking maybe that Google

1159
01:08:23.489 --> 01:08:25.470
has one set of physical machines among

1160
01:08:25.470 --> 01:08:27.779
GFS and a separate set of physical

1161
01:08:27.779 --> 01:08:40.489
machines that run MapReduce jobs okay

1162
01:08:40.579 --> 01:08:44.789
right so the question is what does this

1163
01:08:44.789 --> 01:08:48.630
arrow here actually involve and the

1164
01:08:48.630 --> 01:08:50.220
answer that actually it sort of changed

1165
01:08:50.220 --> 01:08:51.630
over the years as Google's

1166
01:08:51.630 --> 01:08:55.800
involve this system but you know what

1167
01:08:55.800 --> 01:08:58.199
this in those general case if we have

1168
01:09:02.880 --> 01:09:05.130
is a bit like AFS you might have used on

1169
01:09:05.130 --> 01:09:07.229
Athena where you go talk to some

1170
01:09:07.229 --> 01:09:09.810
collection and your data split over a big

1171
01:09:12.149 --> 01:09:14.579
retrieve your data in that case what

1172
01:09:14.579 --> 01:09:17.840
this arrow might represent is the Map

1173
01:09:17.840 --> 01:09:20.520
MapReduce worker process has to go off

1174
01:09:20.520 --> 01:09:22.649
and talk across the network to the

1175
01:09:22.649 --> 01:09:25.800
correct GFS server or maybe servers that

1176
01:09:25.800 --> 01:09:28.350
store it's part of the input and fetch

1177
01:09:28.350 --> 01:09:30.949
it over the network to the MapReduce

1178
01:09:30.949 --> 01:09:33.449
worker machine in order to pass the map

1179
01:09:33.449 --> 01:09:35.310
and that's certainly the most general

1180
01:09:35.310 --> 01:09:37.920
case and that was eventually how

1181
01:09:37.920 --> 01:09:40.800
MapReduce actually worked in the world

1182
01:09:40.800 --> 01:09:44.819
of this paper though and and if you did

1183
01:09:44.819 --> 01:09:45.930
that that's a lot of network

1184
01:09:45.930 --> 01:09:47.909
communication are you talking about ten

1185
01:09:47.909 --> 01:09:49.350
terabytes of data and we have moved 10

1186
01:09:49.350 --> 01:09:51.600
terabytes across their data center

1187
01:09:51.600 --> 01:09:54.270
network which you know data center

1188
01:09:54.270 --> 01:09:55.739
networks wanting gigabits per second but

1189
01:09:55.739 --> 01:09:57.779
it's still a lot of time to move tens of

1190
01:09:57.779 --> 01:10:02.460
terabytes of data in order to try to and

1191
01:10:02.460 --> 01:10:04.170
indeed in the world of this paper in

1192
01:10:04.170 --> 01:10:07.350
2004 the most constraining bottleneck in

1193
01:10:07.350 --> 01:10:08.850
their MapReduce system was Network

1194
01:10:08.850 --> 01:10:11.609
throughput because they were running on

1195
01:10:11.609 --> 01:10:13.590
a network if you sort of read as far as

1196
01:10:13.590 --> 01:10:18.770
the evaluation section their network

1197
01:10:18.770 --> 01:10:24.750
their network as was they had thousands

1198
01:10:24.750 --> 01:10:27.229
of machines

1199
01:10:27.479 --> 01:10:30.909
whatever and they would collect machines

1200
01:10:30.909 --> 01:10:32.920
they would plug machines and you know

1201
01:10:32.920 --> 01:10:35.109
each rack of machines and you know an

1202
01:10:35.109 --> 01:10:36.520
Ethernet switch for that rack or

1203
01:10:36.520 --> 01:10:38.109
something but then you know they all

1204
01:10:38.109 --> 01:10:40.449
need to talk to each other but there was

1205
01:10:40.449 --> 01:10:43.989
a route Ethernet switch that all of the

1206
01:10:43.989 --> 01:10:45.520
Rockies are net switches talked to and

1207
01:10:45.520 --> 01:10:47.890
this one and you know so if you just

1208
01:10:52.960 --> 01:10:54.880
half the time the communication between

1209
01:10:54.880 --> 01:10:56.199
them has to pass through this one

1210
01:10:56.199 --> 01:10:58.409
wouldn't switch their routes which had

1211
01:10:58.409 --> 01:11:01.479
only some amount of total throughput

1212
01:11:01.479 --> 01:11:05.649
which I forget you know some number of

1213
01:11:05.649 --> 01:11:09.909
gigabits per second and I forget the

1214
01:11:09.909 --> 01:11:13.590
number well but when I did the division

1215
01:11:13.590 --> 01:11:17.890
that is divided up to the total

1216
01:11:17.890 --> 01:11:19.119
throughput available in the routes which

1217
01:11:19.119 --> 01:11:21.640
by the roughly 2000 servers that they

1218
01:11:21.640 --> 01:11:23.770
used in the papers experiments what I

1219
01:11:23.770 --> 01:11:26.170
got was that each machine share of the

1220
01:11:30.609 --> 01:11:36.310
per second in their setup 50 megabits

1221
01:11:36.310 --> 01:11:41.529
per second per machine and then might

1222
01:11:45.430 --> 01:11:47.439
quite small compared to how fast disks

1223
01:11:53.770 --> 01:11:56.439
like a tremendous limit and so they

1224
01:11:56.439 --> 01:11:57.760
really stood on their heads in the

1225
01:12:02.979 --> 01:12:05.859
bunch of tricks to avoid sending stuff

1226
01:12:10.569 --> 01:12:14.380
would they ran the gfs servers and the

1227
01:12:14.380 --> 01:12:16.810
MapReduce workers on the same set of

1228
01:12:27.100 --> 01:12:29.529
thousand machines and then when the

1229
01:12:29.529 --> 01:12:33.430
master was splitting up the map work and

1230
01:12:33.430 --> 01:12:34.630
sort of farming it out to different

1231
01:12:34.630 --> 01:12:39.390
workers it would cleverly when it was

1232
01:12:39.390 --> 01:12:41.550
about to run the map that was going to

1233
01:12:41.550 --> 01:12:44.640
read from input file one it would figure

1234
01:12:44.640 --> 01:12:47.789
out from GFS which server actually holds

1235
01:12:47.789 --> 01:12:50.340
input file one on its local disk and it

1236
01:12:55.710 --> 01:12:59.189
machine so that by default this arrow

1237
01:12:59.189 --> 01:13:01.979
was actually local local read from the

1238
01:13:01.979 --> 01:13:03.449
local disk and did not involve the

1239
01:13:03.449 --> 01:13:05.159
network and you know depending on

1240
01:13:05.159 --> 01:13:07.289
failures or load or whatever that

1241
01:13:11.970 --> 01:13:13.619
machine and stored the data thus saving

1242
01:13:13.619 --> 01:13:17.399
them vast amount of time that they would

1243
01:13:22.770 --> 01:13:26.250
they played is that map as I mentioned

1244
01:13:26.250 --> 01:13:28.470
before stores this output on the local

1245
01:13:28.470 --> 01:13:29.939
disk of the machine that you run the map

1246
01:13:29.939 --> 01:13:31.859
on so again storing the output of the

1247
01:13:31.859 --> 01:13:33.270
map does not require network

1248
01:13:33.270 --> 01:13:35.479
communication he's not immediately

1249
01:13:46.979 --> 01:13:49.649
defined in order to group together all

1250
01:13:49.649 --> 01:13:51.510
of the values associated with the given

1251
01:13:51.510 --> 01:13:55.260
key and pass them to a single invocation

1252
01:13:55.260 --> 01:13:57.750
to produce on some machine this is going

1253
01:13:57.750 --> 01:13:59.939
to require network communication we're

1254
01:13:59.939 --> 01:14:02.189
gonna you know we want to need to fetch

1255
01:14:02.189 --> 01:14:03.840
all these and give them a single

1256
01:14:03.840 --> 01:14:05.970
machine that have to be moved across the

1257
01:14:05.970 --> 01:14:08.670
network and so this shuffle this

1258
01:14:08.670 --> 01:14:11.689
movement of the keys from is kind of

1259
01:14:11.689 --> 01:14:14.850
originally stored by row and on the same

1260
01:14:14.850 --> 01:14:16.739
machine that ran the map we need them

1261
01:14:16.739 --> 01:14:18.779
essentially to be stored on by column on

1262
01:14:18.779 --> 01:14:19.800
the machine that's going to be

1263
01:14:23.609 --> 01:14:25.439
essentially column storage is called the

1264
01:14:25.439 --> 01:14:28.529
paper calls a shuffle and it really that

1265
01:14:28.529 --> 01:14:30.479
required moving every piece of data

1266
01:14:34.470 --> 01:14:36.300
need it and now it's like the expensive

1267
01:14:36.300 --> 01:14:41.869
part of the MapReduce yeah

1268
01:14:51.840 --> 01:14:53.859
you're right you can imagine a different

1269
01:14:53.859 --> 01:14:55.239
definition in which you have a more kind

1270
01:14:55.239 --> 01:14:57.989
of streaming reduce I don't know I

1271
01:15:09.939 --> 01:15:11.979
make it easy to program by people who

1272
01:15:11.979 --> 01:15:13.989
just had no idea of what was going on in

1273
01:15:13.989 --> 01:15:16.659
the system so it may be that you know

1274
01:15:16.659 --> 01:15:18.460
this speck this is really the way reduce

1275
01:15:18.460 --> 01:15:22.659
functions look and you know in C++ or

1276
01:15:22.659 --> 01:15:24.850
something like a streaming version of

1277
01:15:30.189 --> 01:15:33.250
simple but you know maybe it could be

1278
01:15:33.250 --> 01:15:35.319
done that way and indeed many modern

1279
01:15:35.319 --> 01:15:37.960
systems people got a lot more

1280
01:15:37.960 --> 01:15:41.529
sophisticated with modern things that

1281
01:15:41.529 --> 01:15:43.420
are the successors the MapReduce and

1282
01:15:43.420 --> 01:15:45.430
they do indeed involve processing

1283
01:15:45.430 --> 01:15:48.640
streams of data often rather than this

1284
01:15:48.640 --> 01:15:50.739
very batch approach there is a batch

1285
01:15:50.739 --> 01:15:52.779
approach in the sense that we wait until

1286
01:15:52.779 --> 01:15:54.970
we get all the data and then we process

1287
01:15:54.970 --> 01:15:57.250
it so first of all that you then have to

1288
01:15:57.250 --> 01:15:59.670
have a notion of finite inputs right

1289
01:15:59.670 --> 01:16:02.170
modern systems often do indeed you

1290
01:16:02.170 --> 01:16:05.979
streams and and are able to take

1291
01:16:05.979 --> 01:16:08.909
advantage of some efficiencies do that

1292
01:16:08.909 --> 01:16:15.460
MapReduce okay so this is the point at

1293
01:16:15.460 --> 01:16:17.380
which this shuffle is where all the

1294
01:16:17.380 --> 01:16:19.449
network traffic happens this can

1295
01:16:23.920 --> 01:16:26.710
the the output of the sort has the same

1296
01:16:26.710 --> 01:16:29.439
size as the input to the sort so that

1297
01:16:29.439 --> 01:16:30.850
means that if you're you know if your

1298
01:16:30.850 --> 01:16:32.890
input is 10 terabytes of data and you're

1299
01:16:32.890 --> 01:16:34.750
running a sort you're moving 10

1300
01:16:34.750 --> 01:16:36.220
terabytes of data across a network at

1301
01:16:36.220 --> 01:16:38.409
this point and your output will also be

1302
01:16:38.409 --> 01:16:40.779
10 terabytes and so this is quite a lot

1303
01:16:40.779 --> 01:16:42.430
of data and then indeed it is from any

1304
01:16:42.430 --> 01:16:44.140
MapReduce jobs although not all there's

1305
01:16:44.140 --> 01:16:46.449
some that significantly reduce the

1306
01:16:46.449 --> 01:16:49.689
amount of data at these stages somebody

1307
01:16:52.899 --> 01:16:55.149
MapReduce job and indeed that was often

1308
01:16:55.149 --> 01:16:56.979
what people wanted to do and

1309
01:16:56.979 --> 01:16:58.390
in case the output of the reduce might

1310
01:16:58.390 --> 01:17:00.399
be enormous like four sort or web and

1311
01:17:00.399 --> 01:17:03.399
mixing the output of the produces on ten

1312
01:17:03.399 --> 01:17:05.260
terabytes of input the output of the

1313
01:17:05.260 --> 01:17:07.720
reduces again gonna be ten terabytes so

1314
01:17:07.720 --> 01:17:09.250
the output of the reduce is also stored

1315
01:17:09.250 --> 01:17:12.640
on GFS and the system would you know

1316
01:17:12.640 --> 01:17:13.869
reduce would just produce these key

1317
01:17:13.869 --> 01:17:18.369
value pairs but the MapReduce framework

1318
01:17:18.369 --> 01:17:20.319
would gather them up and write them into

1319
01:17:20.319 --> 01:17:23.680
giant files on GFS and so there was

1320
01:17:23.680 --> 01:17:27.489
another round of network communication

1321
01:17:27.489 --> 01:17:30.220
required to get the output of each

1322
01:17:35.229 --> 01:17:37.960
think that they could have played the

1323
01:17:37.960 --> 01:17:39.640
same trick with the output of storing

1324
01:17:39.640 --> 01:17:42.489
the output on the GFS server that

1325
01:17:42.489 --> 01:17:46.449
happened to run the MapReduce worker

1326
01:17:46.449 --> 01:17:48.970
that ran the reduce and maybe they did

1327
01:17:48.970 --> 01:17:51.760
do that but because GFS as well as

1328
01:17:51.760 --> 01:17:53.979
splitting data for performance also

1329
01:17:53.979 --> 01:17:55.930
keeps two or three copies for fault

1330
01:18:05.699 --> 01:18:08.199
and I was this network communication

1331
01:18:10.659 --> 01:18:17.680
in 2004 in 2020 because this network

1332
01:18:17.680 --> 01:18:19.869
arrangement was such a limiting factor

1333
01:18:19.869 --> 01:18:21.789
for so many things people wanted to do

1334
01:18:21.789 --> 01:18:23.920
in datacenters modern data center

1335
01:18:28.960 --> 01:18:30.640
typical data center network you might

1336
01:18:30.640 --> 01:18:32.890
see today actually has many root instead

1337
01:18:32.890 --> 01:18:34.329
of a single root switch that everything

1338
01:18:34.329 --> 01:18:37.630
has to go through you might have you

1339
01:18:37.630 --> 01:18:40.270
know many root switches and each rack

1340
01:18:40.270 --> 01:18:42.460
switch has a connection to each of these

1341
01:18:42.460 --> 01:18:44.529
sort of replicated root switches and the

1342
01:18:44.529 --> 01:18:46.479
traffic is split up among the root

1343
01:18:46.479 --> 01:18:48.600
switches so modern data center networks

1344
01:18:48.600 --> 01:18:52.270
have far more network throughput and

1345
01:18:52.270 --> 01:18:54.880
because of that actually modern I think

1346
01:18:54.880 --> 01:18:57.100
Google sort of stopped using MapReduce a

1347
01:18:57.100 --> 01:19:00.310
few years ago but before they stopped

1348
01:19:00.310 --> 01:19:02.590
using it the modern MapReduce actually

1349
01:19:02.590 --> 01:19:04.960
no longer tried to run the maps on the

1350
01:19:04.960 --> 01:19:06.939
same machine as the data stored on they

1351
01:19:06.939 --> 01:19:08.140
were happy to vote the data from

1352
01:19:08.140 --> 01:19:11.369
anywhere because they just assumed that

1353
01:19:11.369 --> 01:19:16.439
was extremely fast okay we're out of

1354
01:19:16.439 --> 01:19:18.439
time for MapReduce

1355
01:19:18.439 --> 01:19:21.689
we have a lab due at the end of next

1356
01:19:21.689 --> 01:19:22.350
week

1357
01:19:22.350 --> 01:19:24.840
in which you'll write your own somewhat

1358
01:19:24.840 --> 01:19:27.899
simplified MapReduce so have fun with

1359
01:19:27.899 --> 01:19:28.350
that

1360
01:19:28.399 --> 01:19:28.450
and see you on Thursday


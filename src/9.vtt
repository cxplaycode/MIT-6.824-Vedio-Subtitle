WEBVTT

1
00:00:00.730 --> 00:00:03.410
today I want to do two things I want to

2
00:00:03.410 --> 00:00:06.200
finish the discussion of zookeeper and

3
00:00:06.200 --> 00:00:10.609
then talk about crack the particular

4
00:00:10.609 --> 00:00:12.320
things that I'm most interested in

5
00:00:12.320 --> 00:00:15.140
talking about a bad zookeeper are the

6
00:00:15.140 --> 00:00:17.690
design of its API that allows the

7
00:00:17.690 --> 00:00:19.179
zookeeper to be a general-purpose

8
00:00:19.179 --> 00:00:21.649
service that really bites off

9
00:00:21.649 --> 00:00:24.559
significant tasks that distributed

10
00:00:24.559 --> 00:00:27.199
systems need so why is this you know why

11
00:00:27.199 --> 00:00:29.870
is that a good API design and then the

12
00:00:29.870 --> 00:00:33.649
really more specific topic of mini

13
00:00:33.649 --> 00:00:35.179
transactions turns out this is a

14
00:00:35.179 --> 00:00:38.600
worthwhile idea to know so they got API

15
00:00:38.600 --> 00:00:50.119
and I'm just just a recall zookeepers

16
00:00:50.119 --> 00:00:52.159
based on raft and so we can think of it

17
00:00:52.159 --> 00:00:54.950
as being and indeed it is fault tolerant

18
00:00:54.950 --> 00:00:56.420
and does the right thing with respect to

19
00:00:56.420 --> 00:01:00.679
partitions it has this sort of

20
00:01:09.319 --> 00:01:12.560
analyzing various uses of the zookeeper

21
00:01:12.560 --> 00:01:15.650
interface on the other hand zookeeper

22
00:01:15.650 --> 00:01:18.170
does guarantee that every replicas

23
00:01:18.170 --> 00:01:20.689
process the stream of rights in order

24
00:01:20.689 --> 00:01:22.569
one at a time with all replicas

25
00:01:22.569 --> 00:01:24.920
executing the rights in the same order

26
00:01:24.920 --> 00:01:27.980
so that the replicas advance sort of in

27
00:01:27.980 --> 00:01:30.170
their states of all than exactly the

28
00:01:30.170 --> 00:01:33.409
same way and that all of the operation

29
00:01:33.409 --> 00:01:36.709
reads and writes produced by a generated

30
00:01:36.709 --> 00:01:38.659
by a single client or processed by the

31
00:01:43.900 --> 00:01:45.950
successive operations from a given

32
00:01:45.950 --> 00:01:48.859
client always see the same state or

33
00:01:48.859 --> 00:01:51.680
later in the right stream as the

34
00:01:51.680 --> 00:01:53.870
previous read operation right any

35
00:01:53.870 --> 00:01:59.299
operation from that client okay so

36
00:01:59.299 --> 00:02:02.120
before I dive into what the API looks

37
00:02:02.120 --> 00:02:06.200
like and why it's useful it's worth just

38
00:02:06.200 --> 00:02:08.180
thinking about what kinds of problems

39
00:02:08.180 --> 00:02:10.250
zookeeper is aiming to solve or could be

40
00:02:10.250 --> 00:02:12.469
expected to solve so

41
00:02:12.469 --> 00:02:13.819
for me

42
00:02:25.159 --> 00:02:28.250
this is it as an implementation of the

43
00:02:28.250 --> 00:02:31.550
test and set service that vmware ft

44
00:02:31.550 --> 00:02:34.969
required in order for either server to

45
00:02:34.969 --> 00:02:38.569
take over when the other one failed it

46
00:02:38.569 --> 00:02:39.979
was a bit of a mystery in the vmware

47
00:02:39.979 --> 00:02:40.340
paper

48
00:02:40.340 --> 00:02:43.280
what is this test instant service how is

49
00:02:43.280 --> 00:02:45.169
it may you know is it fault tolerant

50
00:02:45.169 --> 00:02:49.819
does it itself tolerate partitions but

51
00:02:49.819 --> 00:02:51.620
zookeeper actually gives us the tools to

52
00:02:51.620 --> 00:02:55.849
write a fault tolerant test and set

53
00:02:55.849 --> 00:03:00.199
service of exactly the kind that vmware

54
00:03:00.199 --> 00:03:03.530
ft needed that is fault tolerant and

55
00:03:03.530 --> 00:03:05.330
does do the right thing under partitions

56
00:03:05.330 --> 00:03:06.830
that's sort of a central kind of thing

57
00:03:06.830 --> 00:03:09.409
that zookeepers doing there's also a

58
00:03:09.409 --> 00:03:10.849
bunch of other ways that turns out

59
00:03:10.849 --> 00:03:12.379
people use it suki was very successful

60
00:03:12.379 --> 00:03:15.860
people use it for a lot of stuff one

61
00:03:15.860 --> 00:03:17.659
kind of thing people use is just to

62
00:03:21.800 --> 00:03:24.189
example the IP address of the current

63
00:03:24.189 --> 00:03:29.120
master for some set of workers this is

64
00:03:29.120 --> 00:03:33.460
just config configuration information

65
00:03:41.120 --> 00:03:43.129
to have everyone agree on who the new

66
00:03:43.129 --> 00:03:45.289
master is and only elect one master even

67
00:03:45.289 --> 00:03:49.340
if there's partitions you can elect a

68
00:03:49.340 --> 00:03:58.580
master using zookeeper primitives if the

69
00:03:58.580 --> 00:04:00.379
master for small amounts of stated

70
00:04:00.379 --> 00:04:02.360
anyway if whatever master you elect

71
00:04:02.360 --> 00:04:03.680
needs to keep some state it needs to

72
00:04:03.680 --> 00:04:06.340
keep it up-to-date like maybe you know

73
00:04:06.340 --> 00:04:09.379
informations such as who the primary is

74
00:04:13.909 --> 00:04:16.100
state in zookeeper it knows new keepers

75
00:04:16.100 --> 00:04:17.569
not going to lose it if the master

76
00:04:17.569 --> 00:04:19.399
crashes and we elect a new master to

77
00:04:19.399 --> 00:04:21.379
replace it that new master can just read

78
00:04:21.379 --> 00:04:22.639
the old master state right out of

79
00:04:22.639 --> 00:04:24.949
zookeeper and rely on it actually being

80
00:04:30.360 --> 00:04:32.310
know MapReduce like systems workers

81
00:04:38.910 --> 00:04:40.740
with systems like MapReduce you can

82
00:04:45.389 --> 00:04:47.939
zookeeper like writing lists of work in

83
00:04:47.939 --> 00:04:49.949
zookeeper and then worker sort of take

84
00:04:49.949 --> 00:04:52.290
those work items one by one out of

85
00:04:52.290 --> 00:04:54.269
zookeeper and delete them as they

86
00:04:54.269 --> 00:04:56.339
complete them but people use zookeeper

87
00:05:03.800 --> 00:05:06.370
yeah

88
00:05:11.959 --> 00:05:16.529
exactly yeah so the question is oh how

89
00:05:16.529 --> 00:05:18.209
people use zookeeper and in generally

90
00:05:18.209 --> 00:05:19.860
yeah you you would if you're running

91
00:05:19.860 --> 00:05:21.389
some big data center and you run all

92
00:05:21.389 --> 00:05:23.160
kinds of stuff in your data center you

93
00:05:28.110 --> 00:05:30.269
up a zookeeper one zookeeper cluster

94
00:05:30.269 --> 00:05:31.949
because this general purpose can be used

95
00:05:31.949 --> 00:05:34.589
for lots of things so you know five or

96
00:05:34.589 --> 00:05:37.410
seven zookeeper replicas and then as you

97
00:05:37.410 --> 00:05:39.540
deploy various services you would design

98
00:05:43.230 --> 00:05:50.759
cluster alright the API zookeeper looks

99
00:05:50.759 --> 00:05:53.420
like a filesystem some levels it's got a

100
00:05:53.420 --> 00:05:56.129
directory hierarchy you know there's a

101
00:05:56.129 --> 00:05:58.620
root directory and then maybe you could

102
00:05:58.620 --> 00:06:01.350
maybe each application has its own sub

103
00:06:01.350 --> 00:06:03.120
directory so maybe the application one

104
00:06:03.120 --> 00:06:05.959
keeps its files here in this directory

105
00:06:05.959 --> 00:06:08.189
app two keeps its files in this

106
00:06:08.189 --> 00:06:11.519
directory and you know these directories

107
00:06:11.519 --> 00:06:12.930
have files and directories underneath

108
00:06:12.930 --> 00:06:13.920
them

109
00:06:13.920 --> 00:06:16.560
one reason for this is just because you

110
00:06:16.560 --> 00:06:18.170
keeper is like just mentioned is a

111
00:06:18.170 --> 00:06:20.639
design to be shared between many

112
00:06:20.639 --> 00:06:23.160
possibly unrelated activities we just

113
00:06:23.160 --> 00:06:25.319
need a naming system to be able to keep

114
00:06:34.949 --> 00:06:36.600
that a lot of convenient ways of using

115
00:06:36.600 --> 00:06:39.509
zookeeper will involve creating multiple

116
00:06:39.509 --> 00:06:42.329
files let's see a couple examples like

117
00:06:42.329 --> 00:06:47.160
this in a few minutes okay so it looks

118
00:06:47.160 --> 00:06:49.319
like a filesystem this is you know not

119
00:06:52.920 --> 00:06:54.509
file system in the sense of mounting it

120
00:06:54.509 --> 00:06:56.250
and running LS and cat and all those

121
00:06:56.250 --> 00:06:58.290
things it's just that internally it

122
00:06:58.290 --> 00:07:00.360
names objects with these path names so

123
00:07:00.360 --> 00:07:04.259
you know one this x y&z here few

124
00:07:04.259 --> 00:07:06.209
different files you know when you talk

125
00:07:06.209 --> 00:07:08.250
to me you send an RPC - zookeeper saying

126
00:07:08.250 --> 00:07:11.189
you know please read this data you would

127
00:07:11.189 --> 00:07:13.350
name the data you want maybe add up to

128
00:07:13.350 --> 00:07:16.920
slash X there's just a sort of

129
00:07:16.920 --> 00:07:21.389
hierarchical naming scheme these these

130
00:07:21.389 --> 00:07:23.779
files and directories are called Z nodes

131
00:07:23.779 --> 00:07:27.620
and it turns out it's there's three

132
00:07:27.620 --> 00:07:30.170
types you have to know about that helps

133
00:07:30.170 --> 00:07:31.519
you keep or solve various problems for

134
00:07:31.519 --> 00:07:33.259
us there's just regular Z nodes where if

135
00:07:33.259 --> 00:07:36.379
you create one it's permanent until you

136
00:07:36.379 --> 00:07:40.370
delete it there's a femoral Z nodes

137
00:07:40.370 --> 00:07:42.800
where if a client creates an ephemeral Z

138
00:07:42.800 --> 00:07:45.709
node zookeeper will delete that

139
00:07:45.709 --> 00:07:48.170
ephemeral Z node if it believes that the

140
00:07:52.639 --> 00:07:54.529
of send a little heartbeat in every once

141
00:07:54.529 --> 00:07:56.360
a while into the zookeeper into

142
00:07:56.360 --> 00:07:57.680
zookeeper say oh I'm still alive I'm

143
00:07:57.680 --> 00:07:59.839
still alive so zookeeper won't delete

144
00:07:59.839 --> 00:08:04.220
their ephemeral files and the last

145
00:08:04.220 --> 00:08:07.240
characteristic files may have is

146
00:08:07.240 --> 00:08:10.759
sequential and that means when you ask

147
00:08:10.759 --> 00:08:12.379
to create a file with a given name what

148
00:08:12.379 --> 00:08:14.720
you actually end up creating is a file

149
00:08:14.720 --> 00:08:16.579
with that name but with a number

150
00:08:16.579 --> 00:08:18.829
appended to the main and zookeeper

151
00:08:18.829 --> 00:08:21.230
guarantees never to repeat a number if

152
00:08:21.230 --> 00:08:23.439
multiple clients try to create

153
00:08:23.439 --> 00:08:25.730
sequential files at the same time and

154
00:08:25.730 --> 00:08:29.930
also to always use montt increasing

155
00:08:29.930 --> 00:08:32.240
numbers for the for the sequence numbers

156
00:08:32.240 --> 00:08:34.429
that are pens to filenames and we'll see

157
00:08:34.429 --> 00:08:37.720
all of these things come up in examples

158
00:08:37.720 --> 00:08:40.700
at one level the operations the RPC

159
00:08:40.700 --> 00:08:44.659
interface that zookeeper exposes is sort

160
00:08:44.659 --> 00:08:47.899
of what you might expect for your files

161
00:08:47.899 --> 00:08:51.350
was to create RPC where you give it a

162
00:08:51.350 --> 00:08:57.919
name really a full path name some

163
00:09:09.200 --> 00:09:11.539
create is that it's exclusive that is

164
00:09:11.539 --> 00:09:13.700
when I send a create into zookeeper ask

165
00:09:13.700 --> 00:09:15.230
it to create a file so you keep your

166
00:09:15.230 --> 00:09:18.139
responds with a yes or no if that file

167
00:09:18.139 --> 00:09:20.210
didn't exist and I'm the first client

168
00:09:20.210 --> 00:09:21.830
who wants to create it zookeeper says

169
00:09:21.830 --> 00:09:23.539
yes and creates the file the file

170
00:09:28.730 --> 00:09:30.529
exclusive create and clients know

171
00:09:30.529 --> 00:09:32.240
whether they were the one client if

172
00:09:32.240 --> 00:09:33.470
multiple clients are trying to create

173
00:09:33.470 --> 00:09:35.389
the same file which we'll see in locking

174
00:09:35.389 --> 00:09:36.740
samples

175
00:09:36.740 --> 00:09:38.389
the clients will know whether they were

176
00:09:38.389 --> 00:09:40.220
the one who actually managed to create

177
00:09:40.220 --> 00:09:43.990
the file

178
00:09:46.179 --> 00:09:54.529
there's also delete and one thing I

179
00:09:54.529 --> 00:09:56.269
didn't mention is ever easy note has a

180
00:09:56.269 --> 00:09:57.830
version as a current version number that

181
00:09:57.830 --> 00:10:01.669
advances as its modified and delete

182
00:10:07.820 --> 00:10:10.309
only do this operation if the files

183
00:10:10.309 --> 00:10:12.320
current version number is the version

184
00:10:12.320 --> 00:10:15.379
that was specified and that'll turn out

185
00:10:15.379 --> 00:10:17.240
to be helpful if you're worried about in

186
00:10:17.240 --> 00:10:18.830
situations where multiple clients might

187
00:10:18.830 --> 00:10:20.809
be trying to do the same operation at

188
00:10:20.809 --> 00:10:23.570
the same time so you can pass a version

189
00:10:23.570 --> 00:10:27.470
saying only delete there's an exists

190
00:10:27.470 --> 00:10:33.350
call oh does this path named Xenu exist

191
00:10:33.350 --> 00:10:36.470
an interesting extra argument is that

192
00:10:36.470 --> 00:10:39.740
you can ask to watch for changes to

193
00:10:39.740 --> 00:10:42.230
whatever path name you specified you can

194
00:10:42.230 --> 00:10:43.759
say does this path name exist and

195
00:10:43.759 --> 00:10:46.549
whether or not exists it exists now if

196
00:10:46.549 --> 00:10:48.259
you set this watch if you pass in true

197
00:10:48.259 --> 00:10:50.450
for this watch flag zookeeper guarantees

198
00:10:50.450 --> 00:10:53.600
to notify the client if anything changes

199
00:10:53.600 --> 00:10:55.429
about that path name like it's created

200
00:10:55.429 --> 00:11:00.909
or deleted or modified and furthermore

201
00:11:00.909 --> 00:11:03.830
the the check for whether the file

202
00:11:03.830 --> 00:11:06.740
exists and the setting of the watch

203
00:11:06.740 --> 00:11:08.990
point of the watching information in the

204
00:11:08.990 --> 00:11:11.899
inside zookeeper or atomic so nothing

205
00:11:11.899 --> 00:11:13.330
can happen between the point at which

206
00:11:13.330 --> 00:11:16.519
the point in the write stream which

207
00:11:16.519 --> 00:11:18.679
zookeeper looks to see whether the path

208
00:11:18.679 --> 00:11:20.450
exists and the point in the write stream

209
00:11:20.450 --> 00:11:23.840
at which zookeeper inserts the watch

210
00:11:23.840 --> 00:11:25.879
into its table and then that's like very

211
00:11:25.879 --> 00:11:31.429
important for for correctness we also

212
00:11:31.429 --> 00:11:37.429
get D then you get a path and again the

213
00:11:37.429 --> 00:11:40.629
watch flag and now the watch just

214
00:11:40.629 --> 00:11:43.970
applies to the contents of that file

215
00:11:43.970 --> 00:11:47.379
there's set data

216
00:11:50.470 --> 00:11:55.909
again path the new data and this

217
00:12:00.110 --> 00:12:02.179
does the right if the current version

218
00:12:02.179 --> 00:12:03.409
number of the file is equal to the

219
00:12:03.409 --> 00:12:10.370
number you passed in okay so let's see

220
00:12:10.370 --> 00:12:13.220
how we use this the first maybe almost

221
00:12:13.220 --> 00:12:14.990
this first very simple example is

222
00:12:14.990 --> 00:12:17.379
supposing we have a file in zookeeper

223
00:12:17.379 --> 00:12:20.120
and we want to store a number in that

224
00:12:20.120 --> 00:12:21.590
file and we want to be able to increment

225
00:12:24.919 --> 00:12:27.980
you know I know gets a request from a

226
00:12:27.980 --> 00:12:29.299
web user or something it's going to

227
00:12:29.299 --> 00:12:34.899
increment that count in zookeeper and

228
00:12:34.899 --> 00:12:36.799
more than one client may want to

229
00:12:36.799 --> 00:12:39.259
increment the count that's the critical

230
00:12:39.259 --> 00:12:47.360
thing so an example so one thing to sort

231
00:12:47.360 --> 00:12:49.700
of get out of the way is whether we

232
00:12:49.700 --> 00:12:52.190
actually need some specialized interface

233
00:12:52.190 --> 00:12:57.110
in order to support client coordination

234
00:12:57.110 --> 00:12:59.149
as opposed to just data this looks like

235
00:13:03.220 --> 00:13:06.230
stuff that dated that typical storage

236
00:13:06.230 --> 00:13:09.679
systems provide so for example some of

237
00:13:09.679 --> 00:13:11.179
you have started and you'll all start

238
00:13:11.179 --> 00:13:13.220
soon Ladd 3 in which you build a key

239
00:13:13.220 --> 00:13:15.950
value store where the two operations are

240
00:13:15.950 --> 00:13:20.210
the only operations are put key value

241
00:13:20.210 --> 00:13:27.409
and so one question is can we do you

242
00:13:27.409 --> 00:13:28.639
know all these things that we might want

243
00:13:28.639 --> 00:13:30.200
to do with zookeeper can we just do them

244
00:13:30.200 --> 00:13:32.179
with lab 3 with a key with a key value

245
00:13:32.179 --> 00:13:35.330
put get interface so supposing for my I

246
00:13:35.330 --> 00:13:38.809
want to implement this count thing maybe

247
00:13:38.809 --> 00:13:40.759
I could implement the count with just

248
00:13:40.759 --> 00:13:43.399
lab threes key value interface so you

249
00:13:43.399 --> 00:13:45.679
might increment the count by saying x

250
00:13:45.679 --> 00:13:49.610
equals get you know whatever key were

251
00:13:49.610 --> 00:13:58.240
using and then put that key an X plus 1

252
00:13:59.549 --> 00:14:08.710
why why is this a bad answer yes yes oh

253
00:14:15.909 --> 00:14:19.690
the abstract way of putting it but one

254
00:14:19.690 --> 00:14:20.950
way of looking at it is that of two

255
00:14:20.950 --> 00:14:22.539
clients both want to increment the

256
00:14:22.539 --> 00:14:24.519
counter at the same time they're both

257
00:14:24.519 --> 00:14:26.440
gonna read they're both gonna use get to

258
00:14:26.440 --> 00:14:28.440
read the old value and get you know ten

259
00:14:28.440 --> 00:14:30.970
those gonna add one to ten and get 11

260
00:14:30.970 --> 00:14:33.340
and I was gonna call put with 11 so now

261
00:14:37.779 --> 00:14:39.190
should have ended up increasing it by

262
00:14:39.190 --> 00:14:44.289
two so that's why the lab three cannot

263
00:14:52.960 --> 00:14:55.870
is not lab 3 or gets are not allowed to

264
00:14:55.870 --> 00:14:57.879
return stale data but in zookeeper reads

265
00:14:57.879 --> 00:15:00.490
can be stale and so if you read a stale

266
00:15:00.490 --> 00:15:02.470
version of the current counter and add

267
00:15:05.830 --> 00:15:09.340
know if 30 values 11 but you're get

268
00:15:09.340 --> 00:15:12.100
returns a stale value of 10 you add 1 to

269
00:15:17.470 --> 00:15:19.600
that we have to worry about that

270
00:15:19.600 --> 00:15:24.659
that gets don't return the latest data

271
00:15:25.169 --> 00:15:32.519
ok so how would you do this in zookeeper

272
00:15:32.519 --> 00:15:36.809
here's how I would do this in zookeeper

273
00:15:40.419 --> 00:15:42.710
it turns out you need to do you need to

274
00:15:42.710 --> 00:15:46.340
wrap this code Siemens in a loop because

275
00:15:57.169 --> 00:15:59.149
current value of the counter and the

276
00:15:59.149 --> 00:16:01.639
current version so we're gonna say X V

277
00:16:01.639 --> 00:16:09.710
equals I'm get data and we need to say

278
00:16:09.710 --> 00:16:11.240
final name I don't care what the file

279
00:16:11.240 --> 00:16:13.669
name is we just say that nice now we get

280
00:16:13.669 --> 00:16:16.940
the well we get a value and a version

281
00:16:45.639 --> 00:16:48.620
and if set data is a set data operation

282
00:16:48.620 --> 00:16:50.179
return true meaning it actually did set

283
00:16:50.179 --> 00:16:52.309
the value we're gonna break otherwise

284
00:16:52.309 --> 00:16:55.299
just go back to the top of the loop

285
00:16:55.299 --> 00:17:00.200
otherwise so what's going on here is

286
00:17:04.940 --> 00:17:07.160
the replicas the set data we send

287
00:17:07.160 --> 00:17:08.390
actually did the zookeeper leader

288
00:17:08.390 --> 00:17:10.670
because all rights go to the leader and

289
00:17:10.670 --> 00:17:12.710
what this means is only set the value to

290
00:17:12.710 --> 00:17:15.319
X plus one if the version with the real

291
00:17:15.319 --> 00:17:19.990
version the latest version is still is V

292
00:17:19.990 --> 00:17:23.599
so if we read fresh data and nothing

293
00:17:23.599 --> 00:17:24.890
else is going on in the system like no

294
00:17:24.890 --> 00:17:26.690
other clients are trying to increment

295
00:17:26.690 --> 00:17:29.180
this then we'll read the latest version

296
00:17:29.180 --> 00:17:31.279
latest value we'll add one to the latest

297
00:17:31.279 --> 00:17:34.309
value specify the latest version and our

298
00:17:34.309 --> 00:17:35.960
set data will be accepted by the leader

299
00:17:35.960 --> 00:17:39.410
and we'll get back a positive reply to

300
00:17:39.410 --> 00:17:42.470
our request after it's committed and

301
00:17:42.470 --> 00:17:45.109
we'll break because we're done if we got

302
00:17:45.109 --> 00:17:47.960
stale data here or this was fresh data

303
00:17:47.960 --> 00:17:50.180
but by the time

304
00:17:50.180 --> 00:17:52.130
our set data got to the leader some

305
00:17:52.130 --> 00:17:55.490
other clients set data and some other

306
00:17:55.490 --> 00:17:56.720
client is trying to increment their set

307
00:17:56.720 --> 00:17:58.640
data got there before us our version

308
00:17:58.640 --> 00:18:00.170
number will no longer be fresh in either

309
00:18:05.509 --> 00:18:08.990
won't break out of the loop and we'll go

310
00:18:08.990 --> 00:18:11.569
back and try again and hopefully we'll

311
00:18:11.569 --> 00:18:25.910
succeed this time yes yes so the

312
00:18:25.910 --> 00:18:27.650
question is could this it's a while loop

313
00:18:27.650 --> 00:18:29.569
or we guaranteed is ever going to finish

314
00:18:29.569 --> 00:18:32.539
and no no we're not really guaranteed

315
00:18:32.539 --> 00:18:36.980
that we're gonna finish in practice you

316
00:18:36.980 --> 00:18:39.619
know so for example if our replicas were

317
00:18:39.619 --> 00:18:42.380
reading from is cut off from the leader

318
00:18:42.380 --> 00:18:45.250
and permanently gives us stale data then

319
00:18:45.250 --> 00:18:47.390
you know maybe this is not gonna work

320
00:18:47.390 --> 00:18:51.730
out but you know but in real life well

321
00:18:51.730 --> 00:18:53.990
in real life the you know leaders

322
00:18:53.990 --> 00:18:56.509
pushing all the replicas towards having

323
00:18:56.509 --> 00:18:58.849
identical data to the leader so you know

324
00:18:58.849 --> 00:19:00.980
if we just got stale data here probably

325
00:19:00.980 --> 00:19:02.720
when we go back you know maybe we should

326
00:19:02.720 --> 00:19:04.759
sleep for 10 milliseconds or something

327
00:19:18.680 --> 00:19:21.170
have a thousand clients all trying to do

328
00:19:21.170 --> 00:19:25.579
increments the risk is that maybe none

329
00:19:31.819 --> 00:19:35.269
think one of the most succeed because

330
00:19:35.269 --> 00:19:37.519
you know the the first one that gets its

331
00:19:37.519 --> 00:19:40.549
set data into the leader will succeed

332
00:19:40.549 --> 00:19:41.839
and the rest will all fail because their

333
00:19:41.839 --> 00:19:43.789
version numbers are all too low and then

334
00:19:43.789 --> 00:19:46.519
the next 999 will put and get data's in

335
00:19:46.519 --> 00:19:48.440
and one of them will succeed so it all

336
00:19:48.440 --> 00:19:50.359
have a sort of N squared complexity to

337
00:19:50.359 --> 00:19:54.380
get through all of the all other clients

338
00:19:54.380 --> 00:19:55.940
which is very damaging but it will

339
00:19:55.940 --> 00:19:57.799
finish eventually and so if you thought

340
00:19:57.799 --> 00:19:59.450
you were gonna have a lot of clients you

341
00:19:59.450 --> 00:20:01.910
would use a different strategy here this

342
00:20:01.910 --> 00:20:02.400
is good

343
00:20:02.400 --> 00:20:17.160
or load situations yes if they fit in

344
00:20:17.160 --> 00:20:18.990
memory it's no problem if they don't fit

345
00:20:18.990 --> 00:20:21.660
memory it's a disaster so yeah when

346
00:20:21.660 --> 00:20:23.460
you're using zookeeper you have to keep

347
00:20:23.460 --> 00:20:26.309
in mind that it's yeah it's great for

348
00:20:26.309 --> 00:20:29.220
100 megabytes of stuff and probably

349
00:20:29.220 --> 00:20:31.500
terrible for 100 gigabytes of stuff so

350
00:20:31.500 --> 00:20:32.910
that's why people think of it as storing

351
00:20:32.910 --> 00:20:35.160
configuration information rather than

352
00:20:35.160 --> 00:20:38.900
their we old data of your big website

353
00:20:38.900 --> 00:20:53.700
yes I mean it's sort of watch into this

354
00:20:53.700 --> 00:20:56.150
sequence

355
00:20:58.740 --> 00:21:04.650
yet that could be so if we want if we

356
00:21:04.650 --> 00:21:06.690
wanted to fix this to work under high

357
00:21:06.690 --> 00:21:13.589
load then you would certainly want to

358
00:21:19.740 --> 00:21:21.809
fixing this would be to insert asleep

359
00:21:21.809 --> 00:21:25.950
here and furthermore double the amount

360
00:21:25.950 --> 00:21:30.569
of it sort of randomized sleep whose

361
00:21:30.569 --> 00:21:33.180
span of randomness doubles each time we

362
00:21:39.470 --> 00:21:42.390
it's actually similar to raft leader

363
00:21:42.390 --> 00:21:44.849
election it's a reasonable strategy for

364
00:21:44.849 --> 00:21:47.220
adapting to an unknown number of

365
00:21:47.220 --> 00:21:54.900
concurrent clients so okay tell me

366
00:21:54.900 --> 00:22:03.299
what's right okay so we're getting data

367
00:22:03.299 --> 00:22:07.250
and then watching its true

368
00:22:17.920 --> 00:22:22.750
so yes so if somebody else modifies the

369
00:22:28.630 --> 00:22:30.609
problem is the timing is not working in

370
00:22:30.609 --> 00:22:32.289
your favor like the amount of time

371
00:22:32.289 --> 00:22:34.690
between when I received the data here

372
00:22:34.690 --> 00:22:37.210
and when I send off the message to the

373
00:22:37.210 --> 00:22:39.730
leader with this new set data is zero

374
00:22:39.730 --> 00:22:42.269
that's how much time will pass here

375
00:22:42.269 --> 00:22:47.740
roughly and if some other client is sent

376
00:22:47.740 --> 00:22:51.670
in increment at about this time it's

377
00:22:51.670 --> 00:22:53.410
actually quite a long time between when

378
00:22:53.410 --> 00:22:54.759
that client sends in the increment and

379
00:22:54.759 --> 00:22:56.200
when it works its way through the leader

380
00:22:56.200 --> 00:22:58.180
and is sent out to the followers and

381
00:22:58.180 --> 00:22:59.799
actually executed the followers and the

382
00:22:59.799 --> 00:23:01.539
followers look it up in their watch

383
00:23:01.539 --> 00:23:03.880
table and send me a notification so I

384
00:23:03.880 --> 00:23:06.509
think

385
00:23:17.589 --> 00:23:25.910
it won't give you any read result or if

386
00:23:25.910 --> 00:23:28.670
you read at a point if you're gonna read

387
00:23:28.670 --> 00:23:30.319
at a point that's after where the

388
00:23:30.319 --> 00:23:32.420
modification occurred that should raise

389
00:23:32.420 --> 00:23:34.819
the watch you'll get the notification of

390
00:23:34.819 --> 00:23:36.200
the watch before you get the read

391
00:23:36.200 --> 00:23:40.519
response but in any case I think nothing

392
00:23:40.519 --> 00:23:42.859
like this could save us because what's

393
00:23:42.859 --> 00:23:45.650
gonna happen is all thousand clients are

394
00:23:45.650 --> 00:23:47.650
gonna do the same thing whatever it is

395
00:23:47.650 --> 00:23:50.390
right they're all gonna do again and set

396
00:23:53.480 --> 00:23:54.829
they're all gonna make the same decision

397
00:23:54.829 --> 00:23:57.890
about well they're all not gonna get to

398
00:23:57.890 --> 00:23:59.359
watch because none of them has done the

399
00:23:59.359 --> 00:24:01.339
put data yet right

400
00:24:01.339 --> 00:24:03.950
so the worst case is all the clients are

401
00:24:03.950 --> 00:24:05.690
starting at the same point they all do a

402
00:24:09.410 --> 00:24:10.700
notification because no change has

403
00:24:10.700 --> 00:24:14.450
occurred they all send a set data RPC to

404
00:24:14.450 --> 00:24:17.900
the leader all thousand of them the

405
00:24:23.359 --> 00:24:24.980
it's too late because they've already

406
00:24:24.980 --> 00:24:29.779
sent the set data so it's possible that

407
00:24:29.779 --> 00:24:33.349
watch could help us here but sort of

408
00:24:33.349 --> 00:24:38.960
straightforward version of watch I have

409
00:24:38.960 --> 00:24:42.230
a feeling if you wanted the the mail

410
00:24:42.230 --> 00:24:43.279
we'll talk about this in a few minutes

411
00:24:43.279 --> 00:24:48.109
but the anon heard the second locking

412
00:24:48.109 --> 00:24:50.990
example absolutely solves this kind of

413
00:24:50.990 --> 00:24:53.150
problem so we could adapt to the second

414
00:24:53.150 --> 00:24:54.769
locking example from the paper to try to

415
00:24:54.769 --> 00:24:57.500
cause the increments to happen one at a

416
00:24:57.500 --> 00:25:00.589
time if there's a huge number of clients

417
00:25:00.589 --> 00:25:03.710
who want to do it other questions about

418
00:25:03.710 --> 00:25:08.569
this example okay this is an example of

419
00:25:13.970 --> 00:25:15.740
in a sense that wow there's you know a

420
00:25:15.740 --> 00:25:17.509
lot of funny stuff happening here the

421
00:25:17.509 --> 00:25:22.160
effect is that once it all succeeds we

422
00:25:22.160 --> 00:25:23.750
have achieved an atomic

423
00:25:23.750 --> 00:25:26.420
read-modify-write of the counter right

424
00:25:26.420 --> 00:25:28.750
the difficulty here

425
00:25:28.750 --> 00:25:35.980
is that it's not atomic the reading the

426
00:25:35.980 --> 00:25:37.420
right the read the modifying the right

427
00:25:37.420 --> 00:25:40.150
are not atomic the thing that we have

428
00:25:40.150 --> 00:25:42.640
pulled off here is that this sequence

429
00:25:42.640 --> 00:25:47.410
once it finishes is atomic right we

430
00:25:47.410 --> 00:25:49.269
actually man and once we have to be on

431
00:25:49.269 --> 00:25:50.980
the pass through this that we succeeded

432
00:25:50.980 --> 00:25:53.759
we managed to read increment and write

433
00:25:53.759 --> 00:25:56.440
without anything else intervening we

434
00:25:56.440 --> 00:25:59.970
managed to do these two steps atomically

435
00:25:59.970 --> 00:26:05.109
and you know this is not because this

436
00:26:05.109 --> 00:26:06.789
isn't a full database transaction like

437
00:26:10.690 --> 00:26:12.250
transaction and then read or write

438
00:26:15.609 --> 00:26:17.470
what and then say end transaction and

439
00:26:17.470 --> 00:26:19.450
the database will cleverly commit the

440
00:26:19.450 --> 00:26:21.430
whole thing as an atomic transaction so

441
00:26:21.430 --> 00:26:22.539
real transactions can be very

442
00:26:22.539 --> 00:26:25.809
complicated zookeeper supports this

443
00:26:25.809 --> 00:26:27.880
extremely simplified version of you know

444
00:26:27.880 --> 00:26:30.880
when you're sort of one we can do it

445
00:26:30.880 --> 00:26:34.539
atomic sort of operations on one piece

446
00:26:34.539 --> 00:26:37.450
of data but it's enough to get increment

447
00:26:37.450 --> 00:26:39.250
and some other things so these are for

448
00:26:39.250 --> 00:26:40.660
that reason since they're not general

449
00:26:40.660 --> 00:26:42.640
but they do provide atomicity these are

450
00:26:42.640 --> 00:26:50.470
often called mini transactions and it

451
00:26:50.470 --> 00:26:52.150
turns out this pattern can be made to

452
00:26:52.150 --> 00:26:54.970
work with various other things too like

453
00:26:54.970 --> 00:26:57.359
if we wanted to do the test and set that

454
00:26:57.359 --> 00:27:00.910
vmware ft requires it can be implemented

455
00:27:00.910 --> 00:27:02.589
with very much this setup you know maybe

456
00:27:02.589 --> 00:27:06.670
the old value if it's zero then we try

457
00:27:06.670 --> 00:27:08.500
to set it to one but give this version

458
00:27:08.500 --> 00:27:10.390
number you know nobody else intervened

459
00:27:10.390 --> 00:27:11.829
and we were the one who actually managed

460
00:27:14.799 --> 00:27:17.230
our request and we win somebody else

461
00:27:17.230 --> 00:27:21.279
changes to one after we read it then the

462
00:27:21.279 --> 00:27:23.140
leader will tell us that we lost so you

463
00:27:23.140 --> 00:27:24.880
can do test and set with this pattern

464
00:27:24.880 --> 00:27:29.920
also and you should remember this is the

465
00:27:29.920 --> 00:27:32.430
strategy

466
00:27:33.500 --> 00:27:38.460
okay alright next example I want to talk

467
00:27:38.460 --> 00:27:42.450
about is these locks and I'm talking

468
00:27:42.450 --> 00:27:43.829
about this because it's in the paper not

469
00:27:43.829 --> 00:27:46.710
because I strongly believe that this

470
00:27:46.710 --> 00:27:52.640
kind of lock is useful but they have

471
00:27:52.640 --> 00:27:56.750
they have an example in which a choir

472
00:27:57.650 --> 00:28:01.859
has a couple steps one we try to create

473
00:28:01.859 --> 00:28:05.789
we have a lock file and we try to create

474
00:28:05.789 --> 00:28:11.490
the lock file now again some file with a

475
00:28:11.490 --> 00:28:17.420
femoral set to true and so if that

476
00:28:17.420 --> 00:28:22.259
succeeds then or not we've acquired the

477
00:28:22.259 --> 00:28:27.240
lock the second step that doesn't

478
00:28:34.920 --> 00:28:36.329
true that means the lock file already

479
00:28:36.329 --> 00:28:37.890
exists I mean somebody else has acquired

480
00:28:37.890 --> 00:28:39.329
the lock and so we want to wait for them

481
00:28:39.329 --> 00:28:40.650
to release the lock and they're gonna

482
00:28:40.650 --> 00:28:42.119
release the lock by deleting this file

483
00:28:42.119 --> 00:28:46.640
so we're gonna watch yes

484
00:28:56.740 --> 00:28:59.529
alright so we're gonna watch we're gonna

485
00:29:15.349 --> 00:29:17.240
and if the file still exists right which

486
00:29:17.240 --> 00:29:18.769
we expect it to because after all they

487
00:29:18.769 --> 00:29:20.180
didn't exist presumably would have

488
00:29:20.180 --> 00:29:21.740
returned here so if it exists we want to

489
00:29:29.750 --> 00:29:39.559
three and a step for go to what so the

490
00:29:39.559 --> 00:29:41.240
usual deal is you know we call create

491
00:29:41.240 --> 00:29:45.650
you know maybe we win if it fails we

492
00:29:45.650 --> 00:29:47.480
wait for whoever owns a lock to release

493
00:29:47.480 --> 00:29:49.789
it we get the watch notification when

494
00:29:49.789 --> 00:29:51.680
the file is deleted at that point this

495
00:29:51.680 --> 00:29:53.240
wait finishes and we go back to Mon and

496
00:29:53.240 --> 00:29:54.589
try to recreate the file hopefully we

497
00:29:54.589 --> 00:29:59.359
will get the file this time okay so we

498
00:29:59.359 --> 00:30:01.569
should ask ourselves questions about

499
00:30:01.569 --> 00:30:04.250
possible interleavings of other clients

500
00:30:04.250 --> 00:30:07.910
activities with our four steps so one we

501
00:30:07.910 --> 00:30:09.529
know for sure we know of already if

502
00:30:16.549 --> 00:30:19.130
to process those two to create rpcs one

503
00:30:19.130 --> 00:30:20.299
at a time in some order

504
00:30:20.299 --> 00:30:23.240
so either mike reid will be executed

505
00:30:23.240 --> 00:30:23.599
first

506
00:30:23.599 --> 00:30:24.980
or the other clients create will be

507
00:30:29.690 --> 00:30:31.549
acquire the lock and the other client is

508
00:30:31.549 --> 00:30:33.920
guaranteed to get a false return and if

509
00:30:33.920 --> 00:30:35.539
there are pcs processed first they'll

510
00:30:35.539 --> 00:30:36.829
get the true return and i'm guaranteed

511
00:30:36.829 --> 00:30:38.660
to get the false return and in either

512
00:30:38.660 --> 00:30:40.130
case the file will be created

513
00:30:40.130 --> 00:30:45.670
so we're okay if we have simultaneous

514
00:30:45.670 --> 00:30:51.170
executions of one another question is

515
00:30:51.170 --> 00:30:54.920
well you know if I if create doesn't

516
00:30:54.920 --> 00:30:57.410
succeed for me and I'm gonna call exists

517
00:30:57.410 --> 00:31:01.130
what happens if the lock is released

518
00:31:01.130 --> 00:31:03.349
actually between the create and the

519
00:31:03.349 --> 00:31:05.890
exists

520
00:31:09.740 --> 00:31:12.150
so this is the reason why I rap I have a

521
00:31:15.869 --> 00:31:19.529
before I call exists because it could

522
00:31:26.640 --> 00:31:30.980
back to this go to one and try again

523
00:31:39.359 --> 00:31:43.619
now releases it just as I call exist or

524
00:31:43.619 --> 00:31:45.900
as the replica I'm talking to is in the

525
00:31:45.900 --> 00:31:49.890
middle of processing my exists requests

526
00:31:49.890 --> 00:31:54.930
and the answer to that is that the

527
00:31:54.930 --> 00:31:57.299
whatever replica I'm looking at you know

528
00:31:57.299 --> 00:32:02.880
it's log or guaranteed that rights occur

529
00:32:02.880 --> 00:32:04.170
in some order right

530
00:32:04.170 --> 00:32:06.210
so the repla I'm talking to it's it's

531
00:32:06.210 --> 00:32:10.460
log its proceeding in some way and my

532
00:32:18.420 --> 00:32:21.420
stream right this is a this is a

533
00:32:21.420 --> 00:32:24.569
read-only request and you know the

534
00:32:24.569 --> 00:32:26.130
problem is that somebody's delete

535
00:32:26.130 --> 00:32:27.990
request is being processed at about this

536
00:32:35.130 --> 00:32:37.950
from the other client and the rep and

537
00:32:37.950 --> 00:32:40.470
you know this is my mind the replica

538
00:32:40.470 --> 00:32:42.150
that I'm talking to zookeeper replicas

539
00:32:47.339 --> 00:32:50.279
completely processed here in which case

540
00:32:50.279 --> 00:32:53.279
the replica sees oh the file still

541
00:32:53.279 --> 00:32:56.839
exists and the replica inserts the watch

542
00:32:56.839 --> 00:32:59.250
information into its watch table at this

543
00:32:59.250 --> 00:33:02.519
point and only then executes the delete

544
00:33:02.519 --> 00:33:03.990
so when the delete comes in were

545
00:33:03.990 --> 00:33:05.880
guaranteed that my watch request is in

546
00:33:05.880 --> 00:33:07.529
the replicas watch table and it will

547
00:33:07.529 --> 00:33:11.250
send me a notification right or my exist

548
00:33:11.250 --> 00:33:15.140
requests is executed here at a point

549
00:33:15.140 --> 00:33:17.759
after the delete happen the file doesn't

550
00:33:17.759 --> 00:33:20.640
exist and so now the call returns true

551
00:33:20.640 --> 00:33:20.970
and

552
00:33:20.970 --> 00:33:23.339
no well actually a watch table entry is

553
00:33:23.339 --> 00:33:27.599
entered but we don't care right so it's

554
00:33:27.599 --> 00:33:28.769
quite important that the rights are

555
00:33:28.769 --> 00:33:32.460
sequenced and that reads happen at

556
00:33:32.460 --> 00:33:54.869
definite points between rights yes well

557
00:33:54.869 --> 00:33:57.150
okay so yes so this is where the exists

558
00:33:57.150 --> 00:33:58.980
is executed the file doesn't exist at

559
00:33:58.980 --> 00:34:01.319
this point exists returns false we don't

560
00:34:01.319 --> 00:34:04.890
wait we go to one we create the file and

561
00:34:04.890 --> 00:34:08.670
return we did install a watch here that

562
00:34:08.670 --> 00:34:10.860
watch will be triggered it doesn't

563
00:34:10.860 --> 00:34:11.820
really matter because we're not really

564
00:34:23.300 --> 00:34:26.809
we're not waiting for it but yeah okay

565
00:34:26.809 --> 00:34:28.940
so the file doesn't exist we go to one

566
00:34:28.940 --> 00:34:31.250
somebody else has created the file we

567
00:34:31.250 --> 00:34:33.199
try to create the file that fails we

568
00:34:33.199 --> 00:34:35.869
install another watch and it's a dis

569
00:34:35.869 --> 00:34:38.570
watch that we're not waiting for so this

570
00:34:38.570 --> 00:34:40.130
way does not a wait for anything to

571
00:34:40.130 --> 00:34:42.260
happen although it doesn't really matter

572
00:34:42.260 --> 00:34:47.960
in the moment it's not harmful to to to

573
00:34:47.960 --> 00:34:49.340
break out of this loop early it's just

574
00:34:49.340 --> 00:34:53.570
wasteful anyway we've all the history

575
00:34:53.570 --> 00:34:57.260
this code leaves watches sort of in the

576
00:34:57.260 --> 00:34:58.579
system and I don't really know what does

577
00:34:58.579 --> 00:35:00.739
my new watch on the same file override

578
00:35:00.739 --> 00:35:03.969
my old watch I'm not actually sure

579
00:35:08.559 --> 00:35:12.260
okay I'm finally this example and the

580
00:35:16.159 --> 00:35:18.139
talked about I mean what we were talking

581
00:35:30.139 --> 00:35:32.360
scheme also suffers from the herd effect

582
00:35:32.360 --> 00:35:35.210
in that if there are a thousand clients

583
00:35:35.210 --> 00:35:37.760
trying to get the lock then the amount

584
00:35:37.760 --> 00:35:40.579
of time that's required to sort of grant

585
00:35:40.579 --> 00:35:43.130
the lock to each one of the thousand

586
00:35:43.130 --> 00:35:44.900
clients is proportional to a thousand

587
00:35:44.900 --> 00:35:47.809
squared because after every release all

588
00:35:47.809 --> 00:35:50.630
of the remaining clients get triggered

589
00:35:50.630 --> 00:35:52.219
by this watch all of the remaining

590
00:35:52.219 --> 00:35:53.869
clients go back up here and send in a

591
00:35:53.869 --> 00:35:55.730
create and so the total number create

592
00:35:55.730 --> 00:35:59.239
our pcs generated is basically a

593
00:35:59.239 --> 00:36:02.690
thousand squared so this suffers from

594
00:36:02.690 --> 00:36:06.170
this herd the whole herd of waiting

595
00:36:06.170 --> 00:36:15.380
clients is beating on zookeeper another

596
00:36:15.380 --> 00:36:17.269
name for this is that it's a non

597
00:36:17.269 --> 00:36:23.119
scalable lock or yeah okay and so the

598
00:36:23.119 --> 00:36:26.840
paper is a real deal and we'll see it

599
00:36:26.840 --> 00:36:31.280
more and in other systems and soon

600
00:36:31.280 --> 00:36:32.989
enough serious end of problems the paper

601
00:36:32.989 --> 00:36:34.670
actually talks about how to solve it

602
00:36:34.670 --> 00:36:36.469
using zookeeper and the interesting

603
00:36:36.469 --> 00:36:37.159
thing is that Zook

604
00:36:37.159 --> 00:36:40.340
it's actually expressive enough to be

605
00:36:40.340 --> 00:36:46.429
able to build a more complex lock scheme

606
00:36:46.429 --> 00:36:48.409
that doesn't suffer from this hurt

607
00:36:48.409 --> 00:36:49.909
effect that even of a thousand clients

608
00:36:49.909 --> 00:36:53.659
are waiting the cost of one client

609
00:36:53.659 --> 00:36:55.250
giving up a lock and another acquiring

610
00:37:02.119 --> 00:37:05.480
complex this is the pseudocode in the

611
00:37:05.480 --> 00:37:08.269
paper in section 2.4 it's on page 6 if

612
00:37:08.269 --> 00:37:23.599
you want to follow along so this is and

613
00:37:23.599 --> 00:37:25.550
so this time there is not a single lock

614
00:37:55.849 --> 00:37:57.949
know maybe only one of us at a time

615
00:37:57.949 --> 00:37:59.269
should be allowed to give a lecture in

616
00:37:59.269 --> 00:38:00.800
this lecture hall if you want to give a

617
00:38:00.800 --> 00:38:02.119
lecture in this lecture hall you first

618
00:38:10.610 --> 00:38:12.590
node and zookeeper but it like nobody

619
00:38:12.590 --> 00:38:14.599
cares about its contents we just need it

620
00:38:21.769 --> 00:38:23.210
piyah this it looks like a file system

621
00:38:23.210 --> 00:38:28.570
but it's really a naming system alright

622
00:38:28.570 --> 00:38:31.429
so step one is we create a sequential

623
00:38:31.429 --> 00:38:33.849
file

624
00:38:37.110 --> 00:38:39.840
and so yeah we give it a prefix name but

625
00:38:39.840 --> 00:38:42.570
what it actually creates is you know if

626
00:38:42.570 --> 00:38:45.539
this is the 27th file sequential file

627
00:38:45.539 --> 00:38:48.269
created with with prefix F you know

628
00:38:48.269 --> 00:38:53.239
maybe we get F 27 or something and and

629
00:38:53.239 --> 00:38:56.219
in the sequenced in the sequence of

630
00:38:56.219 --> 00:38:58.710
writes that zookeeper is it's working

631
00:39:05.699 --> 00:39:08.340
always ascending sequence numbers when

632
00:39:08.340 --> 00:39:15.329
you create a sequential file there was

633
00:39:15.329 --> 00:39:16.769
an operation I left off from the list it

634
00:39:16.769 --> 00:39:18.150
turns out you can get a list of files

635
00:39:18.150 --> 00:39:25.219
you can get a list of files underneath

636
00:39:30.420 --> 00:39:31.559
you can get a list of all the files that

637
00:39:31.559 --> 00:39:33.389
are currently in that directory so we're

638
00:39:33.389 --> 00:39:35.940
gonna list the files let's start with

639
00:39:35.940 --> 00:39:41.579
that you know maybe list f star we get

640
00:39:48.719 --> 00:39:51.539
look at that number if there's no lower

641
00:39:51.539 --> 00:39:54.329
numbered file in this list then we win

642
00:39:54.329 --> 00:39:55.440
and we get the lock

643
00:39:55.440 --> 00:39:57.360
so if our sequential file is the lowest

644
00:39:57.360 --> 00:40:00.869
number file with that name prefix we win

645
00:40:00.869 --> 00:40:10.980
so no lower number we've quired the lock

646
00:40:10.980 --> 00:40:18.420
and we can return if there is one then

647
00:40:18.420 --> 00:40:21.469
again what we want to wait for then

648
00:40:21.469 --> 00:40:23.579
what's going on is that these

649
00:40:23.579 --> 00:40:25.920
sequentially numbered files are setting

650
00:40:25.920 --> 00:40:28.679
up the order in which the lock is going

651
00:40:28.679 --> 00:40:30.630
to be granted to the different clients

652
00:40:30.630 --> 00:40:33.139
so if we're not the winner of the lock

653
00:40:33.139 --> 00:40:35.570
what we need to do is wait for the

654
00:40:35.570 --> 00:40:39.210
previously numbered with the client who

655
00:40:39.210 --> 00:40:41.550
created the previously numbered file to

656
00:40:41.550 --> 00:40:43.289
release to acquire and then release the

657
00:40:43.289 --> 00:40:45.619
lock and we're going to release the lock

658
00:40:45.619 --> 00:40:47.969
the convention for releasing the locking

659
00:40:47.969 --> 00:40:49.880
in this system is for

660
00:40:49.880 --> 00:40:51.260
remove the file to remove your

661
00:40:51.260 --> 00:40:53.480
sequential file so we want to wait for

662
00:40:53.480 --> 00:40:56.329
the previously numbered sequential file

663
00:40:56.329 --> 00:40:59.119
to be deleted and then it's our turn and

664
00:40:59.119 --> 00:41:01.309
we get the lock so we need to call

665
00:41:01.309 --> 00:41:05.900
exists so we're gonna say if the call

666
00:41:05.900 --> 00:41:09.219
exists mostly to set a watch point

667
00:41:09.219 --> 00:41:16.269
so it's you know next lower number file

668
00:41:25.690 --> 00:41:28.550
then so that's step 5

669
00:41:33.980 --> 00:41:35.480
because it already exists we're gonna go

670
00:41:35.480 --> 00:41:41.659
back to listing the yeah the files so

671
00:41:41.659 --> 00:41:44.480
this is a choir releases just I delete

672
00:41:44.480 --> 00:41:47.599
if I acquire the lock I delete my the

673
00:41:47.599 --> 00:41:50.829
file I created complete with my number

674
00:41:50.829 --> 00:41:53.829
yes

675
00:42:03.500 --> 00:42:08.750
is we got the list of files we know the

676
00:42:08.750 --> 00:42:11.269
next lower number file there's a

677
00:42:11.269 --> 00:42:12.860
guarantee of the sequential file

678
00:42:12.860 --> 00:42:15.769
creation is that once filed 27 is

679
00:42:15.769 --> 00:42:18.619
created no file with a lower number will

680
00:42:18.619 --> 00:42:20.960
ever subsequently be created so we now

681
00:42:20.960 --> 00:42:22.730
know nothing else could sneak in here so

682
00:42:22.730 --> 00:42:25.849
how could the next lower number file you

683
00:42:25.849 --> 00:42:27.440
know why why do we need to list again

684
00:42:34.519 --> 00:42:37.630
Britney guess the answer

685
00:42:53.460 --> 00:42:59.400
before we noticed or have died and this

686
00:43:06.960 --> 00:43:13.349
there's an ephemeral file you know even

687
00:43:13.349 --> 00:43:17.280
if we're 27th in line number 26 may have

688
00:43:17.280 --> 00:43:19.860
died before getting the lock if number

689
00:43:19.860 --> 00:43:22.769
26 dies the system automatically deletes

690
00:43:22.769 --> 00:43:25.469
their ephemeral files and so if that

691
00:43:25.469 --> 00:43:27.210
happened now we need to wait for number

692
00:43:27.210 --> 00:43:31.679
25 that is the next you know it if all

693
00:43:31.679 --> 00:43:33.900
files you know 2 through 27 and and

694
00:43:33.900 --> 00:43:35.429
we're 27 if they're all they are and

695
00:43:35.429 --> 00:43:37.650
they're all waiting there's a lock if if

696
00:43:37.650 --> 00:43:39.570
the one before is dies before getting

697
00:43:39.570 --> 00:43:41.579
the lock now we need to wait for the

698
00:43:41.579 --> 00:43:43.980
next next lower number file not because

699
00:43:48.449 --> 00:43:50.670
the files in case our predecessor in the

700
00:43:50.670 --> 00:43:53.579
list of waiting clients turned out to

701
00:43:53.579 --> 00:44:00.800
die yes

702
00:44:02.210 --> 00:44:04.500
if there's no lower numbered file than

703
00:44:04.500 --> 00:44:09.710
you have acquired the lock absolutely

704
00:44:09.710 --> 00:44:15.510
yes how does this not suffer from the

705
00:44:15.510 --> 00:44:20.159
herd effect suppose we have a thousand

706
00:44:20.159 --> 00:44:22.460
clients waiting and currently client

707
00:44:22.460 --> 00:44:24.719
made through the first five hundred and

708
00:44:24.719 --> 00:44:30.300
client five hundred holds the lock every

709
00:44:30.300 --> 00:44:31.949
client waiting every client is sitting

710
00:44:31.949 --> 00:44:36.150
here waiting for an event but only the

711
00:44:36.150 --> 00:44:38.579
client that created file five hundred

712
00:44:38.579 --> 00:44:41.250
and one he's waiting for the vision of

713
00:44:41.250 --> 00:44:44.130
file five hundred so everybody's waiting

714
00:44:44.130 --> 00:44:45.989
for the next lower number so five

715
00:44:45.989 --> 00:44:48.329
hundred is waiting for 499 twenty nine

716
00:44:48.329 --> 00:44:51.840
nine but everybody everybody's waiting

717
00:44:51.840 --> 00:44:53.969
for just one file when I release the

718
00:44:53.969 --> 00:44:56.400
lock there's only one other client the

719
00:44:56.400 --> 00:44:57.840
next higher numbered client that's

720
00:44:57.840 --> 00:44:59.789
waiting for my file so when I release

721
00:44:59.789 --> 00:45:02.630
the lock one client gets a notification

722
00:45:02.630 --> 00:45:07.289
one client goes back and lists the files

723
00:45:07.289 --> 00:45:10.650
one client and one client now has the

724
00:45:10.650 --> 00:45:14.489
lock so the sort of expense you know no

725
00:45:14.489 --> 00:45:15.539
matter how many clients that are the

726
00:45:15.539 --> 00:45:18.659
expense of one of each release and

727
00:45:26.699 --> 00:45:28.559
acquire here is that every single

728
00:45:28.559 --> 00:45:31.559
waiting client is notified and every

729
00:45:31.559 --> 00:45:33.300
single one of them sends a write request

730
00:45:33.300 --> 00:45:38.239
than the create request into zookeeper

731
00:45:42.349 --> 00:45:54.730
oh you're free to get a cup of coffee

732
00:45:54.730 --> 00:45:57.920
yeah I mean this is you know what the

733
00:45:57.920 --> 00:46:00.710
programming interface looks like is not

734
00:46:00.710 --> 00:46:03.500
our business but this is either and

735
00:46:07.159 --> 00:46:09.230
program looks like one is there's some

736
00:46:09.230 --> 00:46:11.929
thread that's actually in a synchronous

737
00:46:11.929 --> 00:46:13.820
wait it's made a function call saying

738
00:46:13.820 --> 00:46:14.869
please acquire this lock and the

739
00:46:14.869 --> 00:46:16.130
function hold doesn't return until the

740
00:46:16.130 --> 00:46:17.840
locks finally acquired or the

741
00:46:17.840 --> 00:46:20.329
notification comes back of much more

742
00:46:23.809 --> 00:46:25.849
zookeeper and don't wait and then

743
00:46:25.849 --> 00:46:28.280
separately there's some way of seeing

744
00:46:28.280 --> 00:46:29.719
well as you keep your said anything

745
00:46:29.719 --> 00:46:32.960
recently or I have some go routine whose

746
00:46:32.960 --> 00:46:34.420
job it is just wait for the next

747
00:46:34.420 --> 00:46:36.980
whatever it is from zookeeper in the

748
00:46:40.369 --> 00:46:41.690
interesting stuff comes up on the apply

749
00:46:41.690 --> 00:46:44.300
channel so that's a more likely way to

750
00:46:44.300 --> 00:46:45.679
structure this but yeah you're totally

751
00:46:45.679 --> 00:46:48.980
either through threading or some sort of

752
00:47:06.500 --> 00:47:11.690
the person before me has neither died

753
00:47:20.869 --> 00:47:22.639
still alive and still waiting for the

754
00:47:22.639 --> 00:47:25.670
lock or still alive and holds the lock

755
00:47:25.670 --> 00:47:28.539
we don't really know

756
00:47:35.510 --> 00:47:38.519
it does it as long as that client 500

757
00:47:38.519 --> 00:47:42.239
still live if if this exists fails that

758
00:47:42.239 --> 00:47:43.739
means one of two things either my

759
00:47:43.739 --> 00:47:45.150
predecessor held the lock and is

760
00:47:45.150 --> 00:47:47.670
released it and deleted their file or my

761
00:47:47.670 --> 00:47:49.619
predecessor didn't hold the lock they

762
00:47:49.619 --> 00:47:52.880
exited and zookeeper deleted their file

763
00:47:52.880 --> 00:47:55.349
because it was an ephemeral file so

764
00:47:55.349 --> 00:47:58.699
there's two reasons to come out of this

765
00:47:58.699 --> 00:48:01.679
to come out of his weight or four they

766
00:48:01.679 --> 00:48:03.690
exist to return false and that's why we

767
00:48:03.690 --> 00:48:08.519
have to like we check everything you

768
00:48:08.519 --> 00:48:09.659
know you really don't know what the

769
00:48:09.659 --> 00:48:13.699
situation is after the exists completes

770
00:48:30.230 --> 00:48:32.670
that might that yeah maybe maybe that

771
00:48:32.670 --> 00:48:33.719
could need to work that sounds

772
00:48:33.719 --> 00:48:34.769
reasonable

773
00:48:39.989 --> 00:48:43.769
release only involves a few clients two

774
00:48:43.769 --> 00:48:45.980
clients

775
00:48:48.940 --> 00:48:52.760
alright this pattern to me actually

776
00:48:52.760 --> 00:48:54.349
first saw this pattern a totally

777
00:48:54.349 --> 00:48:56.929
different context and scalable locks for

778
00:49:02.449 --> 00:49:04.869
of a lock

779
00:49:10.219 --> 00:49:12.650
I find it one of those interesting

780
00:49:12.650 --> 00:49:18.739
constructions I've ever seen now and so

781
00:49:18.739 --> 00:49:20.780
like I'm impressed that zookeeper is

782
00:49:20.780 --> 00:49:22.969
able to express it and it's a valuable

783
00:49:22.969 --> 00:49:28.309
construct having said that I'm a little

784
00:49:28.309 --> 00:49:31.519
bit at sea about why zookeeper about why

785
00:49:38.690 --> 00:49:41.900
like threading locks and go because in

786
00:49:41.900 --> 00:49:43.429
threading there's no notion of threads

787
00:49:43.429 --> 00:49:45.230
failing at least if you don't want them

788
00:49:45.230 --> 00:49:46.519
there to be there's no notions of

789
00:49:46.519 --> 00:49:48.170
threads just sort of randomly dying and

790
00:49:52.429 --> 00:49:54.679
case and go that when you use it if

791
00:49:54.679 --> 00:49:56.989
everybody uses mutexes correctly you are

792
00:49:56.989 --> 00:49:59.260
getting atomicity for the sequence of

793
00:49:59.260 --> 00:50:02.210
operations inside the mutex that you

794
00:50:02.210 --> 00:50:04.760
know if you take out a lock and go and

795
00:50:04.760 --> 00:50:06.590
you do 47 different read and write a lot

796
00:50:06.590 --> 00:50:07.820
of variables and then release the lock

797
00:50:07.820 --> 00:50:09.349
if everybody follows that locking

798
00:50:09.349 --> 00:50:12.559
strategy nobody's ever going to see some

799
00:50:12.559 --> 00:50:14.480
sort of weird intermediate version of

800
00:50:14.480 --> 00:50:16.429
the data as of halfway through you're

801
00:50:16.429 --> 00:50:18.170
updating it right just makes things

802
00:50:18.170 --> 00:50:20.900
atomic no argument these locks aren't

803
00:50:20.900 --> 00:50:22.820
really like that because if the client

804
00:50:22.820 --> 00:50:25.550
that holds the lock fails it just

805
00:50:25.550 --> 00:50:28.309
releases the lock and somebody else can

806
00:50:28.309 --> 00:50:30.349
pick up the lock so it does not

807
00:50:30.349 --> 00:50:33.409
guarantee atomicity because you can get

808
00:50:33.409 --> 00:50:35.300
partial failures and distributed systems

809
00:50:35.300 --> 00:50:37.730
where you don't really get partial

810
00:50:37.730 --> 00:50:41.510
failures of ordinary threaded code so if

811
00:50:41.510 --> 00:50:43.699
the current lock holder had the lock and

812
00:50:43.699 --> 00:50:45.469
needed to update a whole bunch of things

813
00:50:45.469 --> 00:50:46.909
that were protected by that lock before

814
00:50:46.909 --> 00:50:48.860
releasing and only got halfway through

815
00:50:48.860 --> 00:50:51.250
updating this stuff and then crashed

816
00:50:51.250 --> 00:50:53.420
then the lock will get released you'll

817
00:50:53.420 --> 00:50:55.760
get the lock and yet when you go to look

818
00:50:55.760 --> 00:50:58.760
at the data it's garbage because it's

819
00:50:58.760 --> 00:51:00.380
just whatever random seed it was in the

820
00:51:00.380 --> 00:51:00.849
middle of

821
00:51:00.849 --> 00:51:04.599
updated so there's these locks don't by

822
00:51:04.599 --> 00:51:06.519
themselves provide the same atomicity

823
00:51:06.519 --> 00:51:09.909
guarantee that threading locks do and so

824
00:51:09.909 --> 00:51:11.409
we're sort of left to imagine for

825
00:51:11.409 --> 00:51:13.269
ourselves by the paper or why you would

826
00:51:16.960 --> 00:51:21.130
paper so I think if you use locks like

827
00:51:21.130 --> 00:51:22.539
this then you sort in a distributed

828
00:51:22.539 --> 00:51:24.849
system then you have two general options

829
00:51:24.849 --> 00:51:28.239
one is everybody who acquires a lock has

830
00:51:28.239 --> 00:51:30.849
to be prepared to clean up from some

831
00:51:30.849 --> 00:51:33.340
previous disaster right so you acquire

832
00:51:33.340 --> 00:51:35.440
this lock you look at the data you try

833
00:51:35.440 --> 00:51:37.630
to figure out gosh if the previous owner

834
00:51:37.630 --> 00:51:38.800
of a lot crashed

835
00:51:38.800 --> 00:51:41.849
you know when I'm looking at the data

836
00:51:41.849 --> 00:51:44.260
you know how can I fix the data to make

837
00:51:44.260 --> 00:51:46.269
up how can I decide if the previous

838
00:51:46.269 --> 00:51:48.550
owner crashed and what do I do to fix up

839
00:51:48.550 --> 00:51:51.969
the data and you can play that game

840
00:51:51.969 --> 00:51:55.539
especially if the convention is that you

841
00:51:58.900 --> 00:52:00.550
sequence the previous holder crashed

842
00:52:00.550 --> 00:52:04.570
assuming they crashed but it's a you

843
00:52:04.570 --> 00:52:05.710
know it's a tricky game the requires

844
00:52:05.710 --> 00:52:07.869
thought of a kind you don't need for

845
00:52:07.869 --> 00:52:10.690
like thread locking um the other reason

846
00:52:10.690 --> 00:52:12.659
maybe these locks would make sense is if

847
00:52:12.659 --> 00:52:16.269
there's sort of soft locks protecting

848
00:52:16.269 --> 00:52:17.710
something that doesn't really matter

849
00:52:17.710 --> 00:52:20.260
so for example if you're running

850
00:52:26.710 --> 00:52:30.639
sure only one task only one worker

851
00:52:30.639 --> 00:52:33.159
executed each task so workers gonna run

852
00:52:38.440 --> 00:52:42.159
releases it well the way not produce

853
00:52:42.159 --> 00:52:44.469
works it's actually proof against

854
00:52:44.469 --> 00:52:49.809
crashed workers anyway so if you grab a

855
00:52:49.809 --> 00:52:51.159
lock and you crash halfway through your

856
00:52:51.159 --> 00:52:53.440
MapReduce job so what the next person

857
00:52:53.440 --> 00:52:55.239
who gets the lock you know because your

858
00:52:55.239 --> 00:52:56.590
lock will be released when you crash the

859
00:52:56.590 --> 00:52:57.670
next version who gets it will see you

860
00:52:57.670 --> 00:52:59.530
didn't finish the task and just we

861
00:52:59.530 --> 00:53:01.539
execute it and it's just not a problem

862
00:53:01.539 --> 00:53:03.960
because of the way MapReduce is defined

863
00:53:03.960 --> 00:53:05.980
so you could use these locks or some

864
00:53:05.980 --> 00:53:09.130
kind of soft lock thing although anyway

865
00:53:09.130 --> 00:53:11.260
and you know maybe the other thing which

866
00:53:11.260 --> 00:53:13.150
we should be thinking about is that some

867
00:53:13.150 --> 00:53:14.170
version of this

868
00:53:14.170 --> 00:53:17.619
be used to do things like elect a master

869
00:53:17.619 --> 00:53:19.300
but if what we're really doing here is

870
00:53:19.300 --> 00:53:22.510
electing a master you know we could use

871
00:53:22.510 --> 00:53:23.889
code much like this and that would

872
00:53:23.889 --> 00:53:25.570
probably be a reasonable approach yeah

873
00:53:25.570 --> 00:53:42.280
oh yeah yeah yeah so the picking of

874
00:53:42.280 --> 00:53:43.869
paper talk that remember the text in the

875
00:53:43.869 --> 00:53:45.789
paper were says it's going to delete the

876
00:53:45.789 --> 00:53:47.800
ready file and then do a bunch of

877
00:53:47.800 --> 00:53:49.960
updates to files and then recreate the

878
00:53:49.960 --> 00:53:51.780
ready file that would that is a

879
00:53:51.780 --> 00:53:55.269
fantastic way of sort of detecting and

880
00:53:55.269 --> 00:53:57.250
coping with the possibility that the

881
00:53:57.250 --> 00:53:58.929
previous lock held or the previous

882
00:54:02.619 --> 00:54:05.519
never be created

883
00:54:18.400 --> 00:54:21.739
Inigo program yeah sadly that is

884
00:54:21.739 --> 00:54:25.489
possible and you know either okay so the

885
00:54:25.489 --> 00:54:27.559
question is nothing about zookeeper but

886
00:54:27.559 --> 00:54:29.329
if you're writing threaded code and go a

887
00:54:29.329 --> 00:54:32.239
thread acquires a lock could it crash

888
00:54:32.239 --> 00:54:34.489
while holding the lock halfway through

889
00:54:34.489 --> 00:54:37.250
whatever stuff it's supposed to be doing

890
00:54:37.250 --> 00:54:38.420
while holding a lock and the answer is

891
00:54:48.409 --> 00:54:54.800
it and my advice about how to think

892
00:54:54.800 --> 00:54:56.989
about that is that the program is now

893
00:54:56.989 --> 00:55:00.280
broken and you've got to kill it because

894
00:55:00.280 --> 00:55:02.929
in threaded code the way the thing about

895
00:55:02.929 --> 00:55:06.769
locks is that while the lock is held the

896
00:55:06.769 --> 00:55:09.849
invariants in the data don't hold so

897
00:55:09.849 --> 00:55:12.889
there's no way to proceed if the lock

898
00:55:12.889 --> 00:55:15.349
holder crashes there's no safe way to

899
00:55:15.349 --> 00:55:17.480
proceed because all you know is whatever

900
00:55:17.480 --> 00:55:18.860
the invariants were that the lock was

901
00:55:18.860 --> 00:55:23.659
protecting no longer hold so and so and

902
00:55:23.659 --> 00:55:24.889
if you do want to proceed you have to

903
00:55:24.889 --> 00:55:27.409
leave the lock marked as held so that no

904
00:55:27.409 --> 00:55:29.170
one else will ever be able to acquire it

905
00:55:29.170 --> 00:55:31.820
and you know unless you have some clever

906
00:55:31.820 --> 00:55:33.349
idea that's pretty much the way you have

907
00:55:33.349 --> 00:55:35.119
to think about it in a threaded program

908
00:55:35.119 --> 00:55:37.130
because that's kind of the style with

909
00:55:37.130 --> 00:55:38.269
which people write threaded lock

910
00:55:38.269 --> 00:55:40.340
programs if you're super clever you

911
00:55:40.340 --> 00:55:44.780
could play the same kinds of tricks like

912
00:55:44.780 --> 00:55:49.309
this ready flag trick now it's super

913
00:55:49.309 --> 00:55:51.110
hard and go because the memory model

914
00:55:51.110 --> 00:55:54.429
says there is nothing you can count on

915
00:55:54.429 --> 00:55:56.630
except if there's a happens before

916
00:55:56.630 --> 00:55:59.119
relationship so if you play this game of

917
00:55:59.119 --> 00:56:00.949
writing changing some variables and then

918
00:56:00.949 --> 00:56:04.760
setting a done flag that doesn't mean

919
00:56:10.579 --> 00:56:13.190
then can anything be said about the

920
00:56:13.190 --> 00:56:15.530
order in which or in even whether the

921
00:56:15.530 --> 00:56:18.280
updates happen so this is very very hard

922
00:56:18.280 --> 00:56:21.550
it rivairy hard and go to recover from a

923
00:56:21.550 --> 00:56:25.179
crash of a thread that holds the lock

924
00:56:25.179 --> 00:56:30.420
here is maybe a little more possible

925
00:56:44.340 --> 00:56:46.719
it's just two pieces of high bid one is

926
00:56:46.719 --> 00:56:48.400
at these clever ideas for high

927
00:56:52.630 --> 00:56:55.719
consistency and the other interesting

928
00:56:55.719 --> 00:56:57.280
thing uninteresting take-home is that

929
00:57:02.559 --> 00:57:04.900
of coordination service in a way that

930
00:57:04.900 --> 00:57:06.820
simpler schemes like put get interfaces

931
00:57:06.820 --> 00:57:09.219
just can't do so they worked out a set

932
00:57:09.219 --> 00:57:11.559
of functions here that allows you to do

933
00:57:11.559 --> 00:57:13.630
things like write mini transactions and

934
00:57:13.630 --> 00:57:15.940
build your own locks and it all works

935
00:57:15.940 --> 00:57:22.570
out although requires care okay now I

936
00:57:22.570 --> 00:57:24.670
want to turn to today's paper which is

937
00:57:24.670 --> 00:57:34.150
crack the the reason why we're reading a

938
00:57:41.320 --> 00:57:43.599
fault tolerance and as we'll see the

939
00:57:43.599 --> 00:57:46.750
properties you get out of crack or its

940
00:57:46.750 --> 00:57:49.449
predecessor chain replication are very

941
00:57:49.449 --> 00:57:52.750
different in interesting ways from the

942
00:57:52.750 --> 00:57:54.159
properties you get out of a system like

943
00:57:54.159 --> 00:57:58.630
raft and so I'm actually going to talk

944
00:57:58.630 --> 00:58:00.250
about so crack is sort of an

945
00:58:00.250 --> 00:58:01.809
optimization to an older scheme called

946
00:58:01.809 --> 00:58:08.949
chain replication chain replications

947
00:58:12.460 --> 00:58:14.400
that use it

948
00:58:14.400 --> 00:58:16.929
crack is an optimization to it that

949
00:58:16.929 --> 00:58:18.610
actually does a similar trick -

950
00:58:18.610 --> 00:58:20.110
zookeeper where it's trying to increase

951
00:58:20.110 --> 00:58:24.909
weed throughput by allowing reads to two

952
00:58:24.909 --> 00:58:26.769
replicas to any replicas so that you get

953
00:58:26.769 --> 00:58:29.590
you know number of replicas factor of

954
00:58:29.590 --> 00:58:32.320
increase in the read performance the

955
00:58:32.320 --> 00:58:34.630
interesting thing about crack is that it

956
00:58:34.630 --> 00:58:39.760
does that while preserving

957
00:58:39.760 --> 00:58:41.420
linearise ability

958
00:58:41.420 --> 00:58:43.519
unlike zookeeper which you know it

959
00:58:43.519 --> 00:58:44.719
seemed like in order to be able to read

960
00:58:47.690 --> 00:58:50.590
linearizable crack actually manages to

961
00:58:50.590 --> 00:58:53.869
do these reads from any replica while

962
00:58:53.869 --> 00:58:56.150
preserving strong consistency I'm just

963
00:58:56.150 --> 00:59:00.289
pretty interesting okay so first I want

964
00:59:00.289 --> 00:59:01.789
to talk about the older system chain

965
00:59:11.989 --> 00:59:13.219
copies you want to make sure they all

966
00:59:13.219 --> 00:59:14.750
seen the same sequence of right so it's

967
00:59:14.750 --> 00:59:17.690
like a very familiar basic idea but it's

968
00:59:17.690 --> 00:59:21.409
a different topology then raft so the

969
00:59:21.409 --> 00:59:25.639
idea is that there's a chain of servers

970
00:59:25.639 --> 00:59:29.480
and chain replication and the first one

971
00:59:29.480 --> 00:59:32.900
is called the head last one's called the

972
00:59:32.900 --> 00:59:36.889
tail when a right comes in when a client

973
00:59:36.889 --> 00:59:39.429
wants to write something say some client

974
00:59:39.429 --> 00:59:42.380
it sends always Albright's get sent to

975
00:59:42.380 --> 00:59:46.489
the head the head updates its or

976
00:59:46.489 --> 00:59:48.409
replaces its current copy of the data

977
00:59:48.409 --> 00:59:49.820
that the clients writing so you can

978
00:59:49.820 --> 00:59:54.650
imagine be go put key value store so you

979
00:59:54.650 --> 00:59:56.599
know if everybody started out with you

980
00:59:56.599 --> 00:59:58.940
know version a of the data and under

981
00:59:58.940 --> 01:00:01.219
chain replication when the head process

982
01:00:01.219 --> 01:00:02.539
is the right and maybe we're writing

983
01:00:02.539 --> 01:00:04.550
value B you know the head just replaces

984
01:00:04.550 --> 01:00:07.760
its a with a B and passes the right down

985
01:00:07.760 --> 01:00:11.150
the chain as each node sees the right it

986
01:00:11.150 --> 01:00:13.929
replaces over writes its copy the data

987
01:00:13.929 --> 01:00:17.269
the new data when the right gets the

988
01:00:17.269 --> 01:00:21.710
tail the tail sends the reply back to

989
01:00:21.710 --> 01:00:23.449
the client saying we completed your

990
01:00:23.449 --> 01:00:25.269
right

991
01:00:25.269 --> 01:00:30.949
that's how rights work reads if a client

992
01:00:30.949 --> 01:00:33.500
wants to do a read it sends the read to

993
01:00:33.500 --> 01:00:35.869
the tail the read request of the tail

994
01:00:35.869 --> 01:00:38.150
and the tail just answers out of its

995
01:00:38.150 --> 01:00:40.309
current state so if we ask for this

996
01:00:40.309 --> 01:00:42.260
whatever this object was the tail which

997
01:00:42.260 --> 01:00:45.500
is I hope current values be weeds are a

998
01:00:52.500 --> 01:00:55.119
okay so it should think for a moment

999
01:00:55.119 --> 01:00:59.320
like why to chain chain replication so

1000
01:00:59.320 --> 01:01:01.179
this is not crack just to be clear this

1001
01:01:01.179 --> 01:01:03.130
is chain replication chain replication

1002
01:01:10.329 --> 01:01:12.880
can essentially view it as really than

1003
01:01:12.880 --> 01:01:14.489
the purposes of thinking about

1004
01:01:14.489 --> 01:01:16.809
consistency it's just this one server

1005
01:01:16.809 --> 01:01:19.210
the server sees all the rights and it

1006
01:01:19.210 --> 01:01:21.130
sees all the reads and process them one

1007
01:01:21.130 --> 01:01:24.130
at a time and you know a read will just

1008
01:01:24.130 --> 01:01:25.780
see the latest value that's written and

1009
01:01:25.780 --> 01:01:27.130
that's pretty much all there is to it

1010
01:01:27.130 --> 01:01:29.289
from the point of view look if there's

1011
01:01:29.289 --> 01:01:33.360
no crashes what the consistency is like

1012
01:01:34.829 --> 01:01:45.369
pretty simple the failure recovery the a

1013
01:01:45.369 --> 01:01:47.889
lot of the rationale behind chain

1014
01:01:47.889 --> 01:01:51.639
replication is that the set of states

1015
01:01:51.639 --> 01:01:53.679
you can see when after there's a failure

1016
01:01:58.389 --> 01:02:01.900
writes get propagated and at a high

1017
01:02:01.900 --> 01:02:03.429
level what's going on is that any

1018
01:02:03.429 --> 01:02:05.920
committed write that is any rate that

1019
01:02:05.920 --> 01:02:07.690
could have been acknowledged to a client

1020
01:02:07.690 --> 01:02:09.909
to the writing client or any rate that

1021
01:02:09.909 --> 01:02:12.570
could have been exposed in a read

1022
01:02:12.570 --> 01:02:14.469
that'll neither of those will ever

1023
01:02:14.469 --> 01:02:16.179
happen unless that write reached the

1024
01:02:16.179 --> 01:02:17.769
tail in order for it to reach the tail

1025
01:02:17.769 --> 01:02:19.750
it had to a pass through them in process

1026
01:02:19.750 --> 01:02:22.630
by every single node in the chain so we

1027
01:02:22.630 --> 01:02:24.219
know that if we ever exposed to write

1028
01:02:24.219 --> 01:02:26.889
ever acknowledged write ever use it to a

1029
01:02:26.889 --> 01:02:29.260
read that means every single node in the

1030
01:02:29.260 --> 01:02:33.489
tail must know about that right we don't

1031
01:02:33.489 --> 01:02:34.960
get these situations like if you'll call

1032
01:02:34.960 --> 01:02:37.929
figure seven figure eight and RAF paper

1033
01:02:37.929 --> 01:02:39.639
where you can have just hair-raising

1034
01:02:39.639 --> 01:02:41.619
complexity and how the different

1035
01:02:41.619 --> 01:02:44.469
replicas differ if there's a crash here

1036
01:02:44.469 --> 01:02:47.949
you know either that it is committed or

1037
01:02:47.949 --> 01:02:49.929
it before the crash should reach some

1038
01:02:49.929 --> 01:02:52.539
point and nowhere after that point

1039
01:02:52.539 --> 01:02:53.949
because the progress of rights has

1040
01:02:53.949 --> 01:02:55.690
always menu so committed rights are

1041
01:02:55.690 --> 01:02:57.610
always known everywhere if a right isn't

1042
01:02:57.610 --> 01:02:58.690
committed that means that before

1043
01:02:58.690 --> 01:03:00.550
whatever crash it was that disturb the

1044
01:03:00.550 --> 01:03:01.809
system the rate of got into a certain

1045
01:03:04.679 --> 01:03:07.190
point there's really the only two setups

1046
01:03:07.190 --> 01:03:12.440
and at a high level failure recovery is

1047
01:03:12.440 --> 01:03:16.400
relatively simple also if the head fails

1048
01:03:16.400 --> 01:03:19.260
then to a first approximation the next

1049
01:03:19.260 --> 01:03:21.530
node can simply take over his head and

1050
01:03:21.530 --> 01:03:24.840
nothing else needs to get done because

1051
01:03:24.840 --> 01:03:27.210
any rate that made it as far as the

1052
01:03:27.210 --> 01:03:28.619
second node while it was the head that

1053
01:03:32.519 --> 01:03:34.559
made it to the head before a crash but

1054
01:03:34.559 --> 01:03:36.690
the head didn't forward it well that's

1055
01:03:39.869 --> 01:03:41.369
it an acknowledgment to the writing

1056
01:03:41.369 --> 01:03:43.199
client because the write didn't get down

1057
01:03:43.199 --> 01:03:45.389
here so we're not obliged to do anything

1058
01:03:45.389 --> 01:03:47.639
about a write it only reached a crashed

1059
01:03:47.639 --> 01:03:50.309
head before it failed I may be the

1060
01:03:50.309 --> 01:03:52.739
client where we sinned but you know not

1061
01:03:58.949 --> 01:04:01.380
next node can directly take over because

1062
01:04:01.380 --> 01:04:04.920
everything the tale knew then next the

1063
01:04:04.920 --> 01:04:06.630
node just before it also knows because

1064
01:04:06.630 --> 01:04:08.550
the tale only hears things from the node

1065
01:04:08.550 --> 01:04:14.699
just before it and it's a little bit

1066
01:04:14.699 --> 01:04:16.230
complex of an intermediate node fails

1067
01:04:16.230 --> 01:04:18.809
but basically what needs to be done is

1068
01:04:18.809 --> 01:04:20.369
we need to drop it from the chain and

1069
01:04:20.369 --> 01:04:22.349
now there may be rights that it had

1070
01:04:22.349 --> 01:04:24.150
received that the next node hasn't

1071
01:04:24.150 --> 01:04:27.389
received yet and so if we drop a note

1072
01:04:27.389 --> 01:04:29.639
out of the chain the predecessor may

1073
01:04:29.639 --> 01:04:33.510
need to resend recent rights to the to

1074
01:04:33.510 --> 01:04:37.739
its new successor right that's the

1075
01:04:37.739 --> 01:04:41.400
recovery in a nutshell that's for why

1076
01:04:41.400 --> 01:04:45.510
this construction why this instead of

1077
01:04:45.510 --> 01:04:47.550
something else like why this verse is

1078
01:04:47.550 --> 01:04:54.829
wrapped for example the performance

1079
01:05:01.590 --> 01:05:03.269
of you know some number of replicas

1080
01:05:03.269 --> 01:05:05.579
right with the leader it's not in a

1081
01:05:05.579 --> 01:05:07.769
chain we got these the replicas are all

1082
01:05:07.769 --> 01:05:09.989
directly fed by the leader so if a

1083
01:05:09.989 --> 01:05:11.969
client right comes in or a client read

1084
01:05:11.969 --> 01:05:15.599
for that matter the the leader has to

1085
01:05:15.599 --> 01:05:18.320
send it itself to each of the replicas

1086
01:05:18.320 --> 01:05:20.639
whereas in chain replication the leader

1087
01:05:20.639 --> 01:05:22.349
on the head only has to do once and

1088
01:05:22.349 --> 01:05:23.789
these cents on the network are actually

1089
01:05:23.789 --> 01:05:26.849
reasonably expensive and so that means

1090
01:05:26.849 --> 01:05:28.800
the load on a raft leader is going to be

1091
01:05:28.800 --> 01:05:31.170
higher than the load on a chain

1092
01:05:31.170 --> 01:05:33.539
replication leader and so that means

1093
01:05:33.539 --> 01:05:37.349
that you know as the number of client

1094
01:05:51.329 --> 01:05:53.550
interesting difference between chain

1095
01:05:53.550 --> 01:05:55.800
replication and raft is that the reeds

1096
01:05:55.800 --> 01:05:59.460
in raft are all also required to be

1097
01:05:59.460 --> 01:06:01.110
processed by the leaders the leader sees

1098
01:06:01.110 --> 01:06:02.579
every single request from clients

1099
01:06:02.579 --> 01:06:04.829
where's here the head sees everybody

1100
01:06:04.829 --> 01:06:08.190
sees all the rights but only a tail sees

1101
01:06:08.190 --> 01:06:11.130
the reed requests so there may be an

1102
01:06:11.130 --> 01:06:12.539
extent to which the load is sort of

1103
01:06:12.539 --> 01:06:13.769
split between the head and the tail

1104
01:06:13.769 --> 01:06:17.599
rather than concentrated in the leader

1105
01:06:17.599 --> 01:06:24.929
and and as I mentioned before the

1106
01:06:24.929 --> 01:06:27.869
failure different sort of analysis

1107
01:06:27.869 --> 01:06:28.679
required to think about different

1108
01:06:28.679 --> 01:06:30.900
failure scenarios is a good deal simpler

1109
01:06:30.900 --> 01:06:32.690
and chain replication than it is and

1110
01:06:32.690 --> 01:06:35.400
raft and as a big motivation because

1111
01:06:35.400 --> 01:06:39.769
it's hard to get this stuff correct yes

1112
01:06:45.340 --> 01:06:48.199
yeah so if the tale fails but its

1113
01:06:48.199 --> 01:06:50.360
predecessor had seen a right that the

1114
01:06:50.360 --> 01:06:52.460
tale hadn't seen then the failure of

1115
01:06:56.150 --> 01:06:58.579
the new tale and so he could respond to

1116
01:06:58.579 --> 01:07:00.579
the client it probably won't because it

1117
01:07:00.579 --> 01:07:04.369
you know it wasn't a tail when it

1118
01:07:04.369 --> 01:07:07.159
received the right and so the client may

1119
01:07:07.159 --> 01:07:08.929
resend the right and that's too bad and

1120
01:07:08.929 --> 01:07:10.849
so we need duplicate suppression

1121
01:07:10.849 --> 01:07:14.960
probably at the head basically all the

1122
01:07:14.960 --> 01:07:16.940
systems were talking about require in

1123
01:07:39.559 --> 01:07:42.289
makes the decisions about how to that's

1124
01:07:42.289 --> 01:07:45.880
a outstanding question the question is

1125
01:07:45.880 --> 01:07:48.170
or rephrase the question a bit if

1126
01:07:48.170 --> 01:07:51.679
there's a failure like or suppose the

1127
01:07:51.679 --> 01:07:54.230
second node stops being able to talk to

1128
01:07:54.230 --> 01:07:58.400
the head can this second node just take

1129
01:08:02.510 --> 01:08:04.309
take over his head and tell clients to

1130
01:08:04.309 --> 01:08:06.710
talk to me instead of the old head but

1131
01:08:06.710 --> 01:08:11.139
what do you think that's not like a plan

1132
01:08:15.139 --> 01:08:17.789
with the usual assumptions we make about

1133
01:08:17.789 --> 01:08:20.520
how the network behaves that's a recipe

1134
01:08:20.520 --> 01:08:24.510
for split brain right if you do exactly

1135
01:08:24.510 --> 01:08:26.369
what I said because of course what

1136
01:08:26.369 --> 01:08:28.859
really happened was that look the

1137
01:08:28.859 --> 01:08:31.260
network failed here the head is totally

1138
01:08:31.260 --> 01:08:33.329
alive and the head thinks its successor

1139
01:08:33.329 --> 01:08:35.939
has died you know the successors

1140
01:08:35.939 --> 01:08:37.170
actually alive it thinks the head has

1141
01:08:37.170 --> 01:08:39.390
died and they both say well gosh that

1142
01:08:39.390 --> 01:08:40.979
other server seems to have died I'm

1143
01:08:40.979 --> 01:08:42.960
gonna take over and the head is gonna

1144
01:08:42.960 --> 01:08:44.699
say oh I'll just be a sole replica and I

1145
01:08:44.699 --> 01:08:47.489
you know act as the head and the tail

1146
01:08:47.489 --> 01:08:49.199
because the rest of the change seems to

1147
01:08:49.199 --> 01:08:50.909
have gone away and second I'll do the

1148
01:08:50.909 --> 01:08:51.810
same thing and now we have two

1149
01:08:51.810 --> 01:08:55.920
independent split brain versions of the

1150
01:08:55.920 --> 01:08:57.659
data which will gradually get out of

1151
01:08:57.659 --> 01:09:04.529
sync so this construction is not proof

1152
01:09:10.920 --> 01:09:13.439
brain and what that means in practice is

1153
01:09:13.439 --> 01:09:16.529
if it cannot be used by itself it's like

1154
01:09:19.939 --> 01:09:24.149
replication story so it's it's very

1155
01:09:43.560 --> 01:09:46.500
single story about who constitutes the

1156
01:09:49.739 --> 01:09:51.239
and some people think the chain is this

1157
01:09:51.239 --> 01:09:53.579
other node so what's that's usually

1158
01:09:53.579 --> 01:10:00.239
called as a configuration manager and

1159
01:10:00.239 --> 01:10:02.970
its job is just a monitor aliveness and

1160
01:10:02.970 --> 01:10:05.369
every time it sees of all the servers

1161
01:10:05.369 --> 01:10:06.779
every time Isis every time the

1162
01:10:06.779 --> 01:10:08.670
configuration manager thinks the

1163
01:10:08.670 --> 01:10:10.859
server's dead it sends out a new

1164
01:10:10.859 --> 01:10:13.680
configuration in which you know that

1165
01:10:19.979 --> 01:10:21.569
configuration manager thinks is that may

1166
01:10:21.569 --> 01:10:22.800
or may not be dead but we don't care

1167
01:10:22.800 --> 01:10:25.619
because everybody is required to follow

1168
01:10:25.619 --> 01:10:26.960
then your configuration

1169
01:10:26.960 --> 01:10:29.489
and so there can't be any disagreement

1170
01:10:29.489 --> 01:10:31.890
because there's only one party making

1171
01:10:31.890 --> 01:10:33.359
these decisions not going to disagree

1172
01:10:33.359 --> 01:10:35.430
with itself of course how do you make a

1173
01:10:35.430 --> 01:10:36.810
service that's fault tolerant and

1174
01:10:36.810 --> 01:10:38.369
doesn't disagree with itself but doesn't

1175
01:10:38.369 --> 01:10:39.779
suffer from split brain if there's

1176
01:10:39.779 --> 01:10:41.369
network partitions and the answer to

1177
01:10:41.369 --> 01:10:43.380
that is that the configuration manager

1178
01:10:43.380 --> 01:10:49.109
usually uses wrath or paxos or in the

1179
01:10:49.109 --> 01:10:52.260
case of crack zookeeper which itself of

1180
01:10:52.260 --> 01:10:56.600
course is built on a raft like scheme so

1181
01:10:56.600 --> 01:11:00.449
so you to the usual complete set up in

1182
01:11:00.449 --> 01:11:01.800
your data center is it you have a

1183
01:11:01.800 --> 01:11:04.430
configuration manager it's it's based on

1184
01:11:04.430 --> 01:11:06.899
or after PACs or whatever so it's fault

1185
01:11:06.899 --> 01:11:09.359
tolerant and does not suffer from split

1186
01:11:09.359 --> 01:11:11.399
brain and then you split up your data

1187
01:11:11.399 --> 01:11:13.890
over a bunch of change if you know room

1188
01:11:13.890 --> 01:11:15.899
with a thousand servers in it and you

1189
01:11:15.899 --> 01:11:20.699
have you know chain a you know it's

1190
01:11:20.699 --> 01:11:22.560
these servers or the configuration

1191
01:11:22.560 --> 01:11:25.680
manager decides that the change should

1192
01:11:38.850 --> 01:11:40.229
all the clients know all the servers

1193
01:11:40.229 --> 01:11:44.189
know and the individual servers opinions

1194
01:11:44.189 --> 01:11:46.319
about whether other servers are alive or

1195
01:11:46.319 --> 01:11:48.420
dead are totally neither here nor there

1196
01:11:48.420 --> 01:11:54.420
if this server really does die then then

1197
01:11:58.439 --> 01:12:00.329
configuration from the configuration

1198
01:12:00.329 --> 01:12:02.670
manager not allowed to make decisions

1199
01:12:02.670 --> 01:12:07.949
about who's alive and who's dead what's

1200
01:12:07.949 --> 01:12:09.199
that

1201
01:12:09.199 --> 01:12:12.289
oh boy you've got a serious problem so

1202
01:12:15.920 --> 01:12:18.520
different power supplies the whole works

1203
01:12:18.520 --> 01:12:22.819
but this this construction I've set up

1204
01:12:22.819 --> 01:12:24.979
here it's extremely common and it's how

1205
01:12:24.979 --> 01:12:26.840
chain replication is intended to be used

1206
01:12:26.840 --> 01:12:29.869
how cracks intend to be used and the

1207
01:12:29.869 --> 01:12:33.189
logic of it is that like chain require

1208
01:12:33.189 --> 01:12:35.479
replication if you don't have to worry

1209
01:12:35.479 --> 01:12:37.909
about partition and split brain you can

1210
01:12:37.909 --> 01:12:39.979
build very high speed efficient

1211
01:12:39.979 --> 01:12:41.930
replication systems using chain

1212
01:12:41.930 --> 01:12:43.609
replication for example so these

1213
01:12:43.609 --> 01:12:48.710
individual you know data replication and

1214
01:12:48.710 --> 01:12:50.600
we're sharding the data over many chains

1215
01:12:50.600 --> 01:12:52.189
individually this these chains can be

1216
01:12:52.189 --> 01:12:54.859
built to be just the most efficient

1217
01:12:54.859 --> 01:12:56.569
scheme for the particular kind of thing

1218
01:12:56.569 --> 01:12:58.279
that you're replicating you may read

1219
01:12:58.279 --> 01:13:00.409
heavy right heavy whatever but we don't

1220
01:13:00.409 --> 01:13:01.699
have to worry too much about partitions

1221
01:13:01.699 --> 01:13:04.430
and then all that worry is concentrated

1222
01:13:17.970 --> 01:13:23.529
okay so your question is why are we

1223
01:13:23.529 --> 01:13:25.689
using chain replication here instead of

1224
01:13:25.689 --> 01:13:33.100
raft okay so it's like a totally

1225
01:13:40.779 --> 01:13:42.579
construction because even if we're using

1226
01:13:42.579 --> 01:13:48.510
raft here we still need one party to

1227
01:14:02.649 --> 01:14:03.220
the data

1228
01:14:03.220 --> 01:14:04.989
somebody needs to decide how the data is

1229
01:14:04.989 --> 01:14:07.210
assigned to the different replication

1230
01:14:07.210 --> 01:14:08.829
groups this has to change over time as

1231
01:14:08.829 --> 01:14:10.479
you get more or less Hardware more data

1232
01:14:10.479 --> 01:14:12.640
or whatever so if nothing else the

1233
01:14:12.640 --> 01:14:14.560
configuration manager is saying well

1234
01:14:14.560 --> 01:14:16.329
look you know the keys start with a or B

1235
01:14:16.329 --> 01:14:20.109
goes here or then C or D goes here even

1236
01:14:23.920 --> 01:14:24.760
you know what should we use for

1237
01:14:24.760 --> 01:14:26.979
replication should be chain replication

1238
01:14:26.979 --> 01:14:33.970
or paxos or raft or whatever and people

1239
01:14:33.970 --> 01:14:36.550
do different things some people do

1240
01:14:36.550 --> 01:14:38.739
actually use Paxos based replication

1241
01:14:38.739 --> 01:14:40.149
like spanner which I think we're gonna

1242
01:14:40.149 --> 01:14:43.210
look at later in the semester has this

1243
01:14:43.210 --> 01:14:45.670
structure but it actually uses Paxos to

1244
01:14:45.670 --> 01:14:49.210
replicate rights for the data you know

1245
01:14:49.210 --> 01:14:50.710
the reason why you might not want to use

1246
01:14:50.710 --> 01:14:55.720
PAC so so raft is that it's arguably

1247
01:14:55.720 --> 01:14:57.430
more efficient to use this chain

1248
01:14:57.430 --> 01:14:59.289
construction because it reduces the load

1249
01:14:59.289 --> 01:15:01.659
on the leader and that may or may not be

1250
01:15:01.659 --> 01:15:07.260
a critical issue the a reason to favor

1251
01:15:07.260 --> 01:15:11.590
rafter Paxos is that they do not have to

1252
01:15:11.590 --> 01:15:14.170
wait for a lagging replica this chain

1253
01:15:14.170 --> 01:15:15.970
replication has a performance problem

1254
01:15:20.350 --> 01:15:22.479
you know because every rate has to go

1255
01:15:22.479 --> 01:15:24.550
through every replica even a single slow

1256
01:15:24.550 --> 01:15:27.279
replica slows down all offer all right

1257
01:15:27.279 --> 01:15:29.220
operations and I can be very damaging

1258
01:15:29.220 --> 01:15:30.840
you know if you have thousands of

1259
01:15:30.840 --> 01:15:32.970
servers probably did any given time you

1260
01:15:32.970 --> 01:15:37.470
know seven of them are out to lunch or

1261
01:15:37.470 --> 01:15:39.600
unreliable or slow because somebody's

1262
01:15:39.600 --> 01:15:41.130
installing new software who knows what

1263
01:15:41.130 --> 01:15:46.619
and that so it's a bit damaging to have

1264
01:15:46.619 --> 01:15:48.899
every request be sort of limited by the

1265
01:15:48.899 --> 01:15:52.369
slowest server whereas brafton paxos

1266
01:15:52.369 --> 01:15:54.930
well it's so rad for example if one of

1267
01:15:54.930 --> 01:15:56.399
the followers is so it doesn't matter

1268
01:15:56.399 --> 01:15:57.689
because that leader only has to wait for

1269
01:15:57.689 --> 01:15:59.340
a majority it doesn't have to wait for

1270
01:15:59.340 --> 01:16:01.529
all of them you know ultimately they all

1271
01:16:01.529 --> 01:16:04.909
have to catch up but raft is much better

1272
01:16:04.909 --> 01:16:07.890
resisting transient slowdown and some

1273
01:16:07.890 --> 01:16:09.390
Paxos based systems although not really

1274
01:16:09.390 --> 01:16:12.869
raft are also good at dealing with the

1275
01:16:12.869 --> 01:16:14.369
possibility that the replicas are in

1276
01:16:17.489 --> 01:16:19.140
need a majority you don't have to

1277
01:16:19.140 --> 01:16:21.300
necessarily wait for acknowledgments

1278
01:16:21.300 --> 01:16:23.250
from a distant data center and so that

1279
01:16:23.250 --> 01:16:25.680
can also leads people to use paxos raft

1280
01:16:25.680 --> 01:16:28.649
like majority schemes rather than chain

1281
01:16:28.649 --> 01:16:31.859
replication but this is sort of a it

1282
01:16:35.430 --> 01:16:41.399
overall architecture is in I don't know

1283
01:16:41.399 --> 01:16:42.659
if it's Universal but it's extremely

1284
01:17:02.350 --> 01:17:07.760
like intentional topologies okay the for

1285
01:17:07.760 --> 01:17:10.399
a for a network that's not broken the

1286
01:17:10.399 --> 01:17:12.829
usual assumption is that all the

1287
01:17:12.829 --> 01:17:14.270
computers can talk to each other through

1288
01:17:14.270 --> 01:17:16.220
the network for networks that are broken

1289
01:17:16.220 --> 01:17:18.619
because somebody stepped on a cable or

1290
01:17:18.619 --> 01:17:22.579
some routers misconfigured any crazy

1291
01:17:22.579 --> 01:17:23.899
thing can happen

1292
01:17:23.899 --> 01:17:27.619
so absolutely due to miss configuration

1293
01:17:34.579 --> 01:17:38.529
up but they can't talk to each other so

1294
01:17:38.529 --> 01:17:41.539
yes and and that's a killer for this

1295
01:17:41.539 --> 01:17:42.649
right because other configuration

1296
01:17:42.649 --> 01:17:44.180
manager thinks that are up they can't

1297
01:17:44.180 --> 01:17:46.220
talk to each other boy it's just like

1298
01:17:46.220 --> 01:17:50.449
it's a disaster and if you need your

1299
01:17:50.449 --> 01:17:52.939
system to be resistant to that then you

1300
01:17:52.939 --> 01:17:53.930
need to have a more careful

1301
01:17:57.979 --> 01:17:59.239
I'm only gonna form a chain out of these

1302
01:17:59.239 --> 01:18:01.699
services not only I can talk to that but

1303
01:18:01.699 --> 01:18:03.439
they can talk to each other and sort of

1304
01:18:03.439 --> 01:18:05.750
explicitly check and I don't know if

1305
01:18:05.750 --> 01:18:07.930
that's common I mean I'm gonna guess not

1306
01:18:11.270 --> 01:18:13.479
network partition that's like a

1307
01:18:13.479 --> 01:18:16.520
abstraction and in reality you can get

1308
01:18:16.520 --> 01:18:19.130
any combination of who can talk to who

1309
01:18:19.130 --> 01:18:23.949
else and some are may be very damaging

1310
01:18:24.310 --> 01:18:28.850
okay I'm gonna wrap up and see you next

1311
01:18:28.899 --> 01:18:28.950
week

